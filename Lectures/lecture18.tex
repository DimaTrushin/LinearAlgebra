\ProvidesFile{lecture18.tex}[Лекция 18]


\newpage
\section{Классификационная задача для линейных операторов}

Абстрактные объекты вроде векторных пространств и линейных операторов становятся более знакомыми после выбора базиса.
Пространство превращаются в столбцы, а операторы в квадратные матрицы.
Но так как базис выбирать можно по-разному, то и матрицы в такой ситуации получаются черт знает какими.
Основной вопрос классификационной задачи: как понять по матрицам, что они задают один и тот же линейный оператор, но в разных базисах.
Другой вопрос: к какому самому простому виду можно привести матрицу линейного оператора.
Мы уже видели ответ в случае линейного отображения между разными пространствами, в этом случае все контролируется рангом матриц.
Оказывается, что в случае линейного оператора ситуация сильно сложнее и зависит от выбора поля.

Пусть теперь $\varphi\colon V\to V$ -- линейный оператор, т.е. линейное отображение из векторного пространства в себя.
Тогда при выборе базиса $e$ в $V$ наш оператор превращается в матрицу $A\in \operatorname{M}_n(F)$, где $n = \dim_F V$.
Если же мы выберем другой базис $e'$ в $V$ такой, что $e' = eC$ для некоторой обратимой $C \in \operatorname{M}_n(F)$.
То матрица $\varphi$ в базисе $e'$ будет $C^{-1}AC$.
Заметим сложность ситуации.
Мы теперь не можем независимо домножать нашу матрицу с разных сторон на разные матрицы.
Если думать в терминах элементарных преобразований, мы теперь должны неким сложным образом согласовывать преобразования строк и столбцов.
Из-за этих ограничений кустарными методами (вроде подбора элементарных преобразований) для приведения матрицы в хороший вид нам обойтись не получится.
Более того, степень <<хорошести>> нашей матрицы будет сильно зависеть от свойств поля $F$ над которым определены наши векторные пространства.
А так как элементарные преобразования ничего не знают про свойства поля, то это автоматически означает, что не мы такие неумелые, что не смогли воспользоваться элементарными преобразованиями, а этот метод в лоб просто не работает.

Для преодоления сложившихся трудностей в случае оператора приходится привлекать более продвинутую технику.
К такой технике как раз и относятся собственные векторы и значения, собственные и корневые подпространства.
Кульминацией для нас будет теорема о жордановой нормальной форме.
Формулировать мы ее пока не будем, но обсудим некоторые стратегические соображения.

Для чего вообще меняется базис?
Для того, чтобы сделать вид матрицы линейного оператора максимально простым.
Тогда заменив его простой матрицей, его будет проще изучать.
В идеале простой вид -- это когда много нулей.
Самый желанный для нас вид -- диагональный.
Было бы еще лучше, если бы можно было сделать матрицу диагональной с единицами и нулями на диагонали, но это совсем не возможно.
Например, если оператор не вырожден, то его определитель не меняется, а он совпадает с произведением диагональных элементов матрицы.
То есть скалярную матрицу $\lambda E$ никогда нельзя сделать единичной $E$ путем замены базиса (это так же видно из формулы замены) если $\lambda \neq 1$.

Оказывается и диагональной можно сделать не всякую матрицу путем сопряжения, то есть не всякий линейный оператор приводится к диагональному виду в каком-то базисе.
Потому один из первых вопросов, которым мы хотим заняться -- это вопрос: когда линейный оператор задается диагональной матрицей в некотором базисе.
 
\subsection{Диагонализуемость линейного оператора}

\begin{definition}
Пусть $\varphi\colon V\to V$ -- линейный оператор над некоторым полем $F$.
Будем говорить, что $\varphi$ диагонализуется или диагонализируемый, если в некотором базисе его матрица является диагональной.
\end{definition}

\begin{claim}
\label{claim::EigenRootInd}
Пусть $\varphi\colon V\to V$ -- линейный оператор в некотором векторном пространстве над полем $F$ и пусть $\lambda_1,\ldots,\lambda_k\in F$ -- различные числа.
Тогда
\begin{enumerate}
\item Пространства $V_{\lambda_1},\ldots,V_{\lambda_k}$ линейно независимы.%
\footnote{Определение линейной независимости подпространств~\ref{def::IndepSpaces}.}%
${}^{,\,}$%
\footnote{Прошу обратить внимание, что линейно независимые подпространства могут быть нулевыми или часть из них может быть нулевыми.}

\item Пространства $V^{\lambda_1},\ldots,V^{\lambda_k}$ линейно независимы.
\end{enumerate}
\end{claim}
\begin{proof}
1) В начале покажем случай собственных подпространств.
Пусть $u_1\in V_{\lambda_1},\ldots,u_k\in V_{\lambda_k}$ -- произвольные ненулевые векторы такие, что $u_1 + \ldots + u_k = 0$.
Применим к этому равенству оператор $\varphi - \lambda_1\Identity$.
Тогда $u_1$ занулится, а $(\varphi - \lambda_1\Identity)u_i = (\lambda_i - \lambda_1)u_i$ будет ненулевым вектором из $V_{\lambda_i}$ при $i \neq 1$.
Обозначим эти векторы за $u_2',\ldots,u_k'$.
Тогда мы доказали, что если у нас дана сумма из $k$ ненулевых векторов $u_1+\ldots+u_k = 0$, то мы можем получить более короткую сумму из $k - 1$ вектора $u_2'+\ldots+u_k' = 0$.

2) Теперь давайте разберемся с корневыми.
Пусть $v_1,\ldots,v_s$ -- набор векторов такой, что $v_i\in V^{\lambda_i}$ и $v_1 + \ldots + v_s = 0$, где $s$ -- самое маленькое из возможных.
Если $s = 1$, то имеем $v_1 = 0$ и доказывать нечего.

Теперь считаем, что у нас $s > 1$.
Так как $v_s$ -- корневой, то для некоторого $m$ получаем $(\varphi - \lambda_s\Identity)^m v_s = 0$.
Тогда получим
\[
(\varphi - \lambda_s\Identity)^m v_1 + \ldots + (\varphi - \lambda_s\Identity)^m v_{s-1} = 0
\]
Если мы покажем, что $(\varphi - \lambda_s\Identity)^m v_i$ лежит в $V^{\lambda_i}$ и хотя бы одно из них не ноль, то мы придем к противоречию, так как получим более короткую сумму корневых векторов, дающую ноль.

Каждое подпространство $V^{\lambda_i}$ является $\varphi$ инвариантным.
А значит и $\varphi - \lambda_s \Identity$ инвариантным.
А значит и $(\varphi - \lambda_s \Identity)^m$ инвариантным.
Это показывает, что все слагаемые действительно остаются внутри соответствующего $V^{\lambda_i}$.

Теперь проверим, что хотя бы одно из слагаемых не равно нулю.
Давайте покажем более сильное утверждение, если $v_i \neq 0$, то и $(\varphi - \lambda_s \Identity)^m v_i \neq 0$.
По определению корневого пространства, мы можем найти такое $d$, что
\[
(\varphi - \lambda_i \Identity)^d v_i = 0
\quad\text{и}\quad
u_i = (\varphi - \lambda_i \Identity)^{d-1} v_i \neq 0
\]
В частности $(\varphi - \lambda_i \Identity)u_i = 0$, то есть $u_i$ -- собственный вектор.
Чтобы показать, что $(\varphi - \lambda_s \Identity)^m v_i $ не нулевой, достаточно показать, что
\[
(\varphi - \lambda_i \Identity)^{d-1}(\varphi - \lambda_s \Identity)^m v_i \neq 0
\]
Действительно,
\[
(\varphi - \lambda_i \Identity)^{d-1}(\varphi - \lambda_s \Identity)^m v_i =
(\varphi - \lambda_s \Identity)^m (\varphi - \lambda_i \Identity)^{d-1}v_i =
(\varphi - \lambda_s \Identity)^m u_i
\]
Но по определению $u_i$ -- собственный вектор с собственным значением $\lambda_i$.
Это значит, что умножение на $\varphi$ совпадает с умножением на $\lambda_i$ на векторе $u_i$.
Значит
\[
(\varphi - \lambda_s \Identity)^m u_i = (\lambda_i - \lambda_s)^m u_i \neq 0
\]
\end{proof}

\begin{claim}
\label{claim::DiagCrit}
Пусть $\varphi\colon V\to V$ -- некоторый линейный оператор в векторном пространстве над полем $F$.
Тогда следующие утверждения эквивалентны:
\begin{enumerate}
\item Оператор $\varphi$ диагонализуем.

\item Существует базис из собственных векторов.

\item Существует разложение $V = V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}$.

\item 
\begin{enumerate}
\item Характеристический многочлен раскладывается на линейные множители 
\[
\chi_{\varphi}(t) = (t - \lambda_1)^{r_1} \ldots (t - \lambda_k)^{r_k}
\]

\item для каждого $i$ верно $\dim_F V_{\lambda_i} = r_i$.
\end{enumerate}
\end{enumerate}
\end{claim}
\begin{proof}
(1)$\Leftrightarrow$(2).
Пусть $e$ -- некоторый базис $V$.
Тогда $\varphi e = e A$, где $A$ -- матрица $\varphi$ в базисе $e$.
По определению $\varphi$ диагонализуем в базисе $e$ тогда и только тогда, когда $A$ диагональная.
С другой стороны, все векторы в $e$ собственные тогда и только тогда, когда $A$ диагональная.

(2)$\Rightarrow$(3).
Пусть $e$ -- базис из собственных векторов и $\varphi e = e A$, где $A$ -- диагональная с числами $\lambda_1,\ldots,\lambda_k$ на диагонали (эти числа могут повторяться).
Для удобства переупорядочим вектора так, чтобы одинаковые числа $\lambda_i$ шли по-порядку.
Тогда базис $e$ можно разеделить на части $e = e_1 \sqcup \ldots \sqcup e_k$, где все векторы из $e_i$ являются собственными с собственным значением $\lambda_i$.
Значит $\langle v\mid v\in e_i\rangle\subseteq V_{\lambda_i}$.
А значит 
\[
V = \langle e \rangle = \sum_i \langle e_i\rangle \subseteq  \sum_i V_{\lambda_i} \subseteq V
\]
То есть мы показали, что $V = \sum_i V_{\lambda_i}$ является суммой.
С другой стороны, утверждение~\ref{claim::EigenRootInd} гарантирует, что векторные подпространства $V_{\lambda_i}$ линейно независимы, а значит сумма прямая (одно из эквивалентных определений по утверждению~\ref{claim::DirectSum}).

(3)$\Rightarrow$(2).
Пусть $V = V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}$ и пусть $e_i$ -- какой-нибудь базис $V_{\lambda_i}$.
Тогда по одному из эквивалентных определений прямой суммы (утверждение~\ref{claim::DirectSum}) $e = e_1 \sqcup\ldots \sqcup e_k$ будет базисом для $V$.
Тогда это и есть базис из собственных векторов.

(3)$\Rightarrow$(4).
Выберем как в предыдущем пункте базис $e_i$ в каждом слагаемом $V_{\lambda_i}$.
Тогда $|e_i| = \dim_F V_{\lambda_i}$.
Запишем матрицу нашего оператора в этом базисе в блочном виде
\[
\varphi(e_1,\ldots,e_k)
=
(e_1,\ldots,e_k)
\begin{pmatrix}
{\lambda_1 E}&{}&{}\\
{}&{\ddots}&{}\\
{}&{}&{\lambda_k E}
\end{pmatrix}
\]
где размеры блоков равны в точности $r_i=|e_i| = \dim_F V_{\lambda_i}$.
Тогда характеристический многочлен $\chi_\varphi(t) = (t-\lambda_1)^{r_1}\ldots(t-\lambda_k)^{r_k}$.
Как мы видим многочлен разложился на линейные множители и кратности корней совпали с размерностями $V_{\lambda_i}$.

(4)$\Rightarrow$(3).
Пусть $\lambda_1,\ldots,\lambda_k$ -- все корни характеристического многочлена.
Тогда по утверждению~\ref{claim::EigenRootInd} сумма $V_{\lambda_1}+\ldots +V_{\lambda_k}$ всегда прямая.
То есть мы имеем $V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}\subseteq V$.
И осталось лишь проверить равенство.
Для этого посчитаем размерности.
С одной стороны $\dim_F V = \deg \chi_\varphi$ по определению.
С другой стороны размерность левой части есть
\[
\sum_{i}\dim_F V_{\lambda_i} = \sum_i r_i = \deg \chi_\varphi
\]
В первом равенстве мы воспользовались вторым условием, а во втором равенстве первым (если многочлен разложился на линейные множители, то сумма кратностей его корней равна степени).
Значит обе размерности совпали и пространства оказались равны.
\end{proof}

\paragraph{Примеры}

\begin{enumerate}
\item Рассмотрим оператор $A\colon \mathbb R^2 \to \mathbb R^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{a}&{-b}\\{b}&{a}\end{smallmatrix}\right)$, $a, b\in\mathbb R$.
Если $b\neq 0$ то этот оператор не диагонализуется, потому что его хар многочлен имеет только комплексные корни $a+bi$ и $a - bi$ и не имеет вещественных.
Значит не выполняется пункт 4(a).

\item Теперь рассмотрим оператор заданный той же матрицей, но в случае комплексного векторного пространства $A\colon \mathbb C^2 \to \mathbb C^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{a}&{-b}\\{b}&{a}\end{smallmatrix}\right)$, $a, b\in\mathbb R$.
Этот оператор диагонализуется и в некотором базисе записывается в виде $\left( \begin{smallmatrix}{a + bi}&{0}\\{0}&{a-bi}\end{smallmatrix}\right)$.

\item Теперь рассмотрим оператор $A\colon \mathbb C^2 \to \mathbb C^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{0}&{1}\\{0}&{0}\end{smallmatrix}\right)$.
Тогда его характеристический многочлен $\chi_A(t) = t^2$ раскладывается на линейные множители.
Число $\lambda = 0$ является единственной точкой спектра, то есть это единственное собственное значение.
Собственное подпространство $V_\lambda$ для $\lambda = 0$ задается $\{x\in \mathbb C^2\mid Ax = 0\}$, которое совпадает с $\langle e_1 \rangle$.
То есть $\dim V_\lambda$ не равно кратности корня, то есть он не диагонализуется даже над $\mathbb C$.
\end{enumerate}

Таким образом мы видим, что диагонализуемость зависит от поля.
Часть причин недиагонализуемости -- плохой выбор поля.
В этом случае в утверждении~\ref{claim::DiagCrit} не выполняется условие 4(a).
Такая проблема решается расширением поля до алгебраически замкнутого поля (так всегда можно сделать).
Но последний пример показывает, что существуют операторы, которые не диагонализуются над любым полем.
Это по-настоящему недиагонализуемые операторы.
А действительно важное препятствие к диагонализуемости -- это условие 4(b).
Другими словами условие 4(b) означает, что размерность собственного подпространства должна совпасть с кратностью соответствующего собственного значения.

\subsection{Свойства ограничения оператора}

Теперь нам надо освоить несколько мелких технических утверждений связанных с поведением различных характеристик ограничения оператора на инвариантное подпространство.

\begin{claim}
Пусть $\varphi\colon V\to V$ -- линейный оператор и $U\subseteq V$ -- инвариантное подпространство.
Тогда
\begin{enumerate}
\item Если $\varphi$ обратим, то $\varphi|_U$ обратим.

\item $\spec_F \varphi|_U\subseteq \spec_F \varphi$.
\end{enumerate}
\end{claim}
\begin{proof}
(1) Если $\varphi$ обратим, то $\ker \varphi = 0$, тогда $\ker \varphi|_U = \ker \varphi \cap U = 0$.
А значит $\varphi|_U$ обратим по утверждению~\ref{claim::OperatorInvert}.

(2) Нам надо показать, что если $\lambda\notin\spec_F \varphi$, то $\lambda\notin\spec_F \varphi|_U$.
То есть если $\varphi-\lambda\Identity$ обратим, то и $\varphi|_U-\lambda \Identity$ обратим.
Но это следует из первого пункта и наблюдения 
\[
(\varphi - \lambda \Identity)|_U = \varphi|_U -(\lambda\Identity)|_U = \varphi|_U - \lambda \Identity
\]
\end{proof}

\begin{claim}
\label{claim::RestrictionChar}
Пусть $V = U\oplus W$, $\varphi\colon V\to V$ -- линейный оператор и подпространства $U$ и $W$ являются $\varphi$-инвариантными.
Тогда
\begin{enumerate}
\item $\tr \varphi = \tr \varphi|_U + \tr \varphi|_W$.

\item $\det \varphi = \det \varphi|_U \det \varphi|_W$.

\item $\chi_\varphi(t) = \chi_{\varphi|_U}(t) \chi_{\varphi|_W}(t)$.

\item $\spec_F \varphi = \spec_F \varphi|_U \cup \spec_F \varphi|_W$.
\end{enumerate}
\end{claim}
\begin{proof}
Пусть $e$ -- базис $U$ и $f$ -- базис $W$.
Тогда по одному из эквивалентных определений прямой суммы (утверждение~\ref{claim::DirectSum}) $e \cup f$ будет базисом $V$.
Давайте запишем в этом базисе наш оператор $\varphi$:
\[
\varphi(e,f) = (e,f)
\begin{pmatrix}
{A}&{B}\\
{C}&{D}
\end{pmatrix}
\]
Так как подпространство $U =\langle e \rangle$ $\varphi$-инвариантно, то есть $\varphi(U)\subseteq U$, то $\varphi(e)$ выражается только через $e$.
Последнее означает, что $C = 0$.
Аналогично, так как $W$ $\varphi$-инвариантно, то $B = 0$.
А значит
\[
\varphi(e,f) = (e,f)
\begin{pmatrix}
{A}&{0}\\
{0}&{D}
\end{pmatrix}
\]
При этом по определению $A$ -- это матрица $\varphi|_U$ в базисе $e$, а $D$ -- это матрица $\varphi|_W$ в базисе $f$.
Тогда все четыре утверждения следуют из явного подсчета следа, определителя, характеристического многочлена и спектра для блочно диагональных матриц.
\end{proof}

\subsection{Приведение к верхнетреугольному виду}

\begin{claim}
Пусть $\varphi \colon V\to V$ -- линейный оператор в векторном пространстве размерности $n$ над полем $F$ и пусть $\lambda\in F$ -- корень минимального многочлена для $\varphi$.
Тогда существует базис, в котором матрица $\varphi$ имеет следующий блочный вид
\[
A_\varphi = 
\begin{pmatrix}
{\lambda}&{*}\\
{0}&{B}
\end{pmatrix},\text{ где }
B\in \operatorname{M}_{n-1}(F)
\]
\end{claim}
\begin{proof}
Тут можно было бы воспользоваться утверждением~\ref{claim::EigenSpec}.
Тогда для корня $\lambda$ подпространство $V_\lambda$ не нулевое.
А значит в нем есть искомый ненулевой вектор.
Однако, полезно понимать, как найти собственный вектор с помощью минимального многочлена явно.
Давайте проделаем это.

Пусть $f_{\text{min}}=(t-\lambda)g(t)$.
Тогда $g(\varphi) \neq 0$, то есть $g(\varphi)$ -- ненулевой оператор.
Последнее означает, что для какого-то вектора $v\in V$, $g(\varphi)v\neq 0$.
Обозначим $u = g(\varphi)v$.
Тогда это ненулевой вектор.
С другой стороны
\[
(\varphi - \lambda \Identity)u = (\varphi - \lambda \Identity)g(\varphi)v = f_\text{min}(\varphi)v = 0
\]
То есть $u$ -- ненулевой собственный вектор с собственным значением $\lambda$.
Раз это ненулевой вектор, то множество $\{u\}$ линейно независимое.
А значит его можно дополнить до базиса.
Пусть это будет $u,u_2,\ldots,u_n$.
Тогда в этом базисе матрица оператора $\varphi$ будет иметь заявленный вид.
Действительно, по определению
\[
\varphi(u, u_2,\ldots, u_n) = (u, u_2,\ldots,u_n)
\begin{pmatrix}
{a}&{*}\\
{w}&{B}
\end{pmatrix}
\]
Тогда $\varphi u = a u + (u_2, \ldots, u_n)w$.
Но мы уже знаем, что $\varphi u = \lambda u$.
То есть $a = \lambda$ и $w = 0$.
\end{proof}

\begin{claim}
\label{claim::OperatorUpperTriangle}
Пусть $\varphi\colon V\to V$ -- линейный оператор в векторном пространстве над полем $F$ и пусть минимальный многочлен $f_{\text{min}}$ для $\varphi$ раскладывается на линейные множители $(t-\lambda_1)^{k_1}\ldots (t-\lambda_r)^{k_r}$.
Тогда существует базис в $V$ такой, что матрица $\varphi$ верхнетреугольная с числами $\lambda_1,\ldots,\lambda_r$ на диагонали (возможно с повторениями).
\end{claim}
\begin{proof}
Я не смогу вам дать доказательство полностью на языке операторов, так как мы не владеем некоторыми необходимыми техническими средствами в виде фактор пространств и фактор операторов.
Потому надо будет переформулировать все в терминах матриц.

В начале выберем случайный базис в $V$.
Тогда наш оператор превратится в $A\colon F^n \to F^n$.
Нам надо найти такую обратимую матрицу $D\in \operatorname{M}_n(F)$, что $D^{-1}AD$ будет верхне треугольной с числами $\lambda_1,\ldots, \lambda_r$ на диагонали (может быть с повторениями).

Начнем.
Так как минимальный многочлен раскладывается на линейные, то он имеет корень, например, выберем $\lambda_1$.
Тогда предыдущее утверждение означает, что можно найти обратимую матрицу $C\in \operatorname{M}_n(F)$ такую, что
\[
C^{-1}AC =
\begin{pmatrix}
{\lambda_1}&{*}\\
{0}&{B}
\end{pmatrix}
\]
Заметим, что для произвольного многочлена $f$ верно
\[
f\begin{pmatrix}
{\lambda_1}&{*}\\
{0}&{B}
\end{pmatrix}
=
\begin{pmatrix}
{f(\lambda_1)}&{*}\\
{0}&{f(B)}
\end{pmatrix}
\]
А значит $f_\text{min}$ зануляет блок $B$.
То есть минимальный многочлен для $B$ тоже раскладывается на линейные множители и его корни находятся среди корней $f_\text{min}$, так как минимальный для $B$ делит $f_\text{min}$.
А значит, для $B$ по индукции найдется такая обратимая матрица $T\in \operatorname{M}_{n-1}(F)$, что $T^{-1}BT$ является верхне треугольной с числами $\lambda_1,\ldots,\lambda_r$ на диагонали (быть может с повторениями и пропусками).
Тогда
\[
\begin{pmatrix}
{1}&{}\\
{}&{T}
\end{pmatrix}^{-1}
C^{-1}AC
\begin{pmatrix}
{1}&{}\\
{}&{T}
\end{pmatrix}
=
\begin{pmatrix}
{1}&{}\\
{}&{T}
\end{pmatrix}^{-1}
\begin{pmatrix}
{\lambda_1}&{*}\\
{0}&{B}
\end{pmatrix}
\begin{pmatrix}
{1}&{}\\
{}&{T}
\end{pmatrix}=
\begin{pmatrix}
{\lambda_1}&{*}\\
{0}&{T^{-1}BT}
\end{pmatrix}
\]
Последняя матрица верхне треугольная с числами $\lambda_1,\ldots,\lambda_r$ на диагонали (быть может с повторениями).
То есть мы доказали, что хотели с матрицей
\[
D = C 
\begin{pmatrix}
{1}&{}\\
{}&{T}
\end{pmatrix}
\]
\end{proof}

\begin{claim}
\label{claim::CharPolyOnRootSpace}
Пусть $\varphi\colon V\to V$ -- некоторый линейный оператор на векторном пространстве $V$ над полем $F$.
Тогда $\chi_{\varphi|_{V^\lambda}} (t) = (t - \lambda)^{\dim V^\lambda}$.
\end{claim}
\begin{proof}
Оператор $\varphi|_{V^\lambda}$ зануляется многочленом вида $(t-\lambda)^d$.
Значит его минимальный многочлен имеет вид $(t-\lambda)^k$.
По утверждению~\ref{claim::OperatorUpperTriangle} матрица оператора $\varphi|_{V^\lambda}$ приводится к верхнетреугольному виду с $\lambda$ на диагонали.
А значит $\chi_{\varphi|_{V^\lambda}}(t) = (t - \lambda)^{\dim V^{\lambda}}$.
\end{proof}
