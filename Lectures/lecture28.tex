\ProvidesFile{lecture28.tex}[Лекция 28]


\subsection{Геометрия в Евклидовых пространствах}

\begin{definition}
Пусть $V$ -- евклидово пространство и $v\in V$ -- произвольный вектор.
Определим длину вектора $|v| = \sqrt{(v,v)}$.
\end{definition}

\paragraph{Замечания}

\begin{itemize}
\item Именно для того, чтобы определить длину произвольного вектора, нам нужна положительная определенность в определении скалярного произведения.

\item Обратите внимание, что $|v| = 0$ тогда и только тогда, когда $v = 0$.

\item Если выбрать ортонормированный базис, то $|x| = \sqrt{\sum_{i=1}^n x_i^2}$.
То есть это $|{-}|_2$ норма на $\mathbb R^n$.
На самом деле можно развивать теорию норм на произвольных векторных пространствах, как это делается в функциональном анализе.
\end{itemize}

\begin{claim}
[Неравенство Коши-Буняковского]
Пусть $V$ -- евклидово пространство, $v,u\in V$, тогда $|(v,u)|\leqslant |v| |u|$.
Кроме того, равенство достигается тогда и только тогда, когда $u$ и $v$ лежат на одной прямой.
\end{claim}
\begin{proof}
Так как у нас всего два вектора, можно считать, что $V$ двумерно.
Выберем первый базисный вектор $e_1$ вдоль $v$ (если $v$ нулевой, то выбираем любой), а второй -- любой ортогональный к $e_2$ и длины $1$.
Тогда
\[
v = 
\begin{pmatrix}
{a}\\{0}
\end{pmatrix}
\quad\text{и}\quad
u = 
\begin{pmatrix}
{b}\\{c}
\end{pmatrix}
\]
Тогда $|(v, u)| = |ab|$, а $|v||u| = |a|\sqrt{b^2 + c^2}$.
Доказываемое неравенство превращается в $|ab|\leqslant |a|\sqrt{b^2 + c^2}$, что очевидно.

Давайте проанализируем, когда в нем достигается равенство.
Во-первых, если $a = 0$.
В этом случае $v$ и $u$ лежат на одной прямой.
Если $a \neq 0$, то мы получаем условие $|b| = \sqrt{b^2 + c^2}$, что равносильно тому, что $c = 0$.
В этом случае векторы тоже лежат на одной прямой.
Обратно, если векторы лежат на одной прямой, то $v = \lambda e$ и $u = \mu e$ для некоторого ненулевого вектора $e\in V$.
Тогда $(v,u) = \lambda\mu (e,e)$, а с другой стороны $|v||u| = |\lambda e||\mu e| = |\lambda \mu| |e|^2 = |\lambda \mu|(e,e)$.
\end{proof}

\paragraph{Замечание} 

Хочу обратить внимание на то, что по сути доказательство можно было закончить на первой строчке, где я ссылаюсь  на школьную геометрию.
Вся остальная часть всего лишь доказывала факт из школьной геометрии.
Это было сделано для полноты изложения.
К тому же я продемонстрировал метод последовательного выбора удобных базисных векторов, который часто применяется при решении задач аналитической геометрии.
Подобный метод позволяет упростить разбор общего случая в координатах за счет наличия большого количества нулей в векторах.
В нашем случае ноль был всего один, но это сильно сократило вычисления.

Из неравенства Коши-Буняковского следует, что для любых двух ненулевых векторов $v,u\in V$ верно $-1\leqslant \frac{(v,u)}{|v| |u|}\leqslant 1$.
А значит найдется единственное число $\alpha\in [0,\pi]$ такое, что $\cos \alpha = \frac{(v,u)}{|v| |u|}$.

\begin{definition}
Пусть $V$ -- евклидово пространство и $v,u\in V$ -- два вектора.
Тогда число $\alpha$ такое, что $\cos \alpha = \frac{(v,u)}{|v| |u|}$, называется углом между векторами $v$ и $u$.
Будем этот угол обозначать через $\angle(v, u)$.
\end{definition}

\paragraph{Замечание}

Так как два вектора $v$ и $u$ всегда лежат внутри <<школьной плоскости>>, то определение угла превращается в то самое определение угла, которое дается в школьном курсе геометрии.
Потому от этого угла надо ожидать ровно то же самое поведение, к которому мы привыкли в курсе школьной геометрии.
Просто потому, что это тот же самый угол.


\begin{claim}
[Теорема Пифагора]
\label{claim::Pythagoras}
Пусть $V$ -- евклидово пространство и $v,u\in V$ -- два ортогональных вектора, тогда $|v + u|^2 = |v|^2 + |u|^2$.
\end{claim}
\begin{proof}
Формальное доказательство в этом случае является наиболее простым:
\[
|v+u|^2 = (v+u, v+u) = (v,v) + (v,u)+(u,v) +(u,u) = (v,v) + (u,u) = |v|^2 + |u|^2
\]
Здесь $(v,u)=(u,v) = 0$ из ортогональности $u$ и $v$.
\end{proof}

Процесс, применяемый к базисным векторам в методе Якоби (раздел~\ref{subsection::JacobyAlg}), в случае евклидова пространства называется ортогонализацией Грама-Шмидта.
Единственное отличие -- ортогонализация Грама-Шмидта применяется не только к линейно независимым векторам, а к произвольным системам векторов.

\subsubsection*{Ортогонализация методом Грама-Шмидта}

\paragraph{Дано}

Евклидово пространство $V$, система векторов $\{v_1,\ldots,v_k\}\subseteq V$.%
\footnote{Обратите внимание, что $V$ не обязательно задано как пространство столбцов $\Vector{n}$.
Это может быть и пространство многочленов определенной степени или пространство тригонометрических функций, или вообще что угодно.
Даже если $V$ задано как $\Vector{n}$, то скалярное произведение не обязательно стандартное, т.е. скалярное произведение может быть задано любой положительной симметрической матрицей.}

\paragraph{Задача}

Найти систему ортогональных векторов $\{u_1,\ldots,u_r\}\subseteq V$ такую, что $\langle v_1,\ldots,v_k\rangle = \langle u_1,\ldots,u_r\rangle$.

\paragraph{Алгоритм}

\begin{enumerate}
\item В качестве первого вектора $u_1$ берем первый ненулевой вектор из $v_i$.
Если таких нет, то ответ -- пустое множество.

\item Пусть мы нашли вектора $u_1,\ldots,u_s$ и пусть $v_d$ -- первый еще не просмотренный вектор среди $v_i$.
Посчитать вектор 
\[
u' = v_d - \frac{(v_d, u_1)}{(u_1,u_1)} u_1 - \ldots - \frac{(v_d, u_s)}{(u_s, u_s)}u_s
\]

\item Если $u' \neq 0$ положим $u_{s+1} = u'$, иначе пропустим $v_d$.
Теперь перейдем к предыдущему шагу с вектором $v_{d+1}$ вместо $v_d$.
\end{enumerate}



\subsection{Проекции}
\label{section::OrthoProjection}

\begin{claim}
Пусть $V$ -- евклидово пространство и $U\subseteq V$ -- произвольное подпространство.
Тогда $V = U\oplus U^\bot$.
\end{claim}
\begin{proof}
Это в точности утверждение~\ref{claim::NonDegRestrictionBil} пункт~(2).
\end{proof}

Таким образом в евклидовом пространстве $V$ при фиксированном подпространстве $U\subseteq V$, любой вектор $v\in V$ единственным образом раскладывается в сумму $v = \pr_U v + \ort_U v$, где $\pr_U v \in U$ и $\ort_U v\in U^\bot$.


\begin{definition}
Если $V$ -- евклидово пространство, $U\subseteq V$ -- произвольное подпространство и $v\in V$, то 
\begin{itemize}
\item Вектор $\pr_U v$ называется ортогональной проекцией $v$ на $U$.

\item Вектор $\ort_U v$ называется ортогональной составляющей $v$ относительно $U$.
\end{itemize}
\end{definition}

Обратите внимание, что ортогональная проекция $v$ на $U$ -- это проекция $v$ на $U$ вдоль $U^\bot$, а ортогональная составляющая -- проекция $v$ на $U^\bot$ вдоль $U$.

\subsubsection*{Формула БАБА}

Давайте я в начале разберу задачу нахождения проекции вектора на подпространство вдоль другого подпространства (здесь нам не нужно никакое скалярное произведение).
Пусть $V$ -- некоторое векторное пространство и $V = U\oplus W$.
Тогда на пространстве $V$ задан оператор проекции $P\colon V\to V$ такой, что $\ker P = W$ и $P|_U = \Identity$, то есть, если $v\in V$ раскладывается в сумму $v = u + w$, где $u\in U$ и $w\in W$, то $Pv = u$ -- оператор вычисления проекции на $U$ вдоль $W$.


Теперь мы хотим научиться эффективно считать $P$.
Для этого предположим $V = \mathbb R^n$, $U = \langle u_1,\ldots,u_k\rangle$, $W = \{y\in \mathbb R^n\mid Ay = 0\}$, где $A\in \MatrixDim{s}{n}$.
В этом случае $P\colon \mathbb R^n\to \mathbb R^n$ задается некоторой матрицей.
Наша задача -- найти эту матрицу.

Предположим для простоты, что векторы $u_1,\ldots,u_k$ образуют базис $U$, а строки матрицы $A$ линейно независимы.
Определим матрицу $B = (u_1|\ldots|u_k)\in \MatrixDim{n}{k}$.
Тогда утверждаются следующие вещи:
\begin{enumerate}
\item Количество столбцов $B$ совпадает с количеством строк $A$, то есть $k = s$.

\item Матрица $AB$ обратима.

\item Оператор проекции задается формулой $P = B(AB)^{-1}A$.
Мнемоническое правило <<БАБА>>.
\end{enumerate}
\begin{proof}
Так как мы уже взрослые, я позволю себе пользоваться линейными операторами и отображениями, а не просто матричной техникой.
Матрица $A$ задает линейное отображение $A\colon \mathbb R^n \to \mathbb R^s$ такое, что $\ker A = W$ и $\Im A = \mathbb R^s$ (так как строки матрицы $A$ линейно независимы, то $\rk A = s$, но $\rk A = \dim \Im A$).
Матрица $B$ задает отображение $B\colon \mathbb R^k \to \mathbb R^n$ такое, что $\Im B = U$ и $\ker B = 0$ (так как столбцы $B$ линейно независимы).

(1) Теперь мы знаем, что 
\[
\begin{aligned}
\dim U + \dim W = n\\
\dim \ker A + \dim \Im A = n
\end{aligned}
\quad\text{ то есть }\quad
\begin{aligned}
k + \dim W = n\\
\dim W+ s= n
\end{aligned}
\quad\text{ откуда }\quad
s = k
\]

(2) Теперь рассмотрим отображение $AB\colon \mathbb R^k \to \mathbb R^k$.
Заметим, что $\Im B \cap \ker A = U \cap W = 0$.
Значит $\ker AB = 0$, то есть $AB$ -- обратимый оператор.

(3) Теперь выведем формулу для $P$.
Пусть $v = u + w$, где $v\in \mathbb R^n$ -- произвольный вектор, $u\in U$ и $w\in W$ -- его единственное разложение по прямой сумме подпространств.
Тогда $Av = Au + Aw = Au$.
С другой стороны, так как $u\in U$, мы имеем $u = B x$ для некоторого $x\in \mathbb R^k$.
Тогда $Av = ABx$.
Так как $AB$ обратимая квадратная матрица, имеем $x = (AB)^{-1}Av$.
Значит $u = Bx = B(AB)^{-1}Av$, что и требовалось.
\end{proof}

Обратите внимание, что проектор $P$ на $U$ вдоль $W$ зависит от двух подпространств, а не только от $U$.
Если вы измените одно из них, то проектор изменится.

\subsubsection*{Формула Атата}

Пусть $V = \mathbb R^n$ со стандартным скалярным произведением $(x, y) = x^ty$ и пусть подпространство $U\subseteq V$ задано своим базисом $U = \langle u_1,\ldots,u_k\rangle$.
Составим матрицу $A = (u_1|\ldots|u_k)\in\MatrixDim{n}{k}$.
Тогда $U^\bot = \{y\in \mathbb R^n \mid A^t y = 0\}$.
Пусть теперь $v\in V$ -- произвольный вектор и $v = \pr_U v + \ort_U v$.
Тогда формула <<БАБА>> превращается в $\pr_U v = A(A^tA)^{-1}A^tv$.
Мнемоническое правило для запоминания: в евклидовом пространстве БАБА -- это Атата.

Обратите внимание, что проектор $P$ всегда зависит от двух подпространств: то, на которое проектируем $U$, и то, вдоль которого проектируем $W$.
Но в случае ортогонального проектирования $W = U^\bot$, потому ортопроектор $P$ реально зависит только от одного подпространства.

\subsection{Расстояния и углы}

\begin{definition}
Пусть $V$ -- евклидово пространство и $u,v\in V$ -- произвольные векторы.
Тогда определим расстояние между векторами по формуле $\rho(u,v) = |u - v|$.
\end{definition}

\begin{claim}
Расстояние $\rho$ на евклидовом пространстве $V$ удовлетворяет следующим свойствам:
\begin{enumerate}
\item {\bf Невырожденность} $\rho(v,u) \geqslant 0$ для любых $u,v\in V$ причем равенство достигается тогда и только тогда, когда $v = u$.

\item {\bf Симметричность} $\rho(u,v) = \rho(v,u)$ для любых $u,v\in V$.

\item {\bf Неравенство треугольника} $\rho(u,v)\leqslant \rho(u,w) + \rho(w,v)$ для любых $u,v,w\in V$.
\end{enumerate}
\end{claim}
\begin{proof}
(1) По определению $\rho(v,u) = |v-u| \geqslant 0$ причем равенство достигается тогда и только тогда, когда $v - u = 0$.

(2) По определению $\rho(v, u) = |v - u| = |u-v| = \rho(u,v)$.

(3) Нам надо доказать $\rho(u,v)\leqslant \rho(u,w) + \rho(w,v)$, то есть $|u-v|\leqslant |u-w| + |w - v|$.
Положим $x = u - w$ и $y = w - v$.
Тогда нам надо доказать $|x + y|\leqslant |x| + |y|$.
Так как левая и правая части этого неравенства неотрицательные, то оно равносильно неравенству $|x+y|^2\leqslant (|x| + |y|)^2$.
Проверяем:
\[
|x+y|^2 = (x+y, x+y) = (x, x) + 2(x, y) + (y,y) = |x|^2 + 2 (x, y) + |y|^2
\]
По неравенству Коши-Буняковского $(x, y)\leqslant |x| |y|$.
Значит
\[
|x|^2 + 2 (x, y) + |y|^2\leqslant |x|^2 + 2|x||y| + |y|^2= (|x|+|y|)^2
\]
\end{proof}

\paragraph{Замечание}

Если на множестве $X$ задана функция $\rho\colon X\times X\to \mathbb R$ со свойствами из предыдущего утверждения, то пара $(X,\rho)$ называется метрическим пространством.
Самое главное, что известно про метрические пространства -- принцип сжимающих отображений.
Это один из способов доказывать существование объектов с  нужными свойствами.
Есть два важных примера:
\begin{itemize}
\item Теорема о неявной функции.
Ее доказательство через принцип сжимающих отображений в разы проще и доступнее, чем копание в координатах.

\item Теорема о существовании и единственности решения дифференциального уравнения.
Дифференциальное уравнение обычно заменяется на интегральное, после чего можно пользоваться метрическими пространствами и принципом сжимающих отображений.
\end{itemize}
В нашем случае метрика на евклидовом пространстве скорее является случайным гостем, заглянувшем на огонек, нежели чем-то фундаментальным и сверх полезным.
Самым полезным является понимание роли метрических пространств.

\begin{definition}
Пусть $X,Y\subseteq V$ -- произвольные подмножества евклидова пространства, тогда расстояние между ними определяется следующим образом:
\[
\rho(X, Y) = \inf_{\substack{x\in X\\y\in Y}}\rho(x, y)
\]
\end{definition}
Обратите внимание, что это расстояние не удовлетворяет аксиоме не вырожденности, то есть расстояние между разными множествами может быть нулевым.
Например, для этого достаточно, чтобы $X$ и $Y$ имели непустое пересечение.
Но, даже если $X\cap Y = \varnothing$, расстояние может быть нулем.
Например, если $X, Y \subseteq \mathbb R$ и $X = \{\frac{1}{n}\mid n\in \mathbb N\}$ и $Y = - X = \{-\frac{1}{n}\mid n\in \mathbb N\}$.

\begin{definition}
Пусть $V$ -- евклидово пространство, $v\in V$ и $L\subseteq V$ -- некоторое подпространство.
Тогда углом между $v$ и $L$ называется $\angle (v, L) = \inf_{u\in L}\angle (v,u)$.
\end{definition}

Теперь давайте обсудим, как эффективно находить некоторые расстояния и углы.

\begin{claim}
\label{claim::DistAngle}
Пусть $V$ -- евклидово пространство, $L\subseteq V$ -- подпространство, $v\in V$ -- некоторый вектор.
Тогда
\begin{enumerate}
\item $\rho(v, L) = |\ort_L v|$.

\item $\angle(v, L) = \angle(v, \pr_L v)$.%
\footnote{Если $\pr_L v = 0$, то надо считать косинус угла нулевым, то есть угол равным $\pi/2$.}
\end{enumerate}
\end{claim}
\begin{proof}
(1) Пусть $u\in L$ -- произвольный вектор отличный от $\pr_L v$.
Достаточно показать, что 
\[
\rho(v, u) > \rho (v, \pr_Lv) = |v - \pr_Lv| = |\ort_L v|
\]
Рассмотрим треугольник образованный концами следующих трех векторов: $v$, $\pr_Lv$, $u$.
Сторона $v - \pr_Lv = \ort_L v$ ортогональна стороне $u - \pr_L v \in L$.
Значит $u - v$ -- гипотенуза прямоугольного треугольника.Тогда по теорема Пифагора (утверждение~\ref{claim::Pythagoras})
\[
\rho(v,u) = |u - v| = \sqrt{|\ort_Lv|^2 + |u - \pr_Lv|^2 }> |\ort_Lv|
\]
Последнее неравенство строгое, так как $u\neq \pr_Lv$.

(2) В начале рассмотрим случай $\pr_L v = 0$.
Это значит, что вектор $v$ ортогонален $L$ и угол с любым вектором $\pi/2$.
Теперь предположим, что $\pr_L v \neq 0$.
Выберем произвольный вектор $u\in L$ отличный от $\pr_L v$ и имеющий такую же длину, как $\pr_L v$.
Тогда рассмотрим два треугольника: первый на векторах $0$, $v$, $\pr_Lv$, второй $0$, $v$, $u$.
Тогда у обоих треугольников стороны при вершине $0$ попарно одинаковой длины, а противоположные стороны -- $\ort_L v$ и $u - v$ соответственно.
Но по доказанному выше $|\ort_L v| < |u - v|$.
А школьная геометрия учит нас, что в этом случае угол в первом треугольнике меньше, чем во втором.

\paragraph{формальное доказательство}

Для любителей формализма я приготовил второе доказательство.
Случай $\pr_L v = 0$ разбирается так же как и выше.
Случай $\ort_L v = 0$ означает, что $v\in L$.
В этом случае угол между вектором и пространством нулевой и минимум достигается на $u = v$.
Теперь считаем, что $\pr_L v \neq 0$, $\ort_L v \neq 0$ и выберем произвольный вектор $u\in L$ такой, что $|u| = |\pr_L v|$.
Теперь выберем единичный вектор $e_1$ пропорциональный $\pr_L v$.
Выберем единичный вектор $e_2$ в плоскости $\langle \pr_Lv, u\rangle$ ортогональным $e_1$.
Единичный вектор $e_3$ выберем  пропорциональным $\ort_L v$.
Тогда все интересные нам векторы живут в пространстве $\langle e_1,e_2,e_3\rangle$.
Давайте запишем их в этом базисе
\[
v = 
\begin{pmatrix}
{\lambda}\\{0}\\{\mu}
\end{pmatrix},\quad
\pr_Lv = 
\begin{pmatrix}
{\lambda}\\{0}\\{0}
\end{pmatrix},\quad
u = 
\begin{pmatrix}
{a}\\{b}\\{0}
\end{pmatrix},\quad
\text{причем}\quad
\lambda^2 = a^2 + b^2,\;\lambda > 0
\]
Условие $v = \pr_Lv$ означает $b = 0$ и $\lambda = a$.
Нам надо показать, что $\angle (v, \pr_L v) < \angle (v, u)$.
То есть $\cos \angle (v, \pr_L v) > \cos \angle (v, u)$.
То есть надо показать, что 
\[
\frac{(v, \pr_L v)}{|v||\pr_L v|} > \frac{(v, u)}{|v||u|}
\]
Но так как $|u| = |\pr_Lv|$ по выбору, нам надо доказать, что $(v, \pr_L v) > (v, u)$, то есть, что $\lambda^2 > \lambda a$.
Из условия $\lambda^2 = a^2 + b^2$ видно, что $|\lambda| > |a|$ при $b \neq 0$.
Значит $\lambda^2 > |\lambda a|$ при $b \neq 0$.
При $b = 0$ имеем $a = \lambda$ либо $a = -\lambda$.
Если $u \neq \pr_L v$, то возможно только $a = -\lambda$, но тогда неравенство очевидно.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Обратите внимание, что угол между вектором и подпространством всегда находится в интервале $[0, \pi/2]$ или что то же самое косинус угла всегда неотрицательный.

\item Заметим следующую связь $\angle (v, \pr_L v) + \angle (v, \ort_L v) = \pi/2$.
Причем формула верна даже когда проекция или ортогональная составляющая равны нулю.
В этом случае соответствующий угол надо считать равным $\pi/2$.

\item Предположим, что подпространство $L\subseteq V$ имеет коразмерность $1$, то есть $\dim L = \dim V - 1$.
Тогда у нас $L^\bot$ -- одномерно.
Пусть $n\in L^\bot$ какой-нибудь ненулевой вектор.
Таким образом $n$ -- вектор нормали к $L$.
В этом случае $n$ лежит с $\ort_L v$ на одной прямой (может быть сонаправлен ему или смотреть в противоположную сторону).
Тогда $\angle (v, \ort_L v)$ равен либо $\angle (v, n)$ либо $\angle (v, - n)$ (на самом деле не большему из этих двух).
То есть в этом случае можно взять к $L$ произвольную нормаль $n$, посчитать угол $\alpha$.
Если он оказался больше $\pi/2$ заменить его на $\pi - \alpha$.
После чего найти угол с подпространством как $\pi/2$ минус полученный угол.
\end{itemize}


\subsection{Метод наименьших квадратов}

Пусть мы хотим решить систему $Ax = b$, где $A\in \MatrixDim{m}{n}$, $b\in \mathbb R^m$ и $x\in \mathbb R^n$ -- столбец неизвестных.
И предположим, что система не имеет решений, но от этого наше желание ее решить не становится слабее.
Давайте обсудим, как удовлетворить наши желания в подобной ситуации и когда такие ситуации обычно встречаются.

Введем на пространстве $\mathbb R^m$ стандартное скалярное произведение $(x,y) = x^t y$.
Тогда, на процесс решения системы можно смотреть так: мы подбираем $x\in \mathbb R^n$ так, чтоб $\rho(Ax, b) = 0$.
Если решить систему невозможно, то этот подход подсказывает, как надо поступить.
Надо пытаться минимизировать расстояние между $Ax$ и $b$.
То есть решить задачу
\[
\begin{aligned}
&\rho(Ax, b)\to \min\\
&x\in \mathbb R^n
\end{aligned}
\]
Теперь давайте поймем, как надо решать такую задачу.
Пусть матрица $A$ имеет вид $A = (A_1|\ldots|A_n)$, где $A_i\in \mathbb R^m$ -- ее столбцы.
Тогда система $Ax = b$ означает, $x_1A_1 + \ldots + x_n A_n = b$.
То есть система разрешима тогда и только тогда, когда $b\in \langle A_1,\ldots, A_n\rangle$.
Значит наша задача минимизировать расстояние между $b$ и $\langle A_1,\ldots,A_n\rangle$.
Тогда утверждение~\ref{claim::DistAngle} подсказывает, что минимум расстояния достигается на $b_0 = \pr_{\langle A\rangle}b$.
В этом случае вместо исходной системы $Ax = b$ мы должны решить систему $Ax = b_0$.
И если $x_0$ -- ее решение, то $\rho(Ax_0, b)$ как раз и будет минимальным.

Давайте теперь предположим, что столбцы матрицы $A$ линейно независимы.
Тогда по формуле <<Атата>> мы знаем, что $b_0 = A(A^tA)^{-1}A^tb$.
Кроме этого должно выполняться $b_0 = Ax_0$.
Так как столбцы $A$ линейно независимы, такое $x_0$ должно быть единственным.
Но мы видим, что в качестве $x_0$ подходит $x_0 = (A^tA)^{-1}A^tb$.


\subsection{Матрица Грама}
\label{subsection::Gram}

Давайте поговорим о еще одном объекте, который возникает в связи с конечной системой векторов в Евклидовом пространстве.
Таким объектом является матрица Грама.
Она в частности используется для определения объемов.

\begin{definition}
Пусть $V$ -- евклидово пространство и $v_1,\ldots,v_k\in V$ -- произвольный набор векторов ($k$ НЕ обязательно равно размерности пространства).
Тогда матрица
\[
G(v_1,\ldots,v_k) =
\begin{pmatrix}
{(v_1,v_1)}&{\ldots}&{(v_1,v_k)}\\
{\vdots}&{\ddots}&{\vdots}\\
{(v_k,v_1)}&{\ldots}&{(v_k,v_k)}\\
\end{pmatrix}
\in\Matrix{k}
\]
называется матрицей Грама системы векторов $v_1,\ldots,v_k$.%
\footnote{Обратите внимание, тут важен порядок векторов.
То есть формально матрица Грама зависит от набора $(v_1,\ldots,v_k)\in V^k$.}
\end{definition}

Если $e_1,\ldots,e_n$ -- некоторый базис пространства $V$, то $B = G(e_1,\ldots,e_n)$ -- матрица скалярного произведения заданная в базисе $e_1,\ldots,e_n$.
Таким образом матрица Грама -- это некоторое обобщение матрицы билинейной формы.

Теперь вспомним, что у любой билинейной формы есть операторная запись.
Давайте введем следующее обозначение: для произвольных векторов $w, u\in V$ положим $w\cdot u = (w, u)$.
Тогда для набора $v = (v_1,\ldots,v_k)$ выполнено
\[
G(v_1,\ldots,v_k) = 
\begin{pmatrix}
{v_1}\\{\vdots}\\{v_k}
\end{pmatrix}
\cdot
\begin{pmatrix}
{v_1}&{\ldots}&{v_k}
\end{pmatrix}
=
v^t \cdot v
\]

Пусть теперь $C\in \MatrixDim{k}{r}$ -- некоторая матрица.
Тогда из набора $(v_1,\ldots,v_k)$ можно построить новый набор $(u_1,\ldots,u_r) = (v_1,\ldots,v_k)C$ или кратко $u = vC$.
Тогда 
\[
G(u) = G(vC) = (vC)^t \cdot vC = C^t v^t \cdot v C = C^t G(v) C
\]
То есть $G((v_1,\ldots,v_k)C) = C^t G(v_1,\ldots,v_k)C$.


\begin{claim}
\label{claim::GramMatrixFull}
Пусть $v_1,\ldots,v_k\in V$ -- произвольный набор векторов в евклидовом пространстве.
Тогда
\begin{enumerate}
\item Пусть $\alpha_1,\ldots,\alpha_k \in \mathbb R$, введем обозначение $\alpha = (\alpha_1,\ldots,\alpha_k)^t$.
Тогда следующие условия эквивалентны:
\begin{enumerate}
\item $\alpha_1 v_1 + \ldots + \alpha_k v_k = 0$.

\item $G(v_1,\ldots,v_k)\alpha= 0$.

\item $\alpha^tG(v_1,\ldots,v_k)\alpha= 0$.
\end{enumerate}

\item $\rk G(v_1,\ldots,v_k) = \dim \langle v_1,\ldots,v_k\rangle$.

\item  $\det G(v_1,\ldots,v_k)\geqslant 0$.
При этом равенство достигается тогда и только тогда, когда векторы линейно зависимы.

\item Если $C\in\Matrix{k}$ является матрицей элементарного преобразования I или II типа, то 
\[
\det G(v_1,\ldots,v_k) = \det G((v_1,\ldots,v_k)C)
\]
\end{enumerate}
\end{claim}
\begin{proof}
1) Пусть $v = (v_1,\ldots,v_k)$.
Тогда условие $v\alpha = 0$ влечет, что $v^t \cdot v\alpha = 0$.
Но это означает, что $G(v_1,\ldots,v_k) \alpha = 0$.
Домножая слева на $\alpha^t$ получаем, что $\alpha^t G(v_1,\ldots,v_k) \alpha = 0$.
Осталось показать из последнего в первое.
Для этого заметим, что $0 = \alpha^t G(v_1,\ldots,v_k) \alpha = \alpha^t v^t \cdot v\alpha = (v\alpha, v\alpha)$.
А значит $v\alpha = 0$.

2) Пункт~(1) эквивалентность (a) и (b) означает, что векторы $v_1,\ldots,v_k$ обладают теми же линейными зависимостями, что и столбцы матрицы $G(v_1,\ldots,v_k)$.
В частности столбцовый ранг $G(v_1,\ldots,v_k)$ равен рангу системы векторов $(v_1,\ldots,v_k)$.
А последняя совпадает с размерностью линейной оболочки $\langle v_1,\ldots,v_k\rangle$.

3) Давайте на пространстве $\mathbb R^k$ рассмотрим билинейную форму $\beta(x,y) = x^t G(v_1,\ldots,v_k)y$.
Давайте покажем, что она не отрицательно определена.
Тогда, ее определитель должен быть не отрицательным.
Действительно, тогда в каком-то базисе ее матрица диагональна с $1$ и $0$ на диагонали, а значит в этом базисе определитель будет неотрицательным.
Но определитель билинейной формы не меняет знак при замене базиса (раздел~\ref{subsection::BilChar}).
Теперь остается лишь проверить, что $\beta(x,x)\geqslant 0$ для любого $x\in \mathbb R^k$.
Действительно
\[
x^t G(v_1,\ldots,v_k) x = x^t v^t\cdot v x = (vx, vx) \geqslant 0
\]
Последнее неравенство выполнено для любого вектора $vx\in V$ по определению скалярного произведения.

4) Если $C$ -- матрица элементарного преобразования, то $G((v_1,\ldots,v_k)C) = C^t G(v_1,\ldots,v_k)C$.
А значит $\det(G((v_1,\ldots,v_k)C)) = \det(G(v_1,\ldots,v_k))\det C^2$.
Но для элементарных преобразований I и II типов $\det C = \pm 1$.
\end{proof}

Заметим, что из третьего пункта следует вот какое наблюдение.
Если $v_1,\ldots, v_k$ -- линейно независимый набор векторов, из которого процессом ортогонализации Грама-Шмидта мы получили набор $u_1,\ldots,u_k$, то $\det G(u_1,\ldots,u_k) = \det G(v_1,\ldots,v_k)$.


