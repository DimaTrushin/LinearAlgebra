\ProvidesFile{lecture24.tex}[Лекция 24]


\subsection{Матричные характеристики билинейной формы}
\label{subsection::BilChar}

В случае линейного отображения или оператора мы поступали так: выбирали базисы (или базис) и задавали их матрицами.
Потом считали какие-то характеристики этих самых матриц и показывали, что они не зависят от базисов.
В случае линейных отображений между разными пространствами у нас по сути была только одна характеристика -- ранг.
Оказывается, что для билинейных форм на паре разных пространств это тоже корректная характеристика.

\paragraph{Ранг}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и в каких то парах базисов она задана матрицами $B$ и $B'$.
Тогда $B' = C^t B D$ для некоторых невырожденных матриц $C$ и $D$.
Тогда $\rk B' = \rk B$, так как он не меняется при умножении слева и справа на невырожденную матрицу (утверждение~\ref{claim::rkInvariance}).

\subsection{Ортогональные дополнения и ядра}

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма.
Тогда
\begin{itemize}
\item Если $X\subseteq V$ -- произвольное подмножество, тогда правое ортогональное дополнение к $X$ это
\[
X^\bot = \{u\in U \mid \beta(x,u) = 0\,\forall x\in X\} = \{u\in U\mid \beta(X,u) = 0\}
\]

\item Если $Y\subseteq U$ -- произвольное подмножество, тогда левое ортогональное дополнение к $Y$ это
\[
{}^\bot Y = \{v\in V\mid \beta(v, y) = 0\,\forall y\in Y\} = \{v\in V\mid \beta(v, Y) = 0\}
\]
\end{itemize}
\end{definition}

\paragraph{Замечания}

\begin{itemize}
\item
Когда понятно о чем идет речь и нет путаницы, обычно оба дополнения обозначают $X^\bot$ и $X^\bot$.
Обычно это не мешает если пространства $V$ и $U$ разные, так как каждое дополнение живет в своем отдельном пространстве.
Однако, если форма определена на одном пространстве $\beta\colon V\times V\to F$, то приходится использовать разные обозначения, так как для $X\subseteq V$ определены оба дополнения $X^\bot$ и ${}^\bot X$ и оба живут в $V$.
Я постараюсь различать дополнения там, где это необходимо.

\item 
Стоит отметить, что ортогональное дополнение $X^\bot$ к подмножеству $X\subseteq V$ обязательно будет подпространством в $U$, аналогично и для второго дополнения.
\end{itemize}

\paragraph{Пример}

Давайте выясним как считать левое и правое ортогональные дополнения к подпространству.
Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_{n\,m}(F)$.
Пусть $W = \langle w_1,\ldots,w_s\rangle\subseteq F^n$ -- некоторое подпространство заданное в виде линейной оболочки.
Положим $T = (w_1|\ldots|w_s)\in\operatorname{M}_{n\,s}(F)$ -- матрица из столбцов $w_i$.
Тогда 
\[
W^\bot = \{y\in F^m \mid T^t By = 0\}
\]
Аналогично, если $U=\langle u_1,\ldots,u_r \rangle \subseteq F^m$ -- подпространство и $P = (u_1|\ldots|u_r)$ -- матрица из столбцов $u_i$.
То
\[
{}^\bot U = \{x\in F^n\mid x^t B P = 0\} = 
\{x\in F^n \mid P^t B^t x = 0\}
\]

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда ее правым ядром называется $\ker^R \beta =V^\bot$, а левым ядром $\ker^L\beta = {}^\bot U$.
\end{definition}

Смысл левого ядра в том, что это такие векторы из $V$, которые на что из $U$ ни умножай, все равно получишь $0$.
В этом смысле -- это не интересные векторы, изучение которых с точки зрения билинейной форм невозможно.
Аналогично с правым ядром.

\paragraph{Пример}

Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная правилом $\beta(x,y) = x^t By$, где $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
$\ker^R \beta = \{y\in F^m\mid By = 0\}$ и $\ker^L\beta = \{x\in F^n \mid x^t B = 0\} = \{x\in F^n \mid B^t x = 0\}$.

\begin{definition}
Билинейная форма $\beta\colon V\times U\to F$ называется невырожденной, если $\ker^R\beta = 0$ и $\ker^L\beta = 0$.
\end{definition}


\begin{claim}
Билинейная форма $\beta\colon V\times U\to F$ невырождена тогда и только тогда, когда $\dim V = \dim U$ и матрица формы $\beta$ невырождена.
\end{claim}
\begin{proof}
Пусть форма $\beta$ невырожденная.
Выберем базисы в $V$ и $U$, тогда наша билинейная форма превратится в $\beta\colon F^n \times F^m\to F$ по правилу $(x,y)\mapsto x^t B y$ для некоторой матрицы $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
\[
\ker^R \beta = \{y\in F^m\mid By = 0\}\quad\text{и}\quad \ker^L\beta = \{x\in F^n \mid B^t x = 0\}
\]
Если размерности пространств разные, то матрица $B$ не квадратная и хотя бы одно из ядер не ноль, так как хотя бы одна из систем $B y = 0$ или $B^t x = 0$ содержит переменных больше чем уравнений, а значит есть ненулевое решение.
Теперь мы знаем, что $B$ квадратная и система $By = 0$ имеет только нулевые решения, значит $B$ -- невырожденная матрица по утверждению~\ref{claim::InvertibleDiscription}.

Обратно, пусть $\dim V = \dim U$ и матрица $B_\beta$ не вырождена.
Тогда в каких-то координатах $\beta$ записывается так $\beta\colon F^n \times F^n \to F$ по правилу $(x,y)\mapsto x^t By$.
Так как $B$ невырожденная, то системы $B y = 0$ и $B^t x = 0$ имеют только нулевые решения, значит оба ядра нулевые, значит форма невырожденная.
\end{proof}

\begin{claim}
\label{claim::BilinearKernels}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда:%
\footnote{Это утверждение является прямым аналогом утверждения~\ref{claim::ImKer} для линейных отображений и является очередным проявлением тривиального наблюдения для систем линейных уравнений, что количество главных и свободных переменных равно количеству всех переменных.}
\begin{enumerate}
\item $\dim\ker^L \beta + \rk \beta = \dim V$

\item $\dim\ker^R \beta + \rk \beta = \dim U$
\end{enumerate}
\end{claim}
\begin{proof}
Докажем для определенности первое утверждение, другое ему симметрично.
Записав все в координатах, мы имеем $\beta \colon F^n \times F^m \to F$ по правилу $\beta(x,y) = x^t By$.
И для левого ядра мы имеем $\ker^L \beta = \{x\in F^n \mid B^t x = 0\}$.
Тогда $\dim \ker^L\beta$ -- количество свободных переменных системы $B^t x=0$ (раздел~\ref{section::Subspaces} о ФСР), $\rk \beta$ -- количество главных переменных системы $B^t x = 0$ (совпадает со строчным рангом $B^t$), а $\dim V $ -- количество переменных системы $B^t x = 0$.
Ну а количество главных плюс количество свободных переменных -- это все переменные.
\end{proof}

\subsection{Двойственность для подпространств}

\begin{claim}
\label{claim::DualitySpaces}
Пусть $\beta\colon V\times U\to F$ -- невырожденная билинейная форма.
Тогда:
\begin{enumerate}
\item Для любого подпространства $W\subseteq V$ выполнено
\[
\dim W^\bot + \dim W = \dim V
\]

\item Для любого подпространства $W\subseteq V$ выполнено ${}^\bot(W^\bot) = W$.

\item Для любых подпространств $W\subseteq E\subseteq V$ верно, что $W^\bot \supseteq E^\bot$.
Причем $W = E$ тогда и только тогда, когда $W^\bot = E^\bot$.

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W + E)^\bot = W^\bot \cap E^\bot
\]

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W\cap E)^\bot = W^\bot + E^\bot
\]
\end{enumerate}
Аналогично выполнены все свойства для подпространств $W\subseteq U$ и их левых ортогональных дополнений ${}^\bot W$.
\end{claim}
\begin{proof}
Давайте прежде всего перейдем в координаты выбрав какой-нибудь базис $V$ и $U$.
Тогда получим $\beta\colon F^n \times F^n \to F$ по правилу $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_n(F)$ -- невырожденная матрица.

(1) Пусть $W = \langle w_1,\ldots,w_r \rangle$ задано своим базисом и $T = (w_1|\ldots|w_r)\in \operatorname{M}_{n\,r}(F)$.
Тогда $W^\bot = \{y\in F^n \mid T^tB y = 0\}$.
Так как матрица $B$ невырожденная, то $\rk(T^t B) = \rk (T^t) = r$ (утверждение~\ref{claim::rkInvariance}).
В очередной раз все интерпретируем в терминах свободных и главных переменных системы $T^t By = 0$.
Имеем: $\dim W = r$ -- это количество главных переменных системы, $\dim W^\bot$ -- это количество свободных переменных, а $\dim V$ -- это количество всех переменных, что и требовалось.

(2) Давайте в начале покажем, что $W \subseteq {}^\bot(W^\bot)$, а потом сравним их размерности.
Пусть $w\in W$, нам надо показать, что $w\in {}^\bot (W^\bot)$.
То есть нам надо показать, что $\beta(w, W^\bot) = 0$.
То есть для любого $v\in W^\bot$ надо показать, что $\beta(w,v) = 0$.
Однако, по определению, если $v\in W^\bot$, то $\beta(w, v) = 0$ для любого $w\in W$, что и требовалось.
Теперь надо показать, что пространства имеют одинаковую размерность.
Для этого воспользуемся пунктом~(1):
\[
\dim {}^\bot (W^\bot) = \dim U - \dim W^\bot = \dim U- (\dim V - \dim W) = \dim W
\]
последнее равенство в силу того, что $\dim U = \dim V$.
А раз пространства вложены и имеют одинаковую размерность, то они совпадают.

(3) Пусть $W\subseteq E\subseteq V$, тогда
\[
W^\bot = \{u\in U\mid \beta(w, u) = 0,\,w\in W\} \quad \text{и} \quad E^\bot = \{u\in U\mid \beta(e, u) = 0,\,e\in E\}
\]
Заметим, что так как $W\subseteq E$, то справа ограничений не меньше, чем слева, а значит пространство не больше.

Пусть теперь $E, W\subseteq V$ -- произвольные подпространства.
Тогда если они равны, то и их ортогональные дополнения равны.
Обратно, пусть $W^\bot = E^\bot$, тогда ${}^\bot(W^\bot) = {}^\bot(E^\bot)$, то есть по пункту~(2) $W = E$.

(4) Рассмотрим левую и правую части равенства $(W + E)^\bot = W^\bot\cap E^\bot$ отдельно:
\[
(W+E)^\bot = \{u\in U\mid \beta(w + e, u) = 0,\,\forall w\in W,\,\forall e\in E\}
\]
С другой стороны
\begin{gather*}
W^\bot\cap E^\bot = \{u\in U\mid \beta(w, u) = 0,\,\forall w\in W\}\cap \{u\in U\mid \beta(e,u) = 0,\,\forall e\in E\} =\\
\{u\in U\mid \beta(w,u) = 0,\forall w\in W\text{ и }\beta(e,u)=0,\,\forall e\in E\}
\end{gather*}
Если $u\in W^\bot\cap E^\bot$, то $\beta(w,u) = 0$ и $\beta(e,u) = 0$ для любых $w\in W$ и $e\in E$, а значит и $\beta(w + e, u) = 0$, то есть $u\in (W+E)^\bot$.
Обратно, если $u\in (W+E)^\bot$, то $\beta(w+e,u) = 0$ для любых $w\in W$ и $e\in E$.
В частности для любого $w\in W$ и $e = 0\in E$ получаем $\beta(w, u) = 0$, аналогично $w = 0\in W$ и любого $e \in E$ получаем $\beta(e, u) = 0$.
То есть $u\in W^\bot\cap E^\bot$.

(5) Выведем это утверждение из предыдущего с помощью остальных.
Действительно, чтобы доказать равенство $(W\cap E)^\bot = W^\bot + E^\bot$, необходимо и достаточно доказать ${}^\bot((W\cap E)^\bot) = {}^\bot(W^\bot + E^\bot)$ по пункту~(3) вторая часть.
В силу~(2) это равносильно $W\cap E = {}^\bot(W^\bot+E^\bot)$.
По пункту~(4) для левых ортогональных дополнений получаем, что правая часть совпадает с ${}^\bot(W^\bot) \cap {}^\bot(E^\bot)$.
И опять воспользовавшись~(2), получаем $W\cap E$, то есть левую часть.
\end{proof}

К этому утверждению надо относиться так.
Процедура взятия ортогонального дополнения <<переворачивает>> множество подпространств <<вверх ногами>>, меняет размерность на <<коразмерность>>%
\footnote{Для подпространства $U\subseteq V$ его коразмерность -- это $\dim V - \dim U$.}%
, обращает включения и меняет местами сумму и пересечение.
Это один из способов переформулировать задачи про подпространства и сводить одни к другим.
Если у вас есть задача на пересечение подпространств, то перейдя к ортогональным дополнениям, вы получаете эквивалентную задачу на сумму подпространств и решить ее -- то же самое, что решить исходную задачу.
Например, алгоритмы на поиск суммы и пересечения подпространств заданных порождающими, можно превратить в алгоритмы на поиск суммы и пересечения подпространств заданных системами, применив переход к ортогональному дополнению.
