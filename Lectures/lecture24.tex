\ProvidesFile{lecture24.tex}[Лекция 24]


\paragraph{Матричный формализм}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и пусть $v = (v_1,\ldots,v_s)$ -- некоторый набор векторов из $V$ и $u = (u_1,\ldots,u_t)$ -- набор векторов из $U$.
Тогда рассмотрим следующую конструкцию
\[
v^t\cdot_\beta u = 
\begin{pmatrix}
{v_1}\\{\vdots}\\{v_s}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{u_1}&{\ldots}&{u_t}
\end{pmatrix}
=
\begin{pmatrix}
{v_1\cdot_\beta u_1}&{\ldots}&{v_1\cdot_\beta u_t}\\
{\vdots}&{\ddots}&{\vdots}\\
{v_s\cdot_\beta u_1}&{\ldots}&{v_s\cdot_\beta u_t}\\
\end{pmatrix}
\]
То есть мы умножаем столбец из векторов из $V$ на строку из векторов из $U$ с помощью билинейной формы, рассматриваемой как оператор умножения.
Тогда результат будет матрица из $\operatorname{M}_{s\,t}(F)$.
Причем умножение происходит по тем же самым формальным правилам, что и обычное матричное умножение, только с использованием $\cdot_\beta$ вместо обычного умножения (которое для векторов даже не определено).

Тогда, если выбрать $e = (e_1,\ldots,e_n)$ -- базис $V$ и $f = (f_1,\ldots,f_m)$ -- базис $U$, то 
\[
B_\beta = e^t\cdot_\beta f = 
\begin{pmatrix}
{e_1}\\{\vdots}\\{e_n}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{f_1}&{\ldots}&{f_m}
\end{pmatrix}
\]
Мы привыкли, что в случае линейных отображений вычисления можно вести в удобной матричной форме.
Последнее равенство позволяет вычисления с билинейными формами сводить к матричной.

\begin{claim}
Пусть $V$ и $U$ -- векторные пространства над полем $F$, $e_1,\ldots,e_n \in V$ -- базис $V$ и $f_1,\ldots,f_m\in U$ -- базис $U$.
Тогда для любого набора чисел $b_{ij}\in F$, где $1\leqslant i \leqslant n$ и $1\leqslant j \leqslant m$, существует единственная билинейная форма $\beta\colon V\times U\to F$ такая, что $\beta(e_i,f_j) = b_{ij}$.
\end{claim}
\begin{proof}
По сути -- это переформулировка утверждения~\ref{claim::BilinearMatrices}.
\end{proof}

\begin{claim}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма.
Пусть в пространстве $V$ зафиксировано два базиса $e=(e_1,\ldots,e_n)$ и $e' = (e_1',\ldots,e_n')$ с матрицей перехода $C\in \operatorname{M}_n(F)$ такой, что $e'=eC$, пусть в пространстве $U$ также зафиксированы два базиса $f = (f_1,\ldots,f_m)$ и $f'=(f_1',\ldots,f_m')$ с матрицей перехода $D\in \operatorname{M}_m(F)$ такой, что $f' = fD$.
Если $B_\beta$ -- матрица $\beta$ в базисах $e$ и $f$ и $B_\beta'$ -- матрица $\beta$ в базисах $e'$ и $f'$, тогда $B_\beta' = C^t B_\beta D$.
\end{claim}
\begin{proof}
Пользуясь только что введенным формализмом можно проделать следующие вычисления:%
\footnote{Обратите внимание, что тут у нас присутствует два умножения: матричное с числами и матричное с билинейной формой.
Порядок этих операций (то есть расстановка скобок) не важны, это следует просто из определения билинейной формы, если присмотреться внимательно.}
\[
B_\beta' = (e')^t\cdot_\beta f' = (eC)^t \cdot_\beta (f D) = (C^t e^t) \cdot_\beta (f D) = C^t(e^t \cdot_\beta f) D = C^t B_\beta D
\]
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Заметим, что если билинейная форма определена на одном пространстве $\beta\colon V\times V\to F$, то достаточно выбрать один базис $e=(e_1,\ldots,e_n)$, после чего коэффициенты $B_\beta$ считаются по правилу $b_{ij} = \beta(e_i,e_j)$.
При этом, если $e'=(e_1',\ldots,e_n')$ -- другой базис и $e'=eC$, где $C$ -- матрица перехода, то $B_\beta' = C^t B C$.

\item Пусть у нас есть два векторных пространства $V$ и $U$.
Тогда на них могут жить два разного рода объектов: линейные отображения и билинейные формы, например, $\phi\colon V\to U$ и $\beta\colon U\times V \to F$.
Если мы выберем базис в $V$ и базис в $U$, то $V$ превращается в $F^n$, а $U$ -- в $F^m$.
В этом случае, линейное отображение $\phi$ описывается некоторой матрицей $A\in\operatorname{M}_{m\,n}(F)$, при этом $\phi(x) = Ax$.
С другой стороны, билинейная форма тоже описывается матрицей $B\in \operatorname{M}_{m\,n}(F)$, при этом $\beta(x,y) = x^tBy$.

Таким образом, для описания и линейных отображений и билинейных форм в фиксированном базисе мы используем матрицы (причем одного и того же размера).
Возникает вопрос: <<а как понять,  когда матрица задает линейное отображение, а когда билинейную форму?>> Если  нам выдали только одну пару базисов и матрицу $S$, то ответ простой -- никак.
В фиксированном базисе они не отличимы.
Мы можем считать нашу матрицу $S$ линейным оператором или билинейной формой, в зависимости от наших предпочтений.
Однако, если нам выдали несколько базисов, например два, и в этих базисах наш объект задается матрицами $S$ и $S'$.
То отличить оператор от билинейной формы можно по формуле перехода, а именно, если задан оператор, то $S' = C^{-1}S D$, а если билинейная форма, то $S' = C^t S D$.
Конечно, если базисы трепетно подобраны (врагом или другом -- это как повезет), то мы все равно можем не заметить разницы.
Но если мы будем сравнивать во всех возможных базисах, то ответ определяется однозначно.
\end{itemize}

\subsection{Матричные характеристики билинейной формы}
\label{subsection::BilChar}

В случае линейного отображения или оператора мы поступали так: выбирали базисы (или базис) и задавали их матрицами.
Потом считали какие-то характеристики этих самых матриц и показывали, что они не зависят от базисов.
В случае линейных отображений между разными пространствами у нас по сути была только одна характеристика -- ранг.
Оказывается, что для билинейных форм на паре разных пространств это тоже корректная характеристика.

\paragraph{Ранг}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и в каких то парах базисов она задана матрицами $B$ и $B'$.
Тогда $B' = C^t B D$ для некоторых невырожденных матриц $C$ и $D$.
Тогда $\rk B' = \rk B$, так как он не меняется при умножении слева и справа на невырожденную матрицу (утверждение~\ref{claim::rkInvariance}).

\subsection{Ортогональные дополнения и ядра}

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма.
Тогда
\begin{itemize}
\item Если $X\subseteq V$ -- произвольное подмножество, тогда правое ортогональное дополнение к $X$ это
\[
X^\bot = \{u\in U \mid \beta(x,u) = 0\,\forall x\in X\} = \{u\in U\mid \beta(X,u) = 0\}
\]

\item Если $Y\subseteq U$ -- произвольное подмножество, тогда левое ортогональное дополнение к $Y$ это
\[
{}^\bot Y = \{v\in V\mid \beta(v, y) = 0\,\forall y\in Y\} = \{v\in V\mid \beta(v, Y) = 0\}
\]
\end{itemize}
\end{definition}

\paragraph{Замечания}

\begin{itemize}
\item
Когда понятно о чем идет речь и нет путаницы, обычно оба дополнения обозначают $X^\bot$ и $X^\bot$.
Обычно это не мешает если пространства $V$ и $U$ разные, так как каждое дополнение живет в своем отдельном пространстве.
Однако, если форма определена на одном пространстве $\beta\colon V\times V\to F$, то приходится использовать разные обозначения, так как для $X\subseteq V$ определены оба дополнения $X^\bot$ и ${}^\bot X$ и оба живут в $V$.
Я постараюсь различать дополнения там, где это необходимо.

\item 
Стоит отметить, что ортогональное дополнение $X^\bot$ к подмножеству $X\subseteq V$ обязательно будет подпространством в $U$, аналогично и для второго дополнения.
\end{itemize}


\paragraph{Пример}

Давайте выясним как считать левое и правое ортогональные дополнения к подпространству.
Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_{n\,m}(F)$.
Пусть $W = \langle w_1,\ldots,w_s\rangle\subseteq F^n$ -- некоторое подпространство заданное в виде линейной оболочки.
Положим $T = (w_1|\ldots|w_s)\in\operatorname{M}_{n\,s}(F)$ -- матрица из столбцов $w_i$.
Тогда 
\[
W^\bot = \{y\in F^m \mid T^t By = 0\}
\]
Аналогично, если $U=\langle u_1,\ldots,u_r \rangle \subseteq F^m$ -- подпространство и $P = (u_1|\ldots|u_r)$ -- матрица из столбцов $u_i$.
То
\[
{}^\bot U = \{x\in F^n\mid x^t B P = 0\} = 
\{x\in F^n \mid P^t B^t x = 0\}
\]


\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда ее правым ядром называется $\ker^R \beta =V^\bot$, а левым ядром $\ker^L\beta = {}^\bot U$.
\end{definition}

Смысл левого ядра в том, что это такие векторы из $V$, которые на что из $U$ ни умножай, все равно получишь $0$.
В этом смысле -- это не интересные векторы, изучение которых с точки зрения билинейной форм невозможно.
Аналогично с правым ядром.

\paragraph{Пример}

Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная правилом $\beta(x,y) = x^t By$, где $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
$\ker^R \beta = \{y\in F^m\mid By = 0\}$ и $\ker^L\beta = \{x\in F^n \mid x^t B = 0\} = \{x\in F^n \mid B^t x = 0\}$.

\begin{definition}
Билинейная форма $\beta\colon V\times U\to F$ называется невырожденной, если $\ker^R\beta = 0$ и $\ker^L\beta = 0$.
\end{definition}


\begin{claim}
Билинейная форма $\beta\colon V\times U\to F$ невырождена тогда и только тогда, когда $\dim V = \dim U$ и матрица формы $\beta$ невырождена.
\end{claim}
\begin{proof}
Пусть форма $\beta$ невырожденная.
Выберем базисы в $V$ и $U$, тогда наша билинейная форма превратится в $\beta\colon F^n \times F^m\to F$ по правилу $(x,y)\mapsto x^t B y$ для некоторой матрицы $B\in \operatorname{M}_{n\,m}(F)$.
Тогда
\[
\ker^R \beta = \{y\in F^m\mid By = 0\}\quad\text{и}\quad \ker^L\beta = \{x\in F^n \mid B^t x = 0\}
\]
Если размерности пространств разные, то матрица $B$ не квадратная и хотя бы одно из ядер не ноль, так как хотя бы одна из систем $B y = 0$ или $B^t x = 0$ содержит переменных больше чем уравнений, а значит есть ненулевое решение.
Теперь мы знаем, что $B$ квадратная и система $By = 0$ имеет только нулевые решения, значит $B$ -- невырожденная матрица по утверждению~\ref{claim::InvertibleDiscription}.

Обратно, пусть $\dim V = \dim U$ и матрица $B_\beta$ не вырождена.
Тогда в каких-то координатах $\beta$ записывается так $\beta\colon F^n \times F^n \to F$ по правилу $(x,y)\mapsto x^t By$.
Так как $B$ невырожденная, то системы $B y = 0$ и $B^t x = 0$ имеют только нулевые решения, значит оба ядра нулевые, значит форма невырожденная.
\end{proof}

\begin{claim}
\label{claim::BilinearKernels}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда:%
\footnote{Это утверждение является прямым аналогом утверждения~\ref{claim::ImKer} для линейных отображений и является очередным проявлением тривиального наблюдения для систем линейных уравнений, что количество главных и свободных переменных равно количеству всех переменных.}
\begin{enumerate}
\item $\dim\ker^L \beta + \rk \beta = \dim V$

\item $\dim\ker^R \beta + \rk \beta = \dim U$
\end{enumerate}
\end{claim}
\begin{proof}
Докажем для определенности первое утверждение, другое ему симметрично.
Записав все в координатах, мы имеем $\beta \colon F^n \times F^m \to F$ по правилу $\beta(x,y) = x^t By$.
И для левого ядра мы имеем $\ker^L \beta = \{x\in F^n \mid B^t x = 0\}$.
Тогда $\dim \ker^L\beta$ -- количество свободных переменных системы $B^t x=0$ (раздел~\ref{section::Subspaces} о ФСР), $\rk \beta$ -- количество главных переменных системы $B^t x = 0$ (совпадает со строчным рангом $B^t$), а $\dim V $ -- количество переменных системы $B^t x = 0$.
Ну а количество главных плюс количество свободных переменных -- это все переменные.
\end{proof}

\subsection{Двойственность для подпространств}

\begin{claim}
\label{claim::DualitySpaces}
Пусть $\beta\colon V\times U\to F$ -- невырожденная билинейная форма.
Тогда:
\begin{enumerate}
\item Для любого подпространства $W\subseteq V$ выполнено
\[
\dim W^\bot + \dim W = \dim V
\]

\item Для любого подпространства $W\subseteq V$ выполнено ${}^\bot(W^\bot) = W$.

\item Для любых подпространств $W\subseteq E\subseteq V$ верно, что $W^\bot \supseteq E^\bot$.
Причем $W = E$ тогда и только тогда, когда $W^\bot = E^\bot$.

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W + E)^\bot = W^\bot \cap E^\bot
\]

\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W\cap E)^\bot = W^\bot + E^\bot
\]
\end{enumerate}
Аналогично выполнены все свойства для подпространств $W\subseteq U$ и их левых ортогональных дополнений ${}^\bot W$.
\end{claim}
\begin{proof}
Давайте прежде всего перейдем в координаты выбрав какой-нибудь базис $V$ и $U$.
Тогда получим $\beta\colon F^n \times F^n \to F$ по правилу $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_n(F)$ -- невырожденная матрица.

(1) Пусть $W = \langle w_1,\ldots,w_r \rangle$ задано своим базисом и $T = (w_1|\ldots|w_r)\in \operatorname{M}_{n\,r}(F)$.
Тогда $W^\bot = \{y\in F^n \mid T^tB y = 0\}$.
Так как матрица $B$ невырожденная, то $\rk(T^t B) = \rk (T^t) = r$ (утверждение~\ref{claim::rkInvariance}).
В очередной раз все интерпретируем в терминах свободных и главных переменных системы $T^t By = 0$.
Имеем: $\dim W = r$ -- это количество главных переменных системы, $\dim W^\bot$ -- это количество свободных переменных, а $\dim V$ -- это количество всех переменных, что и требовалось.

(2) Давайте в начале покажем, что $W \subseteq {}^\bot(W^\bot)$, а потом сравним их размерности.
Пусть $w\in W$, нам надо показать, что $w\in {}^\bot (W^\bot)$.
То есть нам надо показать, что $\beta(w, W^\bot) = 0$.
То есть для любого $v\in W^\bot$ надо показать, что $\beta(w,v) = 0$.
Однако, по определению, если $v\in W^\bot$, то $\beta(w, v) = 0$ для любого $w\in W$, что и требовалось.
Теперь надо показать, что пространства имеют одинаковую размерность.
Для этого воспользуемся пунктом~(1):
\[
\dim {}^\bot (W^\bot) = \dim U - \dim W^\bot = \dim U- (\dim V - \dim W) = \dim W
\]
последнее равенство в силу того, что $\dim U = \dim V$.
А раз пространства вложены и имеют одинаковую размерность, то они совпадают.

(3) Пусть $W\subseteq E\subseteq V$, тогда
\[
W^\bot = \{u\in U\mid \beta(w, u) = 0,\,w\in W\} \quad \text{и} \quad E^\bot = \{u\in U\mid \beta(e, u) = 0,\,e\in E\}
\]
Заметим, что так как $W\subseteq E$, то справа ограничений не меньше, чем слева, а значит пространство не больше.

Пусть теперь $E, W\subseteq V$ -- произвольные подпространства.
Тогда если они равны, то и их ортогональные дополнения равны.
Обратно, пусть $W^\bot = E^\bot$, тогда ${}^\bot(W^\bot) = {}^\bot(E^\bot)$, то есть по пункту~(2) $W = E$.

(4) Рассмотрим левую и правую части равенства $(W + E)^\bot = W^\bot\cap E^\bot$ отдельно:
\[
(W+E)^\bot = \{u\in U\mid \beta(w + e, u) = 0,\,\forall w\in W,\,\forall e\in E\}
\]
С другой стороны
\begin{gather*}
W^\bot\cap E^\bot = \{u\in U\mid \beta(w, u) = 0,\,\forall w\in W\}\cap \{u\in U\mid \beta(e,u) = 0,\,\forall e\in E\} =\\
\{u\in U\mid \beta(w,u) = 0,\forall w\in W\text{ и }\beta(e,u)=0,\,\forall e\in E\}
\end{gather*}
Если $u\in W^\bot\cap E^\bot$, то $\beta(w,u) = 0$ и $\beta(e,u) = 0$ для любых $w\in W$ и $e\in E$, а значит и $\beta(w + e, u) = 0$, то есть $u\in (W+E)^\bot$.
Обратно, если $u\in (W+E)^\bot$, то $\beta(w+e,u) = 0$ для любых $w\in W$ и $e\in E$.
В частности для любого $w\in W$ и $e = 0\in E$ получаем $\beta(w, u) = 0$, аналогично $w = 0\in W$ и любого $e \in E$ получаем $\beta(e, u) = 0$.
То есть $u\in W^\bot\cap E^\bot$.

(5) Выведем это утверждение из предыдущего с помощью остальных.
Действительно, чтобы доказать равенство $(W\cap E)^\bot = W^\bot + E^\bot$, необходимо и достаточно доказать ${}^\bot((W\cap E)^\bot) = {}^\bot(W^\bot + E^\bot)$ по пункту~(3) вторая часть.
В силу~(2) это равносильно $W\cap E = {}^\bot(W^\bot+E^\bot)$.
По пункту~(4) для левых ортогональных дополнений получаем, что правая часть совпадает с ${}^\bot(W^\bot) \cap {}^\bot(E^\bot)$.
И опять воспользовавшись~(2), получаем $W\cap E$, то есть левую часть.
\end{proof}

К этому утверждению надо относиться так.
Процедура взятия ортогонального дополнения <<переворачивает>> множество подпространств <<вверх ногами>>, меняет размерность на <<коразмерность>>%
\footnote{Для подпространства $U\subseteq V$ его коразмерность -- это $\dim V - \dim U$.}%
, обращает включения и меняет местами сумму и пересечение.
Это один из способов переформулировать задачи про подпространства и сводить одни к другим.
Если у вас есть задача на пересечение подпространств, то перейдя к ортогональным дополнениям, вы получаете эквивалентную задачу на сумму подпространств и решить ее -- то же самое, что решить исходную задачу.
Например, алгоритмы на поиск суммы и пересечения подпространств заданных порождающими, можно превратить в алгоритмы на поиск суммы и пересечения подпространств заданных системами, применив переход к ортогональному дополнению.


\subsection{Двойственность для линейных отображений и операторов}

В утверждении ниже, мы предполагаем, что все ортогональные дополнения берутся относительно естественной билинейной формы, то есть $\langle, \rangle\colon V^*\times V\to F$, где $(\xi, v)\mapsto \langle \xi, v\rangle := \xi(v)$.
В данном случае, я не буду различать левые и правые ортогональные дополнения, так как понятно, в каком пространстве находятся соответствующие подпространства.
В частности, если $W\subseteq V$, то $W^\bot\subseteq V^*$ и наоборот, если $W\subseteq V^*$, то $W^\bot \subseteq V$.

Давайте сделаю еще одно полезное замечание относительно изоморфизма $\phi\colon V\to V^{**}$.
Посмотрим на следующую диаграмму
\[
\xymatrix@R=5pt{
	{V^*\times V}\ar[dd]_{\Identity\times \phi}\ar[rd]&{}\\
	{}&{F}\\
	{V^*\times V^{**}}\ar[ru]&{}\\
}
\quad
\xymatrix@R=5pt{
	{(\xi, v)}\ar@{|->}[dd]\ar@{|->}[rrd]&{}&{}\\
	{}&{\phi_v(\xi)}\ar@{=}[r]&{\xi(v)}\\
	{(\xi,\phi_v)}\ar@{|->}[ru]&{}&{}\\
}
\]
Здесь горизонтальные стрелки -- это естественные билинейные формы применения функции к вектору.
Справа показано, как стрелки действуют на элементах.
И равенство показывает, что диаграмма коммутативна.
А это означает, что если мы возьмем подпространство $W\subseteq V^*$, то мы можем проделать с ним две процедуры: 
\begin{enumerate}
\item В начале возьмем ортогональное дополнение относительно верхней формы и получим $W^\bot \subseteq V$, а потом применим $\phi$ и получим $\phi(W^\bot)\subseteq V^{**}$.

\item Сразу возьмем ортогональное дополнение относительно нижней формы и получим $W^\bot \subseteq V^{**}$.
\end{enumerate}
Так вот, коммутативность диаграммы выше означает, что результаты этих двух операций совпадают, то есть полученные подпространства в $V^{**}$ будут одинаковыми.
Я утверждаю, что это объясняется методом пристального взгляда,%
\footnote{Но как обычно слепым для прозрения рекомендую воспользоваться бумажкой и ручкой.
Пригодится в любом случае: либо написать доказательство, либо вытереть слезы отчаяния.}
и мы воспользуемся этим наблюдением в доказательстве следующего результата.

\begin{claim}
[Альтернатива Фредгольма]
\label{claim::Fredholm}
Пусть $\varphi\colon V\to U$ -- некоторое линейное отображение и $\varphi^*\colon U^*\to V^*$ -- сопряженное к нему.
Тогда
\begin{enumerate}
\item $(\Im \varphi)^\bot = \ker \varphi^*$.

\item $(\ker \varphi)^\bot = \Im \varphi^*$.
\end{enumerate}
\end{claim}
\begin{proof}
(1) Следующая цепочка равенств проводит доказательство:
\[
\ker\varphi^* =\{\xi \in U^*\mid \varphi^*(\xi) = 0\} = \{\xi \in U^*\mid \xi \varphi = 0\} = \{\xi\in U^*\mid \xi(\Im \varphi) = 0\} = \{\xi\in U^*\mid \langle \xi, \Im\varphi\rangle = 0\} = (\Im \varphi)^\bot
\]


Теперь докажем, что (1) влечет (2).
По утверждению~\ref{claim::DualitySpaces} пункты~(2) и~(3) равенство $(\ker \varphi)^\bot = \Im \varphi^*$ в пространстве $V^*$  эквивалентно равенству $\ker \varphi = (\Im \varphi^*)^\bot$ в пространстве $V$.
Теперь применим к этому равенству изоморфизм $\phi\colon V\to V^{**}$ (утверждение~\ref{claim::CanonicalIsomorphism}).
Тогда $\ker \varphi$ перейдет в $\ker \varphi^{**}$ (это сразу следует из утверждения~\ref{claim::CanonicalIsomorphism}).
Кроме того, $(\Im \varphi^*)^\bot \subseteq V$, где ортогональное дополнение берется относительно формы $V^*\times V\to F$, перейдет в $(\Im \varphi^*)^\bot\subseteq V^{**}$, где ортогональное дополнение берется относительно формы $V^*\times V^{**}\to F$.
Это получается из замечания перед формулировкой утверждения выше.
Потому нам достаточно показать, что $(\Im \varphi^*)^\bot = \ker(\varphi^*)^*$ в пространстве $V^{**}$.
А это равносильно (1), примененному к отображению $\varphi^*\colon U^*\to V^*$.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Я хочу добавить пару замечаний о пользе утверждения выше.
Не надо ожидать, что оно даст вам супер тайное знание, которое решит все проблемы.
Нет, это скорее доказательство бессмысленного очевидного факта.
Чтобы понять, почему он бессмысленный выберете базис в $V$, потом двойственный к нему в $V^*$ и переформулируйте все на матричном языке.
Очень удивитесь тому, какую элементарную дичь мы тут с вами делаем.
А раз так, то надо бы объяснить, какого я вас тут мучаю заведомо более дурацким доказательством и совсем бессмысленным рассуждением.

Так вот, самая большая польза от этого утверждения -- сломать мозг.
А именно, я хочу продемонстрировать вам конструкции в рамках линейной алгебры, которые мозгу тяжело воспринимать, про которые сложно думать, но которые часто встречаются в математике.
Причина, почему вам сложно, проста -- у вас нет интуиции про эти объекты и вы никогда не проделывали подобные рассуждения и не видели таких конструкций.
И потому самое полезное из произошедшего -- вы все это увидели и у вас был шанс в этом разобраться или попробовать разобраться.
Вот именно необходимость разобраться в абстрактной чуши и есть цель этого утверждения, как и его доказательства.

\item Чуть выше я уже рекомендовал все перевести на матричный язык.
Другое хорошее упражнение -- попробовать расписать явно условие $(\ker \varphi)^\bot = \Im \varphi^*$ и доказать его в лоб без применения двойственности.
Это тоже очень полезное упражнение.
Вообще, очень полезно видеть разные способы доказать одно и то же, это помогает лучше понять место знания в экосистеме изученного.
\end{itemize}