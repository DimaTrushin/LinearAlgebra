\ProvidesFile{lecture25.tex}[Лекция 25]


В случае, когда $\varphi\colon V\to V$ является оператором, то у него намного больше характеристик, чем просто ядро и образ.
Следующее утверждение подытоживает все знания об общих характеристиках воедино.

\begin{claim}
Пусть $\varphi\colon V\to V$ -- некоторый линейный оператор над полем $F$.
Тогда
\begin{enumerate}
\item $\tr \varphi = \tr \varphi^*$.

\item $\det \varphi = \det \varphi^*$.

\item $\chi_\varphi = \chi_{\varphi^*}$.

\item $f_{\text{min},\,\varphi} = f_{\text{min},\,\varphi^*}$.

\item $\spec_F(\varphi) = \spec_F(\varphi^*)$.

\item $\spec_F^I(\varphi) = \spec_F^I(\varphi^*)$.

\item если $U\subseteq V$ -- $\varphi$-инвариантное, то $U^\bot\subseteq V^*$ -- $\varphi^*$-инвариантное.
\end{enumerate}
\end{claim}
\begin{proof}
Все утверждения кроме последнего следуют из утверждения~\ref{claim::DualHomMatrix}, которое гласит, что при правильном выборе базисов в $V$ и $V^*$ матрицы $\varphi$ и $\varphi^*$ отличаются транспонированием.

(7) Нам дано $\varphi(U)\subseteq U$, а надо показать, что $\varphi^*(U^\bot)\subseteq U^\bot$.
Пусть $\xi\in U^\bot$, то есть $\xi(U) = 0$, надо проверить, что $\varphi^*(\xi)(U) = 0$.
То есть надо проверить, что $(\xi \varphi)(U) = 0$.
Но $(\xi \varphi) (U) = \xi(\varphi(U))\subseteq \xi(U) = 0$, что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Таким образом изучение оператора $\varphi$ это тоже самое, что изучение оператора $\varphi^*$, если при этом <<перевернуть>> все пространства.
То есть любой вопрос про $\varphi^*$ можно переформулировать в терминах $\varphi$, заменив размерности подпространств в формулировках на $n$ минус размерность (здесь $n$ -- размерность объемлющего пространства $V$).

\item На самом деле для $\varphi$ и $\varphi^*$ совпадают жордановы нормальные формы.
Действительно, если вы транспонируете матрицу в жордановой нормальной форме, то результирующая матрица будет иметь ту же самую жорданову нормальную форму.

\item Хорошее упражнение для ума: пусть $e_1,\ldots,e_n$ -- жорданов базис для $\varphi$, а $e^1,\ldots,e^n$ -- двойственный к нему базис.
Как из $e^1,\ldots,e^n$ получить жорданов базис для $\varphi^*$?
\end{itemize}

\paragraph{Примеры}

\begin{enumerate}
\item Пусть $\varphi\colon V\to V$ такой, что характеристический многочлен $\varphi$ раскладывается на линейные множители.
А это означает, что $V$ раскладывается в прямую сумму корневых, то есть $V = V^{\lambda_1}\oplus \ldots \oplus V^{\lambda_r}$ (утверждение~\ref{claim::RootSpaceDec}).
Так как характеристический многочлен у $\varphi^*$ такой же, то $V^* = (V^*)^{\lambda_1}\oplus \ldots \oplus (V^*)^{\lambda_r}$.
Давайте покажем, что $(V^{\lambda_1})^\bot = (V^*)^{\lambda_2}\oplus \ldots \oplus (V^*)^{\lambda_r}$.

Самый простой способ сделать это такой.
Рассмотрим многочлен $p(t) = (t-\lambda_1)^{k_1}$, где $k_1$ -- кратность $\lambda_1$ в $\chi_\varphi$.
Тогда $V^{\lambda_1} = \ker p(\varphi)$ (это, например, следует из утверждения~\ref{claim::RootMultGeom}, однако, можно просто взять достаточно большое $k_1$ и можно обойтись леммой о стабилизации -- утверждение~\ref{claim::StabilityLemma}).
По утверждению~\ref{claim::Fredholm} о двойственности для линейных отображений, мы получаем, что $(\ker p(\varphi))^\bot = \Im (p(\varphi))^* = \Im p(\varphi^*)$.
Последнее равенство проверяется в лоб:
\[
p(\varphi)^* =  \Bigl(\sum_ka_k\varphi^k\Bigr)^* = \sum_ka_k \left(\varphi^k\right)^* = \sum_k a_k \left(\varphi^*\right)^k = p(\varphi^*)
\]
По утверждению~\ref{claim::IdealRootDec} для оператора $\varphi^*$ последний образ как раз и равен сумме оставшихся корневых.%
\footnote{Если не понятно, на что я тут ссылаюсь, то загляните в доказательство утверждения~\ref{claim::GenRootDec} пункт~(2), это должно снять все вопросы.}

\item Пусть $\varphi\colon V\to V$ -- линейный оператор над полем $\mathbb C$.
Тогда мы знаем, что обязательно найдется ненулевой собственный вектор $v\in V_\lambda$ для некоторого $\lambda\in \mathbb C$.
Последнее равносильно тому, что найдется инвариантное одномерное подпространство $\langle v \rangle$.
Давайте покажем, как с помощью двойственности автоматически найти инвариантное $n - 1$ мерное подпространство, где $n = \dim V$.
Пусть $U\subseteq V^*$ -- инвариантное одномерное для $\varphi^*$.
Такое существует, потому что у $\varphi^*$ есть собственный вектор (мы над полем $\mathbb C$).
В этом случае $U^\bot\subseteq V$ будет $n-1$ мерным и инвариантным для $\varphi$ по пункту~(7) предыдущего утверждения.
\end{enumerate}

\subsection{Классификационная задача для $\Bil(V,U)$}

\begin{claim}
Пусть $V$ и $U$ -- векторные пространства над полем $F$ размерностей $n$ и $m$, соответственно, и пусть $A,B\in \operatorname{M}_{n\,m}(F)$ -- произвольные матрицы.
Тогда эквивалентны следующие утверждения:
\begin{enumerate}
\item Матрицы $A$ и $B$ задают одну и ту же билинейную форму на $V$ и $U$ в разных базисах, т.е. существует билинейная форма $\beta\colon V\times U\to F$, два базиса в $V$: $e=(e_1,\ldots,e_n)$ и $e'=(e_1',\ldots,e_n')$ и два базиса в $U$: $f=(f_1,\ldots,f_m)$ и $f'=(f_1',\ldots,f_m')$ такие, что $A$ является матрицей $\beta$ в базисах $e$ и $f$, а $B$ является матрицей $\beta$ в базисах $e'$ и $f'$.

\item $\rk A = \rk B$.
\end{enumerate}
\end{claim}
\begin{proof}
Из (1) в (2) мы уже знаем, это корректность ранга билинейной формы.

Из (2) в (1).
Так как ранги матриц $A$ и $B$ одинаковые, мы можем найти такие обратимые матрицы $C,R \in \operatorname{M}_{n}(F)$ и $D, P\in \operatorname{M}_{m}(F)$, что
\[
A = C
\begin{pmatrix}
{E}&{0}\\
{0}&{0}
\end{pmatrix}
D,
\quad
B = R
\begin{pmatrix}
{E}&{0}\\
{0}&{0}
\end{pmatrix}
P
\]
А значит $B = RC^{-1}AD^{-1}P$, то есть $B = S^t A T$, где $S = (RC^{-1})^t$ и $T = D^{-1}P$.
Теперь нам надо найти билинейную форму  и две пары базисов.
Возьмем два произвольных базиса $e=(e_1,\ldots,e_n)$ в $V$ и $f=(f_1,\ldots,f_m)$ в $U$ и положим $\beta\colon V\times U \to F$ по правилу $\beta(v, u) = x^t A y$, где $v = ex$ и $x\in F^n$, $u = f y$ и $y\in F^m$.
После положим $e' = e S$ и $f' = fT$.
Так как $S$ и $T$ невырожденные матрицы, то $e'$ будет базисом $V$, а $f'$ -- базисом $U$.
Тогда в новой паре базисов матрица нашей билинейной формы будет $S^t A T$, что совпадает с $B$ по построению.
\end{proof}

Таким образом, как и в случае линейного отображения, две матрицы задают одну и ту же билинейную форму тогда и только тогда, когда их ранги совпадают.


\subsection{Структура векторного пространства на $\Bil(V,U)$}

До сих пор мы смотрели на $\Bil(V, U)$ как на множество билинейных форм, такой мешок, в котором аморфно лежат все наши замечательные симпатичные формочки, одна лучше другой.
На самом деле $\Bil(V, U)$ само является векторным пространством.
По-простому, это означает, что на $\Bil(V, U)$ есть хорошие операции сложения и умножения на константу.


\begin{definition}
Пусть $\beta_1,\beta_2\colon V\times U \to F$ -- две билинейные формы.
Мы хотим определить форму $\beta_1 + \beta_2$.
Это значит, что нам надо определить отображение $(\beta_1 + \beta_2)\colon V\times U \to F$.
Определим его по правилу
\[
(\beta_1 + \beta_2)(v, u) := \beta_1(v,u) + \beta_2(v,u)
\]
Если $\beta\colon V\times U \to F$ -- билинейная форма и $\lambda\in F$, тогда форму $(\lambda \beta)\colon V\times U \to F$ определим по правилу
\[
(\lambda\beta)(v, u) := \lambda \beta(v, u)
\]
\end{definition}

\paragraph{Замечания}

\begin{itemize}
\item В качестве упражнения я предлагаю проверить, что $\beta_1 + \beta_2$ и $\lambda\beta$ -- это не просто отображения из $V\times U$ в $F$, а билинейные формы, то есть удовлетворяют условиям определения~\ref{def::BilinearForms}.
Это объясняет, что определенные выше операции корректны, то есть их результат -- это тоже билинейная форма.

\item В качестве другого упражнения я предлагаю проверить, что множество $\Bil(V, U)$ является векторным пространством, то есть надо проверить аксиомы из определения~\ref{def::VectorSpace}.

\item Чтобы немного вывернуть мозги наизнанку, давайте вспомним, что билинейные формы имеют операторную запись, то есть вместо $\beta\colon V\times U\to F$ мы будем писать $\cdot_\beta\colon V\times U \to F$, при это $\beta(v, u) = v \cdot_\beta u$.
То есть билинейные формы -- это операции умножения векторов, которые в результате возвращают число.
Так вот, мы только что определили как складывать и умножать на числа операции умножения так, что они остаются операциями умножения!
Определения выше можно переписать так
\begin{align*}
v (\cdot_{\beta_1} + \cdot_{\beta_2}) u &= v \cdot_{\beta_1} u + v \cdot_{\beta_2} u\\
v(\lambda \cdot_\beta) u &= \lambda (v \cdot_\beta u)
\end{align*}
Неправда ли выносит мозг?
Это лишний раз говорит о том, что в математике очень важно правильно думать об объекте.
Сложение билинейных форм не представляется нам чем-то особенным, вот умение складывать операции умножения кажется диким.
Психологические барьеры надо уметь перепрыгивать.
\end{itemize}

Теперь вспомним, что фиксировав базисы в пространствах $V$ и $U$, каждая билинейная форма превращается в матрицу (утверждение~\ref{claim::BilinearMatrices}).
Так вот, заметим, что это превращение является измомрфизмом между векторными пространствами.
Для полноты картины я все же сформулирую сам результат.

\begin{claim}
%\label{claim::BilinearIsomMatrices}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e = (e_1,\ldots,e_n)$ -- базис пространства $V$ и $f=(f_1,\ldots,f_m)$ -- базис пространства $U$.
Тогда отображение $\operatorname{Bil}(V,U)\to \operatorname{M}_{n\,m}(F)$ по правилу $\beta\mapsto B_\beta$ является изоморфизмом.
\end{claim}

Следующая наша задача разобраться со случаем аналогичным случаю оператора, а именно: билинейная форма определена на одном пространстве.


\newpage
\section{Билинейные формы на одном пространстве $\Bil(V)$}

В случае, когда линейное отображение определено на одном пространстве (линейный оператор), у нас в запасе намного больше конструкций и характеристик, чем в случае общего линейного отображения.
Аналогичная ситуация обстоит и с билинейными формами.
В случае, когда билинейная форма живет на одном пространстве у нас на много больше характеристик и поведение ее изучать несколько сложнее.
Ниже я буду рассказывать о билинейных формах на одном пространстве.
Окажется, что от части ситуация с билинейными формами технически сильно проще, чем случай линейных операторов.

\subsection{Симметричность и кососимметричность}

Первое отличие билинейных форм на одном пространстве от общего случая заключается в том, что мы можем подставлять один и тот же вектор как в качестве левого, так и в качестве правого аргумента.
Благодаря можно выделить классы симметричных и кососимметричных форм.

\begin{definition}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма определенная на одном пространстве.
Тогда
\begin{itemize}
\item Будем говорить, что $\beta$ симметричная, если $\beta(u,v) = \beta(v,u)$ для любых $u,v\in V$.

\item Будем говорить, что $\beta$ кососимметрична, если $\beta(v,v) = 0$ для любого $v\in V$.
\end{itemize}
\end{definition}

\paragraph{Замечание}

Давайте обсудим свойство кососимметричности.
Вас должно было удивить такое дурацкое условие в определении.
Однако, теперь, когда мы уже взрослые и знаем разные поля, но толком про них ничего еще и не знаем, нам надо быть чуточку аккуратными.
Если $\beta(v,v) = 0$ для любого $v\in V$, тогда $\beta(v+u,v+u) = 0$ для любых $u,v\in V$.
Это значит
\[
0=\beta(v+u,v+u) = \beta(v,v) + \beta(v,u) + \beta(u,v)+\beta(u,u) = \beta(v,u) + \beta(u,v)
\]
То есть из этого условия вытекает, что $\beta(v,u) = -\beta(u,v)$.
Однако, если нам дано, что $\beta(v,u) = -\beta(u,v)$, то подставив $u = v$, мы получим $\beta(v,v) = -\beta(v,v)$, а значит $2\beta(v,v) = 0$.
И вот тут начинаются чудеса.
Бывают поля, в которых $2 = 0$.
В таких полях условие $\beta(v,v) = 0$ сильнее условия $\beta(v,u) = -\beta(u,v)$.
Если же $2$ обратимо в поле $F$, то оба условия равносильны.
Однако, в общем случае, правильное определение -- потребовать более сильное условие, которое и содержится в определении.


\subsection{Матрица билинейной формы}

Если $\beta\colon V\times V\to F$ -- билинейная форма на пространстве $V$ над полем $F$, то для определения ее матрицы нам достаточно выбрать только один базис.%
\footnote{Вообще говоря никто не мешает выбрать для одного и того же пространства два разных базиса для левого и правого аргумента.
И бывает, что так делать нужно.
Но вообще говоря не стоит.
В разных базисах один и те же векторы имеют разные координаты и у нас ломается главное, что мы имеем -- возможность сравнить левый и правый вектор методом взгляда.}
Если $e_1,\ldots,e_n$ -- базис пространства $V$, то матрица билинейной формы в этом базисе будет $B = (\beta(e_i, e_j))$.
Если мы переходим к новому базису $(e_1',\ldots,e_n') = (e_1,\ldots,e_n) C$, где $C$ -- обратимая матрица, и $B'$ -- матрица билинейной формы в новом базисе, то $B' = C^t B C$.

Обратите внимание, что в случае одного пространства все определяется одним базисом и в этом случае переход от одного базиса к другому на матричном языке идет по формуле $B' = C^t B C$.
Если внимательно посмотреть, как действуют матрицы элементарных преобразований (раздел~\ref{section::ElemMat}), то мы увидим, что переход от $B$ к $C^tBC$ означает совершить одинаковые элементарные преобразования над строками и столбцами матрицы $B$.
А это значит, что в отличие от операторов, нам не нужна никакая сложная теория с собственными значениями.
Достаточно действовать обычным Гауссом.
Чуть ниже мы увидим, что любую симметричную билинейную форму всегда можно диагонализовать.
Но вот сравнение диагональных форм будет требовать некоторых знаний про базовое поле и эту задачу мы решим лишь для полей $\mathbb R$ и $\mathbb C$.
Уже для поля $\mathbb Q$ и билинейных форм на двумерных пространствах мы получим нетривиальную задачу из теории чисел.

\subsection{Матричные характеристики билинейной формы}
\label{subsection::BilChar}


В случае линейного оператора мы поступали так: выбирали базисы и задавали их матрицами.
Потом считали какие-то характеристики этих самых матриц и показывали, что они не зависят от базисов.
Мы уже попробовали такой подход в случае общих билинейных форм (на двух разных пространствах).
Теперь давайте попробуем такой же подход в случае с билинейными формами

\paragraph{Ранг}

Пусть $\beta\colon V\times V\to F$ -- билинейная форма, которая в двух базисах имеет матрицы $B$ и $B'$, соответственно.
Тогда $B' = C^t B C$ для некоторой невырожденной матрицы $C$.
Тогда $\rk B' = \rk B$, так как он не меняется при умножении слева и справа на невырожденную матрицу (утверждение~\ref{claim::rkInvariance}).

\paragraph{След}

Так как $\tr(B') = \tr(C^t B C)$, то вообще говоря след не несет никакой содержательной информации.
Действительно, рассмотрите пример $B = \begin{pmatrix}{1}&{0}\\{0}&{-1}\end{pmatrix}$ и $C = \begin{pmatrix}{a}&{b}\\{c}&{d}\end{pmatrix}$ -- невырожденная матрица, то есть $ad - bc\neq 0$.
Тогда $\tr(B) = 0$ и $\tr(B') = (a^2 + b^2) - (c^2 + d^2)$.
В случае поля $\mathbb R$ или $\mathbb C$ это означает, что след $B'$ может быть каким угодно числом.

\paragraph{Определитель}

В этом случае $\det(B') = \det(C^t B C) = \det(B) \det (C)^2$.
То есть определитель в разных базисах может отличаться на квадрат числа из $F^*$.
В общем случае это означает, что корректно определено понятие $\det(B) = 0$ или $\det(B)\neq 0$.
Например:
\begin{itemize}
\item Если поле $F = \mathbb C$, то условие $\det(B) = 0$ или $\det(B)\neq 0$ -- лучшее, что можно получить.
Действительно, если $\det(B)\neq 0$, то в другом базисе $\det(B') = \det(B)c^2$.
Так как в поле $\mathbb C$ из любого числа можно извлечь корень, то $\det(B')$ можно сделать произвольным.

\item Пусть $F = \mathbb R$, тогда корректно определен знак определителя, то есть $\sgn \det B$ корректно определен.
Действительно, $\det(B') = \det(B) c^2$.
В поле $\mathbb R$ число равно квадрату тогда и только тогда, когда оно положительно.

\item Пусть $F = \mathbb Q$.
Тут ничего особенно сказать нельзя, кроме явной формулировки.
Пусть $\det(B) = p_1^{k_1}\ldots p_r^{k_r}$, где $p_i\in \mathbb N$ -- простые числа, а $k_i\in \mathbb Z$ -- степени (вообще говоря положительные или отрицательные).
Тогда четности чисел $k_i$ определены однозначно независимо от базиса.
\end{itemize}

Как мы видим определитель вообще говоря не определен для билинейной формы, но какие-то характеристики из него вытащить можно и эти характеристики сильно зависят от поля, над которым мы работаем.

\paragraph{Обратимость матрицы}

Так как корректно определен ранг или понятие $\det(B) = 0$, то обратимость матрицы $B$ тоже не зависит от базиса.

\paragraph{Матрицы симметричных и кососимметричных форм}

\begin{claim}
\label{claim::BilSymAntiSym}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма.
Тогда
\begin{enumerate}
\item Билинейная форма $\beta$ симметрична тогда и только тогда, когда матрица $B_\beta$ симметрична $B_\beta^t = B_\beta$.

\item Если $2\neq 0$ в поле $F$, то билинейная форма кососимметрична тогда и только тогда, когда матрица $B_\beta$ кососимметрична $B_\beta^t = - B_\beta$.

\item Если $2 = 0$ в поле $F$, то билинейная форма кососимметрична тогда и только тогда, когда матрица $B_\beta$ симметрична $B_\beta^t = B_\beta$ и имеет нулевую диагональ.%
\footnote{
\label{foot::AntiSymMatrix}
Над полем $F$, где $2 = 0$, эти два условия по определению полагаются условием кососимметричности матрицы.
Тогда получается, что билинейная форма кососимметрична тогда и только тогда, когда матрица кососимметрична.}
\end{enumerate}
\end{claim}
\begin{proof}
(1) Фиксируем базис $e_1,\ldots,e_n$, тогда билинейная форма превращается в $\beta(x,y) = x^t B y$.
Условие $\beta(x,y) = \beta(y,x)$ для любых $x,y\in F^n$ равносильно условию $x^t B y = y^t B x = (y^t B x)^t = x^t B^t y$ для любых $x,y\in F^n$.
Заметим, что это условие равносильно симметричности матрицы $B$, которая в свою очередь является матрицей $B_\beta$ в базисе $e_1,\ldots,e_n$.

(2) Если $2\neq 0$ в поле $F$, то кососимметричность равносильна условию $\beta(x,y) = -\beta(y,x)$.
Выберем произвольный базис $e_1,\ldots,e_n$, тогда $\beta(x,y) = x^t By$.
Тогда условие $\beta(x,y) = -\beta(y,x)$ для всех $x,y\in F^n$ равносильно условию $x^t By = -y^t Bx = -(y^t B x)^t =- x^t B^t y$ для любых $x,y\in F^n$.
Последнее равносильно условию $B^t = - B$.

(3) Пусть теперь $2 = 0$ в $F$.
Тогда надо заметить, что $1 + 1 = 0$, то есть $1 = - 1$.
То есть условие $B^t = B$ и $B^t = - B$ равносильны!
Как и выше выберем некоторый базис $e_1,\ldots,e_n$ и в нем запишем нашу форму в виде $\beta(x,y) = x^t B y$.
Если $\beta$ кососимметрична, то $\beta(x,y) = -\beta(y,x) = \beta(y,x)$, то есть форма симметрична.
А значит $B^t = B$ по первому пункту.
С другой стороны, так как $\beta(e_i,e_i) = 0$, то диагональ матрицы $B$ должна быть нулевой.

Обратно.
Этот случай противной математики, когда без счета в лоб не обойтись (сочувствую нам всем).
Пусть $v = x_1e_1+\ldots+x_ne_n$ -- некоторый вектор, посчитаем $\beta(v,v)$ и покажем, что мы получим ноль.
\begin{gather*}
\beta(v,v) = \beta(\sum_i x_i e_i, \sum_j x_j e_j) = \sum_{ij}x_ix_j \beta(e_i,e_j) =\\ \sum_{i=1}^nx_i^2 \beta(e_i, e_i) + \sum_{i<j}(x_i x_j \beta(e_i, e_j) + x_j x_i \beta(e_j, e_i)) = \sum_{i=1}^nx_i^2 \beta(e_i, e_i) + 2\sum_{i<j}x_i x_j \beta(e_i, e_j)
\end{gather*}
Тогда в последней сумме первое слагаемое равно нулю так как диагональ $B_\beta$ состоит из нулей, а второе равно нулю, так как $2 = 0$.
\end{proof}

Как мы видим в случае $2 = 0$ в поле $F$ нас ждал сюрприз.
Самым большим откровением обычно становится тот факт, что кососимметрические билинейные формы в данном случае становятся специального вида симметрическими формами.
Вот такие чудесатые чудеса.


\subsection{Разложение в прямую сумму}

Пусть $V$ -- векторное пространство над полем $F$.
Тогда будем обозначать через $\SBil(V)$ множество симметричных билинейных форм на $V$, а через $\ABil(V)$ множество кососимметричных билинейных форм.
Напомним, что если $2 = 0$ в поле $F$, то $\ABil(V)\subseteq \SBil(V)$ (утверждение~\ref{claim::BilSymAntiSym}).
Этот пример объясняет ограничение случаем $2 \neq 0$ в следующем утверждении.

\begin{claim}
\label{claim::BilDirectSA}
Пусть $V$ -- векторное пространство над полем $F$ таким, что $2 \neq 0$.
Тогда любая билинейная форма $\beta\colon V\times V\to F$ единственным образом раскладывается в сумму симметричной и кососимметричной.
На языке векторных пространств это означает, что $\Bil(V) = \SBil(V) \oplus \ABil(V)$.
\end{claim}
\begin{proof}
Я лишь предъявлю желаемое разложение 
\[
\beta(v,u) = \frac{\beta(v,u) + \beta(u, v)}{2} + \frac{\beta(v,u) - \beta(u,v)}{2}
\]
Все детали остаются на совести читателя.
\end{proof}

В дальнейшем наше основное внимание будет уделено симметричным билинейным формам.


\subsection{Ограничение билинейной формы на подпространство}

\begin{definition}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма и $U\subseteq V$ -- подпространство.
Тогда через $\beta|_U\colon U\times U\to F$ будем обозначать билинейную форму, действующую по правилу $(u_1, u_2)\mapsto \beta(u_1, u_2)$ для всех $u_1,u_2\in U$.
Форма $\beta|_U$ будет называться ограничением $\beta$ на $U$.%
\footnote{В случае формы $\beta\colon V\times U\to F$ также можно определить ограничение, но для этого нужно иметь пару подпространство $V'\subseteq V$ и $U'\subseteq U$.
Полученная форма будет $\beta|_{V'\times U'}\colon V'\times U' \to F$.
Однако мы ими не пользуемся и обозначения у них ужасные.}
\end{definition}

По-простому, ограничение формы -- это та же самая форма, которая забыла как перемножать все векторы из нашего пространства, а помнит только про перемножение векторов из подпространства.


\paragraph{Замечание}

Пусть $\beta\colon V\times V\to F$ -- некоторая билинейная форма и $U\subseteq V$ -- подпространство.
Пусть $e_1,\ldots,e_n$ -- базис $V$ такой, что $e_1,\ldots,e_k$ образуют базис $U$.
Тогда матрица $\beta$ в базисе $e_1,\ldots,e_n$ будет $B_{\beta} = (\beta(e_i, e_j))_{1\leqslant i,j \leqslant n}$.
С другой стороны, матрица $\beta|_U$ в базисе $e_1,\ldots,e_k$ будет $B_{\beta|_U} = (\beta(e_i, e_j))_{1\leqslant i,j \leqslant k}$.
То есть матрица $B_{\beta|_U}$ -- это левый верхний блок размера $k$ в матрице $B_{\beta}$.
Таким образом, в отличие от линейных операторов, матрицу ограничения  билинейной формы очень легко считать.

\begin{claim}
\label{claim::NonDegRestrictionBil}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма и $U\subseteq V$ -- некоторое подпространство.
Тогда
\begin{enumerate}
\item Выполнены следующие равенства
\begin{enumerate}
\item $\ker^L \beta|_U = U\cap {}^\bot U$

\item $\ker^R \beta|_U = U \cap U^\bot$
\end{enumerate}

\item Следующие условия эквивалентны
\begin{multicols}{2}
\begin{enumerate}
\item $\beta|_U$ невырождена

\item $U\cap {}^\bot U = 0$

\item $U \cap U^\bot = 0$

\item $V = U\oplus U^\bot$

\item $V = U\oplus{}^\bot U$

\item[\vspace{\fill}]
\end{enumerate}
\end{multicols}
\end{enumerate}
\end{claim}
\begin{proof}
(1) Этот пункт проверяется по определению.
Я проверю лишь первый.
Имеем
\[
\ker^L \beta|_U = \{u \in U\mid \beta(u, U) = 0\}
\]
С другой стороны
\[
U \cap {}^\bot U = U \cap \{v\in V\mid \beta(v, U) = 0\} = \{u\in U\mid \beta(u, U) = 0\}
\]
Получили одно и то же.

(2) По определению форма не вырождена тогда и только тогда, когда у нее оба ядра ноль.
Из первого пункта следует, что $U\cap {}^\bot U$ и $U\cap U^\bot$ -- это ядра формы $\beta|_U$.
С другой стороны, так как форма действует на одном пространстве, то размерности ядер совпадают (утверждение~\ref{claim::BilinearKernels}).
Таким образом доказана эквивалентность первых трех пунктов.

Понятно, что (d) -- более сильная версия (c) и (e) -- более сильная версия (b).
Остается показать, что (c) влечет (d) и аналогично (b) влечет (e).
Покажем первое.
Так как подпространства $U$ и $U^\bot$ не пересекаются, то они образуют прямую сумму $U\oplus U^\bot \subseteq V$ и надо лишь показать, что в сумме получается все $V$.
Для этого достаточно показать, что $U^\bot$ имеет размерность хотя бы $\dim V - \dim U$.
Пусть $U = \langle v_1,\ldots, v_k\rangle$, тогда 
\[
U^\bot = \{v\in V \mid \beta(v_1, v) = \ldots = \beta(v_k, v) = 0\}
\]
То есть $U^\bot$ задается системой из $k$ уравнений и $n = \dim V$ переменных.
Значит ранг этой системы не превосходит $k$, а количество свободных переменных не меньше $n - k$ и равно размерности $U^\bot$, что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Если $\beta\colon V\times V\to F$ -- билинейная форма и $U =\langle v \rangle \subseteq V$ -- подпространство порожденное одним вектором $v\neq 0$.
Тогда $v$ -- базис $U$ и матрица $\beta|_U$ в этом базисе -- это $\beta(v, v)$.
Потому $\beta|_U$ невырождена тогда и только тогда, когда $\beta(v, v)\neq 0$.

\item Пусть $\beta\colon V\times V\to F$ -- некоторая билинейная форма, $U\subseteq V$ -- некоторое подпространство.
Предположим, что $V = U \oplus {}^\bot U$.
Выберем базис $U = \langle e_1,\ldots,e_k\rangle$ и базис ${}^\bot U = \langle g_{k+1},\ldots, g_n\rangle$.
Тогда в базисе $e_1,\ldots,e_k, g_{k+1}, \ldots, g_n$ матрица $\beta$ имеет вид
\[
\begin{pmatrix}
{B_{\beta|_U}}&{*}\\
{0}&{B_{\beta|_{{}^\bot U}}}
\end{pmatrix}
\]
Если же $V = U \oplus U^\bot$ и $U^\bot =\langle f_{k+1},\ldots, f_n\rangle$, то в базисе $e_1,\ldots,e_k,f_{k+1},\ldots, f_n$ матрица $\beta$ имеет вид
\[
\begin{pmatrix}
{B_{\beta|_U}}&{0}\\
{*}&{B_{\beta|_{U^\bot}}}
\end{pmatrix}
\]
Если же $U^\bot = {}^\bot U$,%
\footnote{Например такое бывает, если $\beta$ симметрична.}
то, выбрав базисы $U = \langle e_1,\ldots,e_k\rangle$ и $U^\bot = {}^\bot U = \langle e_{k+1}, \ldots, e_n\rangle$, мы получим матрицу для $\beta$ вида
\[
\begin{pmatrix}
{B_{\beta|_U}}&{0}\\
{0}&{B_{\beta|_{U^\bot}}}
\end{pmatrix}
\]
Таким образом выделение угла нулей в матрице билинейной формы -- это вопрос разложения пространства $V$ в прямую сумму подпространства $U$ и одного из его двух ортогональных дополнений.
А выделение блочно диагонального вида означает, что надо подобрать такое $U$, чтобы его левое и правое ортогональные дополнения совпали и все пространство $V$ разваливалось в прямую сумму $U$ и дополнения.
\end{itemize}

\subsection{Диагонализация симметричных форм}

\begin{claim}
\label{claim::SBilToDiag}
Пусть $\beta\colon V\times V\to F$ -- симметричная билинейная форма и $2 \neq 0$ в $F$.
Тогда существует такой базис, что матрица формы $\beta$ имеет диагональный вид.
\end{claim}
\begin{proof}
Что значит найти базис $e_1,\ldots,e_n$, в котором матрица $\beta$ будет диагональной?
Это значит, найти базис, в котором $\beta(e_i,e_j) = 0$ при $i\neq j$.
Потому план будет следующий: если $\beta \neq 0$, то найдем некоторый вектор $v\in V$ такой, чтобы $V = \langle v \rangle \oplus \langle v \rangle^\bot$.
Положим $e_1 = v$, а векторы $e_2,\ldots,e_n$ выберем по индукции в подпространстве $\langle v \rangle^\bot$ (если $\beta$ нулевая, то годится любой базис).
Полученная система векторов будет ортогональным базисом.
Чтобы завершить доказательство, надо объяснить, почему всегда можно выбрать такой вектор $v$.

Рассмотрим значения $\beta(v,v)$ для всех $v\in V$.
Если это значение всегда ноль, то $\beta$ -- кососимметричная, но она одновременно симметричная.
Так как $2 \neq 0$, такое возможно только если $\beta = 0$.
В этом случае все доказано, матрица $\beta$ будет диагональная в любом базисе.
Значит мы можем предположить, что найдется такой $v\in V$, что $\beta(v, v)\neq 0$.
Последнее означает, что $\beta|_U$ невырождена, где $U = \langle v \rangle$.
В силу утверждения~\ref{claim::NonDegRestrictionBil} это означает, что $U \oplus U^\bot = V$.
Обозначим $e_1 = v$ выберем $e_2,\ldots,e_n\in U^\bot$ из индукционного предположения.
\end{proof}


\paragraph{Замечание}

Предположим, что $2 = 0$ в поле $F$.
Тогда рассмотрим $V = F^2$ и зададим билинейную форму $\beta\colon V\times V\to F$ по правилу $(x,y)\mapsto x^t By$ для матрицы%
\footnote{Заметим, что в силу того, что $1 = -1$ в $F$, то эта матрица еще к тому же и кососимметричная.}
\[
B = 
\begin{pmatrix}
{0}&{1}\\
{1}&{0}
\end{pmatrix}
\]
Если выбрать произвольную матрицу $C \in \operatorname{M}_2(F)$ с неопределенными коэффициентам
\[
C = 
\begin{pmatrix}
{a}&{b}\\
{c}&{d}
\end{pmatrix}
\]
Тогда
\[
C^t B C = 
\begin{pmatrix}
{a}&{c}\\
{b}&{d}
\end{pmatrix}
\begin{pmatrix}
{0}&{1}\\
{1}&{0}
\end{pmatrix}
\begin{pmatrix}
{a}&{b}\\
{c}&{d}
\end{pmatrix}
=
\begin{pmatrix}
{a}&{c}\\
{b}&{d}
\end{pmatrix}
\begin{pmatrix}
{c}&{d}\\
{a}&{b}
\end{pmatrix}
=
\begin{pmatrix}
{0}&{ad + bc}\\
{ad +  bc}&{0}
\end{pmatrix}
=
\det(C)B
\]
В выкладках выше не забывайте, что $1 = -1$ в $F$.
Таким образом, какую бы замену мы ни с делали, матрица $B$ лишь изменится на скаляр из $F$ и никогда не диагонализуется.
Значит в предыдущем утверждении нельзя отбросить предположение $2 \neq 0$.

