\ProvidesFile{lecture33.tex}[Лекция 33]


\subsection{Классификация линейных отображений}

Мы с вами уже занимались классификацией линейных отображений между двумя разными пространствами (утверждение~\ref{claim::HomClassification}).
Выбирая базисы в двух пространствах независимо, мы можем добиться того, чтобы матрица превратилась в диагональную с единицами и нулями на диагонали.
По сути, две прямоугольные матрицы задают одно и то же отображение между двумя разными пространствами тогда и только тогда, когда у них одинаковый ранг.
Теперь у нас ситуация немного более жесткая.
У нас теперь есть два пространства $V$ и $U$ и они либо вещественные, либо эрмитовы и задано линейное отображение $\phi\colon V\to U$.
Но теперь в отличие от общего случая я хочу выбирать не произвольные базисы, а только ортонормированные.
Таким образом у меня меньше свободы для модификации матрицы оператора.
Оказывается, что даже в таком жестком случае, можно сделать матрицу оператора диагональной, то вот на диагонали будут стоять произвольные неотрицательные числа.

\begin{claim}
\label{claim::HermEuclHomClass}
Пусть $V$ и $U$ -- евклидовы или эрмитовы пространства и $\phi\colon V\to U$ -- линейное отображение.
Тогда
существует ортонормированный базис $e_1,\ldots,e_n$ в $V$, ортонормированный базис $f_1,\ldots,f_m$ в $U$ и последовательность вещественных чисел $\sigma_1\geqslant \sigma_2 \geqslant \ldots \geqslant \sigma_r > 0$ такие, что матрица $\phi$ имеет вид
\[
\phi(e_1,\ldots,e_n) = (f_1,\ldots,f_m) 
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
\]
При этом числа $\sigma_1,\ldots,\sigma_r$ определены однозначно и называются сингулярными значениями отображения $\phi$.
\end{claim}
\begin{proof}
Для поиска базиса $e_1,\ldots,e_n$ нам надо будет рассмотреть оператор $\phi^*\phi$.
Давайте начнем с трех замечаний
\begin{enumerate}
\item Оператор $\phi^*\phi\colon V\to V$ является самосопряженным.

Действительно, по правилам применения звездочки мы видим $(\phi^*\phi)^* = \phi^* \phi^{**} = \phi^* \phi$.

\item Спектр оператора $\phi^*\phi$ состоит из неотрицательных чисел.

Из обсуждения в разделе~\ref{section::BilinOper} о связи билинейных форм и операторов следует, что нам достаточно проверить, что билинейная форма $\beta(u, v) = (u, \phi^*\phi(v))$ является неотрицательно определенной.
А для этого достаточно проверить, что квадратичная форма $Q(v) = (v, \phi^*\phi(v))$ принимает неотрицательные значения.
Но это непосредственно следует из определения $Q(v) = (v, \phi^* \phi(v)) = (\phi(v), \phi(v)) \geqslant 0$.

\item Ядро оператора $\phi^* \phi$ совпадает с ядром $\phi$.

Ясно, что ядро $\phi$ лежит в ядре композиции $\phi^*\phi$.
С другой стороны, пусть $\phi^*\phi(v) = 0$, тогда $(v, \phi^*\phi(v))= 0$.
А следовательно $|\phi(v)|^2 = (\phi(v), \phi(v)) = (v, \phi^*\phi(v)) = 0$.
Значит и вектор $\phi(v) = 0$.
\end{enumerate}

Теперь мы готовы к доказательству утверждения.
По утверждению~\ref{} существует ортонормированный базис $e_1,\ldots, e_n$ в пространстве $V$ такой, что оператор $\phi^*\phi$ в нем диагонален.
В силу второго замечания, диагональные элементы этого оператора будут неотрицательны.
Давайте упорядочим их по невозрастанию $\lambda_1\geqslant \ldots \lambda_n \geqslant 0$.
Пусть первые $r$ элементов ненулевые, то есть мы имеем $\lambda_1 \geqslant \ldots \lambda_r > 0 = \ldots = 0$.
Определим числа $\sigma_i = \sqrt{\lambda_i}$.

Теперь для $1\leqslant i \leqslant r$ определим векторы $f_i = \frac{1}{\sigma_i}\phi(e_i)$.
Покажем, что $f_i$ это ортонормированная система векторов.
Действительно.
Сначала проверим длины ($1\leqslant i \leqslant r$):
\[
(f_i, f_i) = \left(\frac{1}{\sigma_i}\phi(e_i), \frac{1}{\sigma_i} \phi(e_i)\right) = \frac{1}{\sigma_i^2}(e_i, \phi^*\phi(e_i)) =  \frac{1}{\sigma_i^2}(e_i, \lambda_i e_i) = \frac{\lambda_i}{\sigma_i^2} = 1
\]
Теперь проверим ортогональность ($1\leqslant i, j \leqslant r$):
\[
(f_i, f_j) = \left(\frac{1}{\sigma_i}\phi(e_i), \frac{1}{\sigma_j} \phi(e_j)\right) = \frac{1}{\sigma_i\sigma_j}(e_i, \phi^*\phi(e_j)) =  \frac{1}{\sigma_i\sigma_j}(e_i, \lambda_j e_j)= 0
\]
В частности это значит, что векторы $f_1,\ldots, f_r\in U$ являются линейно независимыми.
Теперь дополним векторы $f_1,\ldots,f_r\in U$ до ортонормированного базиса пространства $U$ и получим $f_1,\ldots,f_m$.
Теперь методом пристального взгляда проверяем, что построенная пара базисов удовлетворяет требуемому условию:
\[
\phi(e_1,\ldots,e_n) = (f_1,\ldots,f_m) 
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
\]

Осталось показать, что числа $\sigma_i$ определены однозначно.
Пусть у нас есть другая пара базисов $e'_1,\ldots,e'_n\in V$ и $f'_1,\ldots,f'_m\in U$ такая, что матрица $\phi$ в этой паре базисов диагональна с положительными числами $\sigma'_1,\ldots,\sigma'_r$ и нулями на диагонали.
Тогда в базисе $e'_1,\ldots,e'_n$ матрица оператора $\phi^*\phi$ будет диагональной с числами $(\sigma'_1)^2,\ldots,(\sigma'_r)^2$ и нулями на диагонали.
Но тогда эти числа совпадают со спектром $\phi^*\phi$.
А так как $\sigma'_i$ положительны, то они однозначно восстанавливаются как корни из спектра $\phi^*\phi$.
\end{proof}

\subsection{SVD или сингулярное разложение}

Давайте переформулируем последнее утверждение в матричных терминах.
Чтобы упростить изложение, я все сделаю для вещественного случая.
Комплексный делается аналогично с той лишь разницей, что везде в формулах транспонированную матрицу $M^t$ надо менять на эрмитово сопряженную $M^* = \bar M^t$ при любой матрице $M$.
В начале я переформулирую утверждение на матричном языке.

\begin{claim}
Пусть дана матрица $A\in \MatrixDim{m}{n}$.
Тогда
\begin{enumerate}
\item Существует $U\in \Matrix{m}$ такая, что $U^t U = E$.

\item Существует $V\in \Matrix{n}$ такая, что $V^t V = E$.

\item Существует последовательность вещественных чисел $\sigma_1\geqslant \sigma_2\geqslant \ldots\geqslant \sigma_r > 0$.
\end{enumerate}
такие, что $A = U \Sigma V^t$, где
\[
\Sigma =
\overbrace{
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
}^n
\left.
\vphantom{
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}&{}\\
{}&{}&{\sigma_r}&{}&{}&{}\\
{}&{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{}&{0}&{}\\
\end{pmatrix}
}
\right\}{\scriptstyle m}
\]
При этом последовательность чисел $\sigma_1,\sigma_2,\ldots,\sigma_r$ определена однозначно.
\end{claim}

\begin{itemize}
\item Разложение матрицы $A$ в этом утверждении называется сингулярным разложением или SVD.

\item
Чтобы свести матричное утверждение к оператоному, надо рассмотреть пространства $\mathbb R^n$ и $\mathbb R^m$ со стандартным скалярным произведением и оператор $\phi\colon \mathbb R^n \to \mathbb R^m$ по правилу $x\mapsto Ax$.
Тогда столбцы матрицы $U$ -- это базисные векторы $f_1,\ldots,f_m\in \mathbb R^m$, а столбцы $V$ -- это базисные векторы $e_1,\ldots,e_n$ из утверждения~\ref{claim::HermEuclHomClass}.

\item
Обратите внимание, что такое разложение не единственное.
Например, если $A$ квадратная и равна единичной матрице, то подойдет любое разложение вида $A = U E U^t$, где $U$ -- произвольная ортогональная матрица.
В данном случае эффект связан с тем, что все сингулярные значения одинаковые.
Но даже, если сингулярные значения разные, вы можете домножать соответствующие столбцы $U$ и $V$ на минус единицу,%
\footnote{В комплексном случае домножать можно на пару сопряженных чисел по модулю равных единице.}
например.
\[
A = (u_1|\ldots | u_m)\Sigma (v_1|\ldots|v_n)^t
\quad\text{тогда}\quad
A = (-u_1|\ldots | u_m)\Sigma (-v_1|\ldots|v_n)^t
\]

\item
Последний пример показывает, что не только разложение неоднозначно, но и что между столбцами $U$ и $V$ есть некоторое условие согласованности.
Из-за этой неоднозначности в разложении, надо аккуратно искать все компоненты этого разложения.
Если вы нашли какие-то части разложения, например матрицы $U$ и $\Sigma$, то матрицу $V$ надо искать не абы каким методом.

\item
Рассмотрим сингулярное разложение $A = U \Sigma V^t$.
В этом случае $AA^t = U \Sigma\Sigma^t U^t$.
Тогда $D= \Sigma\Sigma^t$ будет диагональной матрицей, где на диагонали стоят квадраты сингулярных значений.
При этом по столбцам матрицы $U$ стоят собственные векторы для $AA^t$.
Таким образом, если сингулярное значение $\sigma_i$ отлично от всех остальных, то в вещественном случае вектор $u_i$ определен однозначно с точностью до знака.
\end{itemize}

Заметим, что в SVD матрица $\Sigma$ имеет тот же размер, что и матрица $A$.
Однако, ее правая часть полностью заполнена нулями.
А значит, что умножение этой части на соответствующую часть матрица $V^t$ ни на что не повлияет.
Предположим для определенности, что $n > m$, как на картинках.
Тогда можем определить матрицу $\Sigma_0\in \Matrix{m}$ полученную из $\Sigma$ отрезанием последних $n - m$ нулевых столбцов.
Так же заменим матрицу $V = (v_1|\ldots|v_n)$ на матрицу $V_0 = (v_1|\ldots|v_m)$.
Тогда получим разложение вида 
\[
A = U \Sigma_0 V_0^t
\]
Это разложение называется усеченным сингулярным разложением и многие алгоритмы ищут именно его.
Здесь квадратными являются $U$ и $\Sigma_0$, а столбцы $V_0$ образуют ортонормированную систему (не обязательно базис).
Если бы было $m > n$, то разложение было бы вида
\[
A = U_0 \Sigma_0 V^t
\]
при этом квадратными были бы матрицы $\Sigma_0$ и $V$, а столбцы матрицы $U_0$ образуют ортонормированную систему (не обязательно базис).


Если пойти еще дальше и применить блочные формулы к сингулярному разложению, то можно получить разложение матрицы $A$ в сумму матриц ранга $1$, а именно
\[
A = \sigma_1 u_1 v_1^t + \ldots + \sigma_r u_r v_r^t
\]
Если задать на пространстве матриц $\MatrixDim{m}{n}$ скалярное произведение в виде $(A, B) = \tr(A^t B)$, то матрицы $u_iv_i^t$ будут ортонормированной системой в пространстве матриц, а написанное выше разложение -- это разложение матрицы $A$ по ортонормированному базису, состоящему из матриц ранга $1$.

Последнее разложение используется для сжатия информации с потерей информации.
Например, про матрицу $A$ можно думать как про картинку в шкале серого, где в ячейке матрицы задана интенсивность черного.
В реальной жизни, числа $\sigma_1\geqslant \ldots \geqslant \sigma_r$ убывают очень быстро и последние значения ничтожно малы.
Если вы откините последнее слагаемое $\sigma_ru_rv_r^t$, то вы измените исходную матрицу $A$.
Однако, $u_r$ и $v_r$ -- векторы нормы $1$, а значит их координаты по модулю не больше единицы, а значит у матрицы $\sigma_r u_rv_r^t$ координаты не больше, чем $\sigma_r$ по модулю.
Значит, если $\sigma_r$  мало, то выкидывание последнего слагаемого меняет каждый коэффициент матрицы $A$ на очень малую величину, незаметную для глаза.
Таким образом, вместо того, чтобы хранить матрицу $A$ целиком (для этого нужно $mn$ ячеек памяти), мы можем отрезать в каком-нибудь месте сумму в сингулярном разложении и хранить
\[
A' = \sigma_1 u_1 v_1^t + \ldots + \sigma_k u_k v_k^t
\]
Здесь нам понадобится $k$ ячеек памяти под $\sigma_i$, $km$ ячеек памяти под векторы $u_i$ и $kn$ ячеек памяти под векторы $v_i$.
Итого будет $k(1 + n + m)$ ячеек памяти.
При этом параметр $k$ контролирует степень сжатия картинки.

\subsection{Ортопроекторы}

В этом разделе я хочу обсудить некоторые технические факты, которые мне понадобятся для задачи о низкоранговом приближении.
Давайте напомню, что такое проекторы и ортопроекторы.%
\footnote{Мы уже изучили их в разделе~\ref{section::LinearOpDef} и разделе~\ref{section::OrthoProjection}.}
Пусть $V$ -- векторное пространство, $U, W\subseteq V$ -- его подпространства и $V = U \oplus W$.
Тогда любой вектор $ v$ однозначно раскладывается в сумму $v = u + w$, где $u\in U$ и $w\in W$.
Тогда можно определить отображение $P\colon V\to V$ по правилу $v\mapsto u$.
Как легко видеть $P$ является линейным оператором.
Он называется проектором на $U$ вдоль $W$.
Заметим, что $P$ действует тождественно на $U$ и нулем на $W$.
Кроме того, $U = \Im P$, а $W = \ker P$.
Или в терминах собственных подпространств $U$ -- это $V_1$ собственное подпространство для $1$, а $W = V_0$ -- собственное подпространство для $0$.
Кроме того, мы с вами уже знаем, что оператор $P\colon V\to V$ является проектором тогда и только тогда, когда $P^2 = P$ (утверждение~\ref{claim::Projector}).

Пусть теперь $V$ -- евклидово или эрмитово пространство и $U\subseteq V$ -- некоторое подпространство.
Тогда $V = U\oplus U^\bot$.
В этом случае проектор на $U$ вдоль $U^\bot$ называется ортопроектором на $U$.

\begin{claim}
Пусть $V$ -- евклидово или эрмитово пространство и $P\colon V\to V$ -- некоторый оператор.
Тогда $P$ является ортопроектором тогда и только тогда, когда $P^2 = P$ и $P^* = P$.
\end{claim}
\begin{proof}
Мы уже знаем, что $P$ проектор тогда и только тогда, когда $P^2 = P$.
Надо показать, что при этом условии ядро и образ проектора ортогональны тогда и только тогда, когда $P$ самосопряжен.

Предположим, что $P$ ортопроектор, то есть ядро ортогонально образу.
Выберем $e_1,\ldots,e_k$ -- ортонормированный базис образа и $e_{k+1}, \ldots,e_n$ -- ортонормированный базис ядра.
Тогда в силу ортогональности ядра и образа, $e_1,\ldots,e_n$ будет ортонормированным базисом $V$ и в нем $P$ задан матрицей $A=\left(\begin{smallmatrix}{E}&{0}\\{0}&{0}\end{smallmatrix}\right)$.
То есть $A^t = A$ в вещественном случае или $\bar A^t  = A$ в комплексном.
Последнее означает, что $P$ самосопряжен.

Обратно, пусть $P^* = P$.
Надо показать, что ядро и образ ортогональны.
Пусть $u = Pv\in \Im P$ и $w \in \ker P$, тогда
\[
(u, w) = (Pv, w) = (v, P^*w) = (v, Pw) = (v, 0) = 0
\]
\end{proof}

\begin{claim}
\label{claim::BasisProj}
Пусть $V$ -- евклидово или эрмитово пространство, $P\colon V\to V$ -- ортопроектор на некоторое подпространство $U$ и $e_1,\ldots,e_n$ -- ортонормированный базис пространства $V$.
Тогда
\[
\sum_{i=1}^n |Pe_i|^2 = \dim U
\]
\end{claim}
\begin{proof}
Давайте посчитаем след оператора $P$ двумя разными способами.
Если мы выберем базис $e_1,\ldots,e_k$ в пространстве $U$ и $e_{k+1}, \ldots,e_n$ базис в $U^\bot$, то оператор $P$ в нем задается матрицей $\left(\begin{smallmatrix}{E}&{0}\\{0}&{0}\end{smallmatrix}\right)$.
То есть $\tr P = \dim U$.

С другой стороны, давайте посчитаем левую часть выражения
\[
\sum_{i=1}^n |Pe_i|^2 = \sum_{i=1}^n (Pe_i, Pe_i) = \sum_{i=1}^n (e_i, P^*Pe_i) = \sum_{i=1}^n (e_i, P^2e_i) = \sum_{i=1}^n (e_i, Pe_i)
\]
Так как базис $e_1,\ldots,e_n$ ортонормированный, то скалярное произведение стандартное.
Если $P(e_1,\ldots,e_n) = (e_1,\ldots,e_n)A$, то $(e_i, Pe_i) = a_{ii}$.
Таким образом $ \sum_{i=1}^n (e_i, Pe_i) = \sum_{i=1}^na_{ii} = \tr P$.
\end{proof}

\subsection{Задача о низкоранговом приближении}
\label{section::Approx}

Пусть теперь $A\in \MatrixDim{m}{n}$ или $A\in \operatorname{M}_{m\,n}(\mathbb C)$.
Зададим на пространствах матриц скалярное произведение.
В вещественном случае по формуле $(A, B) = \tr(A^t B)$, а в комплексном $(A, B) = \tr(A^*B)$.
Длина относительно заданного скалярного произведения называется нормой фробениуса и выражается следующим образом (в вещественном или комплексном случае соответственно):
\[
\|A\|_F = \sqrt{\sum_{ij}a_{ij}^2}
\quad\text{или}\quad
\|A\|_F = \sqrt{\sum_{ij}|a_{ij}|^2}
\]
Если матрица $A$ имеет вид $A = (A_1|\ldots|A_n)$, тогда $\|A\|_F^2 = \sum_{i=1}^n |A_i|^2$, где $|A_i|$ -- длина относительно стандартного скалярного произведения для столбца $A_i$.
В силу последнего замечания легко видеть, что норма фробениуса не меняется при домножении матрицы слева или справа на ортогональную матрицу.

Теперь наша задача -- заменить матрицу $A$ на матрицу $B$ ранга не выше $k$, причем мы хотим выбрать $B$ ближайшей в смысле нормы фробениуса.
То есть мы зафиксируем матрицу $A$ и число $k$ и будем решать задачу
\[
\left\{
\begin{aligned}
&\|A - B\|_F \to \min_B\\
&\rk B \leqslant k
\end{aligned}
\right.
\]
Важно понимать, что множество матриц ранга не выше $k$ не образуют линейное подпространство в пространстве матриц.
А значит, тут не получится решить эту задачу просто применением ортопроекторов.
Кроме того, задача может иметь не единственное решение, в некоторых ситуациях ближайших матриц может оказаться бесконечное число.

Обратите внимание, что если $k \geqslant \rk A$, то ответом будет сама матрица $A$.
А если $k < \rk A$, то оказывается, что SVD дает нужный ответ к данной задаче.
Нужно найти для матрицы $A$ сингулярное разложение.
После чего, выбрать в качестве нужной матрицы матрицу
\[
B_k = \sigma_1 u_1 v_1^t + \ldots + \sigma_k u_k v_k^t
\]
Доказательству этого факта будет посвящен этот раздел.

\begin{claim}
\label{claim::LowRankApprox}
Пусть $A\in\MatrixDim{m}{n}$, $A = U \Sigma V^t$ -- ее сингулярное разложение и $\sigma_1\geqslant \sigma_2\geqslant \ldots \geqslant \sigma_r$ -- ее сингулярные числа.
Определим матрицу
\[
\Sigma_k =
\begin{pmatrix}
{\sigma_1}&{}&{}&{}&{}\\
{}&{\ddots}&{}&{}&{}\\
{}&{}&{\sigma_k}&{}&{}\\
{}&{}&{}&{0}&{}\\
\end{pmatrix}
\in \MatrixDim{m}{n}
\]
Тогда матрица $B_k = U\Sigma_k V^t$ будет ближайшей матрицей ранга не более $k$ к матрице $A$ по норме Фробениуса, то есть $\|A - B_k\|_F\leqslant \|A - B\|_F$ для любой матрицы $B\in \MatrixDim{m}{n}$ ранга не более $k$.
\end{claim}
\begin{proof}
Так как у нас есть претендент на минимум, то мы для начала посчитаем значение $\|A-B_k\|_F$.
Получим
\[
\|A-B_k\|_F^2 = \|U(\Sigma - \Sigma_k)V^t\|_F^2 =  \|\Sigma - \Sigma_k\|_F^2
\]
Последнее равенство выполняется в силу того, что $U$ и $V$ ортогональные (см.~замечание в начале раздела~\ref{section::Approx}).
Но матрица $\Sigma-\Sigma_k$ является диагональная с $\sigma_{k+1},\ldots,\sigma_r$ на диагонали.
Потому
\[
\|A-B_k\|_F^2  = \sigma_{k+1}^2 +\ldots + \sigma_r^2
\]
То есть нам теперь надо доказать, что для любой $B\in \MatrixDim{m}{n}$ с условием $\rk B\leqslant k$ выполнено
\[
\|A - B\|_F^2\geqslant \sigma_{k+1}^2 +\ldots + \sigma_r^2
\]
В начале сведем случай к диагональной матрице $A$ следующим образом.
\[
\|A - B\|_F = \|U \Sigma V^t - B\|_F = \|\Sigma - U^t B V\|_F = \|\Sigma - B'\|_F
\]
где $B' = U^t B V$.
Введем обозначения для столбцов матрицы $B' = (B_1|\ldots|B_n)$ и пусть $e_1,\ldots,e_m$ -- стандартный базис в $\mathbb R^m$.
Тогда
\[
 \|\Sigma - B'\|^2_F = \Bigl\| (\sigma_1 e_1|\ldots|\sigma_r e_r|0 |\ldots|0) - (B_1|\ldots|B_n)\Bigl\|^2_F = \sum_{i=1}^r |\sigma_i e_i - B_i|^2 + \sum_{i=r+1}^n|B_i|^2
\]
Теперь оценим это выражение снизу отбросив вторую сумму, получим
\[
 \|\Sigma - B'\|^2_F\geqslant  \sum_{i=1}^r |\sigma_i e_i - B_i|^2
\]
При этом обратите внимание, что если линейная оболочка $U = \langle B_1,\ldots, B_n\rangle $ не лежит в линейной оболочке столбцов матрицы $\Sigma$, то у нас строгое неравенство.
Величина $|\sigma_i e_i - B_i|$ не меньше, чем расстояние от вектора $\sigma_i e_i$ до $U$.
Значит мы можем оценить эту разность через ортогональную составляющую и написать
\[
 \sum_{i=1}^r |\sigma_i e_i - B_i|^2 \geqslant \sum_{i=1}^r |\ort_U(\sigma_i e_i)|^2 =  \sum_{i=1}^r \sigma^2_i| \ort_U(e_i)|^2 
\]
Теперь обозначим $|\ort_U(e_i)|^2 = t_i$ (здесь $1\leqslant i \leqslant m$).
По утверждению~\ref{claim::BasisProj} имеем $\sum_{i=1}^r t_i = \dim \Im \ort_U \geqslant m - k$.
Теперь наша задача показать, что
\[
\sum_{i=1}^t \sigma_i^2 t_i\geqslant \sum_{i = k+1}^r\sigma_i^2\quad\text{при условии}\quad
\left\{
\begin{aligned}
&0 \leqslant t_i \leqslant 1\\
&\sum_{i=1}^r t_i \geqslant m - k
\end{aligned}
\right.
\]
Так как все функции неравенствах линейные и $\sigma_i^2$ упорядочены по убыванию, то мы извлекаем ответ методом пристального взгляда.
А именно, минимум будет в случае
\[
t_1 = 0,\ldots,t_k = 0, t_{k+1} = 1,\ldots,t_m = 1
\]
В этом случае минимальное значение выражения $\sum_{i=1}^t \sigma_i^2 t_i$ как раз получается $ \sum_{i = k+1}^r\sigma_i^2$.
\end{proof}

\paragraph{Замечания}

Обратите внимание на одну тонкость.
 В утверждении выше, мы показали, что матрица $B_k$ построенная по сингулярному разложению подходит в качестве минимума выражения $\|A - B\|_F$ среди матриц ранга $k$.
 Однако, мы с вами не знаем, а есть ли другие минимумы.
 Оказывается, что этот вопрос можно исследовать и ситуация следующая:
\begin{enumerate}
\item Пусть $A = U \Sigma V^t$ -- сингулярное разложение некоторой матрицы $A$ и $\sigma_k = \sigma_{k+1}$.
Тогда матрица $B_k$ ранга $k$, на которой достигается минимум $\|A - B\|_F$, не единственная.
Пример легко увидеть на единичной матрице $A = E$.
Тогда для любой ортогональной матрицы $U$ получим $A = U E U^t$ -- некоторое сингулярное разложение.
Если $U = (u_1|\ldots|u_n)$, то матрица
$u_1 u_1^t$ дает приближение ранга $1$.
Меняя матрицу $U$, можно получить разные приближения.
Например, если $U = E$, то $e_1e_1^t$ будет еще одним примером, где $e_1,\ldots,e_n$ -- стандартный базис.

\item Пусть $A = U \Sigma V^t$ -- сингулярное разложение некоторой матрицы $A$ и $\sigma_k > \sigma_{k+1}$.
Тогда матрица $B_k$ ранга не больше $k$, на которой достигается минимум $\|A - B\|_F$, единственная и имеет ранг $k$.
Делается это так.
Мы можем считать, что матрица $A = \Sigma$ диагональная.
Теперь надо взять произвольную матрицу $B$ ранга не больше $k$, на которой достигается минимум.
Тогда во всех оценках в доказательстве утверждения~\ref{claim::LowRankApprox} будут равенства.
То есть в частности (здесь $r$ -- ранг $A$):
\[
\sum_{i=r+1}^n |B_i|^2 = 0
\]
А значит в матрице $B$ ненулевыми могут быть только первые $r$ столбцов.
Теперь нужно посмотреть на неравенство $|\sigma_i e_i - B_i| \geqslant |\ort_U(\sigma_i e_i)|$ при $1\leqslant i \leqslant r$.
В нем равенство достигается при условии $B_i = \pr_U(\sigma_i e_i)$.
Наконец рассмотрим релаксированную задачу с $t_i$ в самом конце.
Если $\sigma_k > \sigma_{k+1}$, то эта задача имеет единственное решение $t_1 = \ldots = t_k = 0$ и $t_{k+1} = \ldots = t_m = 1$.
Значит
\[
|\ort_U(e_1)| = \ldots = |\ort_U(e_k)| = 0 \quad
|\ort_U(e_{k+1})| = \ldots = |\ort_U(e_m)| = 1
\]
Условие $|\ort_U(e_i)| = 0$ при $1 \leqslant i \leqslant k$ означает, что $e_1,\ldots,e_k \in U$, а значит
\[
B_1 = \pr_U(\sigma_1 e_1) = \sigma_1 e_1,\ldots, B_k = \pr_U(\sigma_k e_k) =\sigma_k e_k.
\]
Теперь смотрим на $|\ort_U(e_{k+i})| = 1$.
Так как $e_{k+i}$ имеют длину $1$, то по теореме Пифагора $e_{k+i} = \ort_U(e_{k+i})$, то есть $B_{k+i} = \pr_U(\sigma_{k+i} e_{k+i}) = 0$.
Значит $B$ совпадает с $\Sigma_k$, что и требовалось.
\end{enumerate}

