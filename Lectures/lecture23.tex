\ProvidesFile{lecture23.tex}[Лекция 23]


\subsection{Индуцированное линейное отображение}

Пусть $\phi\colon V\to U$ и $\psi\colon W\to E$ -- линейные отображения векторных пространств над полем $F$.
Давайте я озвучу план  действий на сейчас.
Мы хотим определить линейное отображение, которое будет обозначаться $\phi\otimes \psi\colon V\otimes_F W\to U\otimes_F E$.
То есть мы перемножаем тензорно области определения двух отображений и их области значений, и на них естественным образом возникает новое отображение, обозначаемое $\phi\otimes\psi$.
Элемент вида $\sum_i v_i\otimes w_i$ оно будет переводить в $\sum_i \phi(v_i) \otimes \psi(w_i)$.
Мы могли бы не сильно мучиться и просто определить это отображение такой формулой, но так как запись тензора в виде такой суммы неоднозначно, то вообще говоря не понятно, почему это определение корректно.
В целом тут есть два подхода как решить эту проблему.
В случае векторных пространств мы знаем как строить линейные отображения.
Достаточно в левом пространстве выбрать базис, задать на базисе отображение как надо, а потом проверить указанное свойство.
И для векторных пространств этого подхода хватает выше крыши.
Однако, я не хочу идти очевидным путем.
Одна из причин в том, что идя необычным маршрутом можно увидеть необычные вещи.
Потому предлагаю чуть-чуть потыкаться носом в формализм, а потом, утерев нос и слезу, пойти радостно применять полученные знания.

\paragraph{Построение индуцированного отображения}

Пусть $\phi\colon V\to U$ и $\psi\colon W\to E$ -- линейные отображения векторных пространств над полем $F$.
Рассмотрим следующую диаграмму
\[
\xymatrix{
	{V\times W}\ar[d]_{\phi\times \psi}\ar[r]^{\otimes}\ar[rd]^{\mu}&{V\otimes_F W}\\
	{U\times E}\ar[r]^{\otimes}&{U\otimes_F E}
}
\quad
\xymatrix{
	{(v, w)}\ar@{|->}[d]_{\phi\times\psi}\ar@{|->}[r]^{\otimes}\ar@{|->}[rd]^{\mu}&{v\otimes w}\\
	{(\phi(v), \psi(w))}\ar@{|->}[r]^{\otimes}&{\phi(v)\otimes\psi(w)}
}
\]
В ней горизонтальные стрелки -- это операции тензорного произведения  на соответствующих пространствах.
Вертикальная стрелка слева -- это отображение множеств, когда мы на паре векторов действуем соответствующим линейным отображением.
А диагональная стрелка $\mu$ -- это композиция вертикальной стрелки и нижнего тензорного произведения.

Методом пристального взгляда определяем, что отображение $\mu\colon V\times W\to U\otimes_F E$ является билинейной (но НЕ обязательно универсальной!) операцией на $V\times W$.
Потому по универсальности $V\otimes_F W$ существует линейное отображение $V\otimes_F W\to U\otimes_F E$ такое, что диаграмма ниже коммутативна
\[
\xymatrix{
	{V\times W}\ar[d]_{\phi\times \psi}\ar[r]^{\otimes}\ar[rd]^{\mu}&{V\otimes_F W}\ar@{-->}[d]\\
	{U\times E}\ar[r]^{\otimes}&{U\otimes_F E}
}
\quad
\xymatrix{
	{(v, w)}\ar@{|->}[d]_{\phi\times\psi}\ar@{|->}[r]^{\otimes}\ar@{|->}[rd]^{\mu}&{v\otimes w}\ar@{|-->}[d]\\
	{(\phi(v), \psi(w))}\ar@{|->}[r]^{\otimes}&{\phi(v)\otimes\psi(w)}
}
\]
Полученное пунктиром отображение и обозначается $\phi\otimes \psi$.
Кроме того, по построению мы видим, что $(\phi\otimes\psi)(v\otimes w) = \phi(v)\otimes\psi(w)$.
А значит по линейности
\[
(\phi\otimes \psi)(\sum_i v_i \otimes w_i) = \sum_i\phi(v_i) \otimes \psi(w_i)
\]


\paragraph{Линейная независимость с векторными коэффициентами}

Теперь я хочу продемонстрировать, как можно применять индуцированные линейные отображения.

\begin{claim}
Пусть $V$ и $U$ -- векторные пространства над полем $F$, $v_1,\ldots,v_n\in V$ -- линейно независимый набор векторов, $u_1,\ldots, u_n\in U$ -- какой-то набор векторов из $U$.
Тогда если $v_1\otimes u_1 + \ldots + v_n \otimes u_n = 0$ в $V\otimes_F U$, то все $u_i = 0$.
\end{claim}
\begin{proof}
Давайте для каждого $i$ построим линейное отображение $\phi_i\colon V\otimes_F U \to U$ такое, что линейная комбинация $v_1\otimes u_1 + \ldots + v_n \otimes u_n$ перейдет в $u_i$.
Тогда это автоматически докажет утверждение.
Действительно, тогда
\[
0 = \phi_i(0) = \phi_i (v_1\otimes u_1 + \ldots + v_n \otimes u_n) = u_i
\]
Осталось лишь построить отображения с нужными свойствами.
Для начала надо заметить, что мы можем построить линейные функции $\xi_i\colon V\to F$ по правилу $\xi_i(v_j) = 1$ если $i = j$ и $0$ при $i\neq j$.
Например, можно дополнить $v_1,\ldots,v_n$ до базиса и потом взять двойственный базис в $V^*$.
Теперь можем рассмотреть следующую композицию
\[
\phi_i\colon V\otimes_F U \stackrel{\xi_i\otimes\Identity}{\longrightarrow} F\otimes_F U = U\quad\text{где}\quad
v\otimes u \mapsto \xi_i(v)\otimes u \mapsto \xi_i(v)u
\]
Теперь проверим, что полученные отображения обладают нужным свойством:
\[
\phi_i(v_1\otimes u_1+\ldots + v_n\otimes u_n) = \phi_i(v_1\otimes u_1) + \ldots + \phi_i(v_n \otimes u_n) = \xi_i(v_1)u_1 + \ldots + \xi_i(v_n) u_n = u_i
\]
Что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Существует и другой способ показать справедливость этого утверждения, разложив все по базису тензорного произведения.
А именно, можно дополнить $v_1,\ldots,v_n$ до базиса $V$, скажем $v_1,\ldots, v_k$.
Выбрать произвольный базис $f_1,\ldots,f_m$ в $U$ и разложить $u_i$ по $f_j$.
После чего тензор $v_1\otimes u_1 + \ldots + v_n\otimes u_n$ раскладывается по базисе $v_i\otimes f_j$ и его коэффициенты приравниваются к нулю.
Очень рекомендую проделать этот способ.

\item Неформально это утверждение означает, что если векторы $v_1,\ldots,v_n$ были линейно независимы с числовыми коэффициентами в $V$, то они остаются линейно независимы в $V\otimes_F U$ с векторными коэффициентами из $U$.
\end{itemize}


\subsection*{Тензорное произведение нескольких пространств}

Пусть $V$, $U$, $W$ -- векторные пространства над полем $F$.
Тогда можно построить тензорное произведение всех трех пространств $V\otimes_F U\otimes_F W$.
Конструкция такая же, как и в случае двух, а именно, пусть $e_1,\ldots,e_n\in V$, $f_1,\ldots,f_m\in U$, $g_1,\ldots, g_r\in W$ -- какие-нибудь базисы соответствующих пространств.
Тогда картинки вида $e_i\otimes f_j\otimes g_k$ являются базисом пространства $V\otimes_F U\otimes_F W$.
В этом случае определена тернарная операция линейная по каждому аргументу $V\times U\times W\to V\otimes_F U\otimes_F W$ по правилу $(v, u, w)\mapsto v\otimes u\otimes w$.%
\footnote{Здесь надо каждый вектор расписать по своему базису и воспользоваться линейностью операции по каждому аргументу.}
То есть каждый элемент такого пространства имеет вид $\sum_i v_i\otimes u_i\otimes w_i$.
Отметим, что это пространство можно было бы строить так: сначала рассмотреть $E = V\otimes_F U$, а потом $E\otimes_F W = V\otimes_F U \otimes_F W$ (либо в другом порядке, метод пристального взгляда показывает, что результат не меняется).

Можно показать, что конструкция выше не зависит от базиса.
Это делается аналогично тому, как мы поступали для двух аргументов.
Можно развить теорию тернарных операций умножения (то есть линейных по каждому аргументу) и доказать, что построенная нами операция будет универсальная.
Но я не буду мучить никого этими тривиальными, но абстрактными рассуждениями.
Главное понять пример для двух пространств, который дает интуицию для произвольного случая.
Аналогично можно строить тензорные произведения любого количества векторных пространств.
И по аналогии, там будут жить картинки с большим количеством перечеркнутых кружков.
Например, можно построить $V\otimes_F V^* \otimes_F U \otimes_F V \otimes_F W$, тогда в нем живут картинки вида $\sum_i v_i\otimes \eta_i\otimes u_i \otimes v_i'\otimes w_i$.
И если я зафиксирую базисы $e_i$ в $V$, $\xi_i$ в $V^*$, $f_i$ в $U$ и $g_i$ в $W$, то картинки вида $e_{i_1}\otimes \xi_{i_2}\otimes u_{i_3}\otimes e_{i_4}\otimes g_{i_5}$ будут базисом пространства $V\otimes_F V^* \otimes_F U \otimes_F V \otimes_F W$.

% TO DO

\subsection{Свертка}

Пусть $V$ -- векторное пространство над полем $F$ и $V^*$ -- его двойственное.
Тогда определена операция
\[
\beta\colon V\times V^* \to F\quad\text{по правилу}\quad (v, \xi)\mapsto \xi(v)
\]
Это правило является билинейным отображением.
Действительно, мы должны проверить следующие четыре свойства:
\begin{enumerate}
\item $\beta(v_1+v_2, \xi) = \xi(v_1 + v_2) = \xi(v_1) + \xi(v_2) = \beta(v_1, \xi) + \beta(v_2, \xi)$.

\item $\beta(\lambda v, \xi) = \xi(\lambda v) = \lambda \xi(v)=\lambda \beta(v, \xi)$.

\item $\beta(v, \xi_1+\xi_2) = (\xi_1 + \xi_2)(v) = \xi_1(v) + \xi_2(v) = \beta(v,\xi_1) + \beta(v,\xi_2)$.

\item $\beta(v,\lambda \xi) = (\lambda\xi)(v) = \lambda \xi(v) = \lambda \beta(v,\xi)$.
\end{enumerate}
В силу универсального свойства тензорного произведения, существует единственное линейное отображение $\ev \colon V\otimes V^*\to F$ такое, что следующая диаграмма коммутативна.
\[
\xymatrix@R=10pt{
	{}&{}&{V\otimes_F V^*}\ar@{-->}[dd]^{\ev}\\
	{V\times V^*}\ar[urr]^{\otimes}\ar[drr]_{\beta}&{}&{}\\
	{}&{}&{F}\\
}
\quad
\xymatrix@R=10pt{
	{}&{}&{v\otimes \xi}\ar@{|-->}[dd]^{\ev}\\
	{(v, \xi)}\ar@{|->}[urr]^{\otimes}\ar@{|->}[drr]_{\beta}&{}&{}\\
	{}&{}&{\xi(v)}\\
}
\]
Отображение $\ev$ называется сверткой.
Из диаграммы выше следует, что на произвольном тензоре вида $\sum_i v_i \otimes \xi_i\in V\otimes V^*$ свертка вычисляется по правилу
\[
\ev\left(\sum_i v_i \otimes \xi_i\right) = \sum_i \xi_i(v_i)
\]
В частности такое отображение корректно и не зависит от представления тензора в виде подобной суммы.

\subsection*{Примеры свертки}

\paragraph{След матрицы}

Пусть $V = F^n$.
Тогда $V^* = (F^n)^*$ -- пространство строк длины $n$, которое тоже можно отождествить с $F^n$.
В разделе~\ref{section::TensorExmpl} мы уже посчитали, что $F^n\otimes_F F^n = \operatorname{M}_n(F)$ по правилу $\sum_i x_i\otimes y_i\mapsto \sum_i x_i y_i^t$.
Тогда отображение свертки превращается в некоторое линейное отображение на квадратных матрицах, как показано ниже
\[
\xymatrix@R=10pt{
	{F^n\otimes_F F^n}\ar@{=}[dd]\ar[rrd]^{\ev}&{}&{}\\
	{}&{}&{F}\\
	{\operatorname{M}_n(F)}\ar[rru]&{}&{}\\
}
\]
Давайте посчитаем, что получится.
Возьмем произвольную матрицу $A\in \operatorname{M}_n(F)$.
Тогда для нее можно найти разложение вида $A = \sum_i x_iy_i^t$, где $x_i,y_i\in F^n$.
Тогда матрица $A$ соответствует тензору $\sum_i x_i \otimes y_i$.
По определению свертка действует так:
\[
\ev\left(\sum_i x_i\otimes y_i\right) = \sum_i y_i^t x_i
\]
С другой стороны давайте посчитаем след матрицы $A$, пользуясь разложением в матрицы ранга $1$, получим
\[
\tr A = \tr\left(\sum_i x_i y_i^t\right) = \sum_i \tr(x_i y_i^t) = \sum_i \tr(y_i^t x_i) = \sum_i y_i^t x_i
\]
Как мы видим, после отождествления тензорного произведения с квадратными матрицами, свертка превращается в операцию вычисления следа матрицы.

Если стартовать с произвольного пространства $V$,  то пространство $V\otimes_F V^*$ отождествляется с $\Hom_F(V, V)$.
Тогда отображение свертки $\ev\colon V\otimes_F V^*\to F$ превращается в отображения взятия следа линейного оператора $\tr\colon \Hom_F(V, V)\to F$.
Это один из способов спрятать использование базиса в определении следа оператора.


\paragraph{Умножение линейных отображений}

Пусть $V$, $U$, $W$ -- векторные пространства над полем $F$.
Тогда у нас есть операция композиции отображений
\[
\circ \colon \Hom_F(U,W)\times \Hom_F(V, U)\to \Hom_F(V, W)\quad (\phi, \psi)\mapsto \phi\circ \psi
\]
Для удобства изобразим на диаграмме
\[
\xymatrix{
	{V}\ar[r]^{\psi}\ar@/_10pt/[rr]_{\phi\circ\psi}&{U}\ar[r]^{\phi}&{W}
}
\]
Теперь вспомним, что у нас есть следующие изоморфизмы (раздел~\ref{section::TensorExmpl})
\[
\Hom_F(V, U)= U\otimes_F V^*\quad \Hom_F(U, W) = W\otimes_F U^*\quad\Hom_F(V, W) = W\otimes_F V^*
\]
Напомню, что, например, при изоморфизме $\Hom_F(V, U)= U\otimes_F V^*$ тензор вида $\sum_i u_i\otimes \xi_i$ переходит в линейное отображение $\phi\colon V\to U$, действующее по правилу $\phi(v) = \sum_i \xi_i(v)u_i$.
С учетом этих изоморфизмов, операция композиции превращается в следующую
\[
\circ\colon W\otimes_F U^* \times U\otimes_F V^* \to W\otimes_F V^*\quad (w\otimes \eta, u\otimes \xi)\mapsto \eta(u) w\otimes \xi
\]
По универсальному свойству тензорного произведения, это дает единственное линейное отображение
\[
W\otimes_F U^* \otimes_F U\otimes_F V^* \to W\otimes_F V^*\quad w\otimes \eta\otimes u\otimes \xi\mapsto \eta(u) w\otimes \xi
\]
Как мы видим, это отображение устроено так: мы сделали свертку для двух центральных множителей $U^*$ и $U$, а остальные тензорные сомножители не тронули.
Если быть чуть-чуть формальнее, то это отображение есть следующая композиция
\[
W\otimes_F U^* \otimes_F U \otimes_F V^* \to W\otimes_F F\otimes_F V^* = W\otimes_F V^*
\]
где первая стрелка задана отображением $ w\otimes \eta\otimes u\otimes \xi\mapsto  w\otimes \eta(u)\otimes \xi$, то есть является отображением
\[
\Identity\otimes \ev\otimes \Identity \colon W\otimes_F U^* \otimes_F U \otimes_F V^* \to W\otimes_F F\otimes_F V^*
\]
А второе отображение $W\otimes_F F\otimes_F V^* \to W\otimes_F V^*$ задано по правилу $w\otimes \alpha \otimes \xi \mapsto \alpha w\otimes \xi$.
Мы уже по сути его встречали в разделе~\ref{section::TensorExmpl}.

\subsection{Тензоры на пространстве $V$}

Пусть $V$ -- некоторое векторное пространство над полем $F$.
Рассмотрим пространство
\[
T^m_k(V) = \underbrace{V\otimes_F \ldots \otimes_F V}_{m}\otimes_F\underbrace{V^*\otimes_F \ldots \otimes_F V^*}_{k} = V^{\otimes m}\otimes_F(V^*)^{\otimes k}
\]
Элементы этого пространства называются тензорами типа $(m, k)$.
Если $e_1,\ldots,e_n$ -- базис пространства $V$, то он естественным образом индуцирует базис в пространстве $T^m_k(V)$.
Прежде всего, он индуцирует двойственный базис $e^1,\ldots,e^n$ в пространстве $V^*$.
После чего, в пространстве $T^m_k(V)$ возникает базис
\[
e_{i_1}\otimes \ldots \otimes e_{i_m}\otimes e^{j_1}\otimes \ldots \otimes e^{j_k},\quad\text{где }\; 1\leqslant i_1,\ldots,i_m,j_1,\ldots,j_k \leqslant n
\]
Тогда произвольный тензор типа $(m,k)$ представляется единственным образом в виде суммы
\[
\sum_{\substack{1\leqslant i_1,\ldots,i_m\leqslant n\\1\leqslant j_1,\ldots,j_k\leqslant n}} a^{i_1,\ldots,i_m}_{j_1,\ldots,j_k}e_{i_1}\otimes \ldots \otimes e_{i_m}\otimes e^{j_1}\otimes \ldots \otimes e^{j_k}
\]
Таким образом любой тензор типа $(m,k)$ задается многомерной матрицей $a^{i_1,\ldots,i_m}_{j_1,\ldots,j_k}$, где индексы поделены на два типа: верхние и нижние.
Таким образом про тензоры можно думать, как про обобщение матриц.
Когда у тензора $1$ индекс, то это столбец или строка.
Когда $2$ индекса -- это прямоугольная таблица, то есть матрица.
А когда $3$ индекса, то получается трехмерная таблица.
Разницу между двумя типами индексов я продемонстрирую на примерах ниже.

\paragraph{Примеры}

\begin{itemize}
\item Тензоры типа $(1,0)$.
По определению $T^1_0(V) = V$.
То есть это векторы.
Тогда $\omega \in T^1_0(V)$ имеет вид $\omega = \sum_i x^i e_i$.
Таким образом тензор типа $(1,0)$ -- это вектор из $V$ и в базисе он задается столбцом с координатами $x^1,\ldots,x^n$.

\item Тензор типа $(0,1)$.
По определению $T^0_1(V) = V^*$.
То есть это линейная функция.
Тогда $\omega\in T^0_1(V)$ имеет вид $\omega = \sum_i y_i e^i$.
Таким образом тензор типа $(0,1)$ -- это линейная функция на $V$ и в базисе она задается строкой с координатами $y_1,\ldots,y_n$.

\item Тензор типа $(1,1)$.
По определению это $T^1_0(V) = V\otimes_F V^* = \Hom_F(V, V)$.
То есть это линейные операторы на $V$.
Тогда $\omega\in T^1_1(V)$ имеет вид $\omega = \sum_{i,j} a^{i}_{j}e_i\otimes e^j$.
Если $\phi$ -- соответствующий оператор, то
\[
\phi(e_1,\ldots,e_n) = (e_1,\ldots,e_n) 
\begin{pmatrix}
{a^{1}_{1}}&{a^{1}_{2}}&{\ldots}&{a^{1}_{n}}\\
{a^{2}_{1}}&{a^{2}_{2}}&{\ldots}&{a^{2}_{n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a^{n}_{1}}&{a^{n}_{2}}&{\ldots}&{a^{n}_{n}}\\
\end{pmatrix}
\quad\text{или}\quad
\phi(e_j) = \sum_i a^i_j e_i
\]
\end{itemize}
Обратите внимание, как выглядят индексы.
Верхний индекс -- это строка, а нижний -- столбец.


\paragraph{Обозначения Эйнштейна}

Как вы видите, расписывать тензоры типа $(m,k)$ -- огромное удовольствие.
Еще большее удовольствие -- количество знаков суммирования, которые у нас возникают.
Для того, чтобы избавиться от лишних знаков суммы принято следующее соглашение: <<если какой-то индекс в выражении встречается и сверху и снизу, то по неву ведется суммирование>>, например
\[
\sum_i x^i e_i = x^i e_i\quad \text{или}\quad \tr A = \sum_{i} a^i_i = a^i_i
\]
В этих соглашениях, если $y_i$ задает линейную  функцию $\xi$, а $x^j$ -- вектор $v$, то $\xi(v)$, записываемой с помощью суммы $\sum_i y_i x^i$ превращается в $y_i x^i$.

Такие обозначения помогают избежать ошибок. Например, если вы случайно перемножили два вектора $v = x^i e_i$ и $u = y^i e_i$, то вы получите $x^i y^i$ и у вас нет суммирования.
Это должно смутить и  навести на мысль, что делается какая-то хрень.
Я не буду использовать обозначения Эйнштейна, но полезно знать о нем, особенно, если вы будете читать какую-нибудь физическую литературу, тензоры там частые гости.

\paragraph{Смена базиса}

Мы с вами до сих пор записывали все тензоры типа $(m,k)$ в одном базисе индуцированным базисом $e_1,\ldots,e_n$.
А что, если мы теперь поменяем базис на $f_1,\ldots,f_n$, то как изменятся координаты тензора?
Давайте запишем явные формулы.
Пусть
\[
(f_1,\ldots,f_n) = (e_1,\ldots,e_n)C,\quad\text{где}\;C\in \operatorname{M}_n(F)
\]
Тогда для двойственных базисов получаем (утверждение~\ref{claim::DualBasisChange})
\[
(f^1,\ldots,f^n) = (e^1,\ldots,e^n)C^{-t}
\]
Введем явные обозначения для коэффициентов матриц $C$ и $C^{-1}$ как ниже
\[
C =
\begin{pmatrix}
{c^1_1}&{\ldots}&{c^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{c^n_1}&{\ldots}&{c^n_n}\\
\end{pmatrix}
\quad\text{и}\quad
C^{-1} =
\begin{pmatrix}
{d^1_1}&{\ldots}&{d^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{d^n_1}&{\ldots}&{d^n_n}\\
\end{pmatrix}
\]
В координатах формулы смены базиса превращаются в следующие
\[
f_j = \sum_i c^i_j e_i
\quad\text{и}\quad
f^j = \sum_i d^j_i e^i
\]
Тот факт, что для двойственного базиса мы используем транспонированную к обратной матрице отражен в этой формуле суммированием по нижнему индексу, а не по верхнему.
Пусть теперь у нас есть тензор $\omega \in T^m_k(V)$ который в двух базисах представляется так:
\begin{align*}
\omega &= \sum_{\substack{1\leqslant i_1,\ldots,i_m\leqslant n\\1\leqslant j_1,\ldots,j_k\leqslant n}} a^{i_1,\ldots,i_m}_{j_1,\ldots,j_k}e_{i_1}\otimes \ldots \otimes e_{i_m}\otimes e^{j_1}\otimes \ldots \otimes e^{j_k}\\
\omega &= \sum_{\substack{1\leqslant i_1,\ldots,i_m\leqslant n\\1\leqslant j_1,\ldots,j_k\leqslant n}} b^{i_1,\ldots,i_m}_{j_1,\ldots,j_k}f_{i_1}\otimes \ldots \otimes f_{i_m}\otimes f^{j_1}\otimes \ldots \otimes f^{j_k}
\end{align*}
Тогда, подставив в последнее равенство выражение $f_i$ через $e_i$ и $f^j$ через $e^j$, мы получим формулу
\[
a^{i_1,\ldots,i_m}_{j_1,\ldots,j_k} = \sum_{\substack{1\leqslant \bar i_1,\ldots,\bar i_m\leqslant n\\1\leqslant \bar j_1,\ldots,\bar j_k\leqslant n}} b^{\bar i_1,\ldots,\bar i_m}_{\bar j_1,\ldots,\bar j_k} c^{i_1}_{\bar i_1}\ldots c^{i_m}_{\bar i_m} d^{\bar j_1}_{j_1}\ldots d^{\bar j_k}_{j_k}
\]
Обратите внимание, что в случае векторов, линейных функций и линейных операторов эти формулы превращаются в уже известные нам:
\begin{itemize}
\item Если $\omega\in T^1_0(V) = V$, то $\omega = \sum_i a^i e_i = \sum_i b^i f_i$.
Тогда $a^i = \sum_j b^j c^i_j$, что в матричной форме превращается в знакомую нам формулу (формулы в конце раздела~\ref{subsection::FnSpace})
\[
\begin{pmatrix}
{a^1}\\{\vdots}\\{a^n}
\end{pmatrix}
=
\begin{pmatrix}
{c^1_1}&{\ldots}&{c^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{c^n_1}&{\ldots}&{c^n_n}\\
\end{pmatrix}
\begin{pmatrix}
{b^1}\\{\vdots}\\{b^n}
\end{pmatrix}
\]

\item Если $\omega\in T^0_1(V) = V^*$, $\omega = \sum_i a_i e^i = \sum_i b_i f^i$.
Тогда $a_i = \sum_j b_j d^j_i$, что превращается в знакомую формулу (это частный случай формул смены координат из раздела~\ref{subsection::FnSpace} но в двойственном пространстве, где делается замена двойственного базиса по формулам из утверждения~\ref{claim::DualBasisChange})
\[
\begin{pmatrix}
{a_1}&{\ldots}&{a_n}
\end{pmatrix}
=
\begin{pmatrix}
{b_1}&{\ldots}&{b_n}
\end{pmatrix}
\begin{pmatrix}
{d^1_1}&{\ldots}&{d^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{d^n_1}&{\ldots}&{d^n_n}\\
\end{pmatrix}
\]

\item Если $\omega\in T^1_1(V) = V\otimes_F V^* = \Hom_F(V, V)$, то $\omega = \sum_{ij}a^i_j e_i \otimes e^j = \sum_{ij} b^i_j f_i\otimes f^j$.
Тогда формула замены координат превращается в формулу (раздел про матрицу линейного оператора в разделе~\ref{section::LinearOpMatrix})
\[
\begin{pmatrix}
{a^1_1}&{\ldots}&{a^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{a^n_1}&{\ldots}&{a^n_n}\\
\end{pmatrix}
=
\begin{pmatrix}
{c^1_1}&{\ldots}&{c^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{c^n_1}&{\ldots}&{c^n_n}\\
\end{pmatrix}
\begin{pmatrix}
{b^1_1}&{\ldots}&{b^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{b^n_1}&{\ldots}&{b^n_n}\\
\end{pmatrix}
\begin{pmatrix}
{d^1_1}&{\ldots}&{d^1_n}\\
{\vdots}&{\ddots}&{\vdots}\\
{d^n_1}&{\ldots}&{d^n_n}\\
\end{pmatrix}
\]
\end{itemize}


\newpage
\section{Билинейные формы}

Мы уже видели с вами разные способы перемножать векторы и нашли самый <<лучший>> -- тензорное произведение.
Однако, универсальные операции не всегда самые интересные.
Так оказывается, что полезно уметь перемножать векторы так, чтобы в результате получалось число.
Такие операции называются билинейными формами.
В случае вещественных пространств особый вид билинейных форм нам даст так называемые скалярные произведения, которые снабдят пространство углами и расстояниями.
Мы хотим плавно двигаться в их сторону.
Тем не менее для полноты картины, я хочу осветить несколько содержательных моментов в общей теории билинейных форм.
В случае общих билинейных форм мы обсудим понятие двойственности.
А потом перейдем к билинейным формам на одном пространстве.
Среди них интерес представляют симметричные билинейные формы.
Для них мы найдем удобную классификацию (на самом деле полная классификация будет только над комплексными и вещественными числами).
Ну а потом мы перейдем к скалярным произведениям.

\subsection{Определение и примеры}

\begin{definition}
\label{def::BilinearForms}
Пусть $V$ и $U$ -- векторные пространства над полем $F$.
Билинейная форма на паре пространств $V$ и $U$ -- это билинейное отображение $\beta\colon V\times U \to F$, то есть такое отображение, что выполнены следующие свойства:
\begin{enumerate}
\item $\beta(v_1 + v_2, u) = \beta(v_1,u) + \beta(v_2,u)$ для всех $v_1,v_2\in V$ и $u\in U$.

\item $\beta(\lambda v, u) = \lambda \beta(v,u)$ для всех $v\in V$, $u\in U$ и $\lambda\in F$.

\item $\beta(v, u_1+u_2) = \beta(v, u_1) + \beta(v,u_2)$ для всех $v\in V$ и $u_1,u_2\in U$.

\item $\beta(v,\lambda u) = \lambda\beta(v,u)$ для всех $v\in V$, $u\in U$ и $\lambda\in F$.
\end{enumerate}
\end{definition}

Таким образом, билинейная форма на $V$ и $U$ -- это правило, которому скармливают два вектора (один из $V$, другой из $U$), а на выходе оно выдает нам число.
Причем это число линейно зависит от каждого из аргументов.
Множество всех билинейных форм на паре пространств $V$ и $U$ будем обозначать через $\Bil(V,U)$.

Билинейная форма -- это функция от двух аргументов.
А про функции от двух аргументов можно думать, как про оператор.
Давайте запишем нашу билинейную форму $\beta\colon V\times U \to F$ в следующем виде $\beta(v,u) = v \cdot_\beta u$, где выражение справа -- это все та же билинейная форма но в операторной записи.
Тогда определение билинейной формы можно переписать так:
\begin{definition}
Пусть $V$ и $U$ -- векторные пространства над полем $F$.
Билинейная форма на паре пространств $V$ и $U$ -- это отображение $\cdot_\beta\colon V\times U \to F$ такое, что
\begin{enumerate}
\item $(v_1 + v_2)\cdot_\beta u = v_1\cdot_\beta u + v_2\cdot_\beta u$ для всех $v_1,v_2\in V$ и $u\in U$.

\item $(\lambda v)\cdot_\beta u = \lambda (v\cdot_\beta u)$ для всех $v\in V$, $u\in U$ и $\lambda\in F$.

\item $v\cdot_\beta( u_1+u_2) = v\cdot_\beta u_1 + v\cdot_\beta u_2$ для всех $v\in V$ и $u_1,u_2\in U$.

\item $v\cdot_\beta(\lambda u) = \lambda (v\cdot_\beta u)$ для всех $v\in V$, $u\in U$ и $\lambda\in F$.
\end{enumerate}
\end{definition}

То есть наше определение превращается в определение умножения дистрибутивного по обоим аргументам и согласованное с умножением на скаляр.

\paragraph{Примеры}

\begin{enumerate}
\item Начнем с самого популярного примера: $\beta\colon F^n\times F^n \to F$ по правилу $\beta(x,y) = x^t y$.
Этот товарищ нам известен в случае $F = \mathbb R$ и $n\leqslant 3$ как скалярное произведение.
Над произвольным полем у него уже нет такой выделенной роли (даже над $\mathbb C$ оно уже не так хорошо как над $\mathbb R$), однако, это достаточно общий пример, как будет видно из классификационной теоремы.
% TO DO
% добавить ссылку на будущую классификационную теорему

\item Пусть $C[a,b]$ -- множество непрерывных функций на отрезке $[a,b]$, тогда рассмотрим следующую форму $\beta\colon C[a,b]\times C[a,b]\to \mathbb R$ по правилу $(f,g)\mapsto \int_a^b f(x)g(x)\,dx$.

\item Отображение $\operatorname{M}_n(F)\times \operatorname{M}_n(F)\to F$ по правилу $(A,B)\mapsto \tr(A^tB)$ так же является билинейной формой.
Тут можно было бы использовать $\tr(AB)$ или другие разновидности.
Однако, версия $\tr(A^tB)$ в случае поля $\mathbb R$ обладает весьма замечательными свойствами, как и пример из пункта~(1).

\item До сих пор у нас были примеры на одном векторном пространстве.
Пусть $V$ -- некоторое векторное пространство, тогда отображение $\langle,\rangle\colon V^*\times V\to F$ по правилу $(\xi,v)\mapsto \langle\xi,v\rangle := \xi(v)$ называется естественной билинейной формой.%
\footnote{Это один из главных примеров, ради которого и вводится понятие билинейной формы на паре разных пространств.
Основной плюс от этого подхода в том, что выражение $\langle \xi,v\rangle$ симметрично относительно своих аргументов в отличие от $\xi(v)$ и позволяет думать и работать с векторами и функциями на равных правах.}%
${}^{,}$%
\footnote{Про эту симметрию мы уже говорили в замечании после определения~\ref{def::DualBasis}.}
\end{enumerate}

\subsection{Матрица билинейной формы}

При изучении любого объекта один из первых вопросов: <<а как этот объект задавать?>> Сейчас мы коснемся этого вопроса для билинейных форм и начнем со следующего.

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e_1,\ldots,e_n\in V$ -- базис пространства $V$ и $f_1,\ldots,f_m\in U$ -- базис пространства $U$.
Тогда матрица $B_\beta$ с коэффициентами $b_{ij} = \beta(e_i,f_j)$ называется матрицей билинейной формы $\beta$ в паре базисов $e_1,\ldots,e_n$ и $f_1,\ldots,f_m$.
\end{definition}


\begin{claim}
\label{claim::BilinearBasis}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e = (e_1,\ldots,e_n)$ -- базис пространства $V$ и $f=(f_1,\ldots,f_m)$ -- базис пространства $U$.
Пусть $v = ex$, $x\in F^n$, $u =fy$, $y\in F^m$ и $B$ -- матрица билинейной формы $\beta$ в базисах $e$ и $f$.
Тогда $\beta(v,u) = x^t B y$.
\end{claim}
\begin{proof}
Действительно, 
\[
\beta(v,u) = \beta(\sum_{i=1}^n x_i e_i, \sum_{j=1}^m y_j f_j) = \sum_{i,j} x_iy_j\beta(e_i, f_j) = x^t B y
\]
\end{proof}

Таким образом, когда вы работаете с парой пространств $V$ и $U$, после выбора базиса они превращаются в $F^n$ и $F^m$, соответственно, а билинейная форма $\beta\colon V\times U\to F$ превращается в отображение $\beta\colon F^n \times F^m \to F$ по правилу $(x,y)\mapsto x^t B y$.

\begin{claim}
\label{claim::BilinearMatrices}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма,  $e = (e_1,\ldots,e_n)$ -- базис пространства $V$ и $f=(f_1,\ldots,f_m)$ -- базис пространства $U$.
Тогда отображение $\operatorname{Bil}(V,U)\to \operatorname{M}_{n\,m}(F)$ по правилу $\beta\mapsto B_\beta$ является биекцией.
\end{claim}
\begin{proof}
Из утверждения~\ref{claim::BilinearBasis} следует, что $\beta(x,y) = x^t B_\beta y$.
Значит, билинейная форма восстанавливается по своей матрице и отображение $\beta\mapsto B_\beta$ инъективно.
Обратно, если $B\in \operatorname{M}_{n\,m}(F)$ -- произвольная матрица, то рассмотрим форму $\beta(x,y) = x^t B y$.
Тогда по определению $B = B_\beta$.
\end{proof}
