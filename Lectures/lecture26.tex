\ProvidesFile{lecture26.tex}[Лекция 26]


\subsection{Симметричный Гаусс}

Теперь, когда мы знаем, что симметричные билинейные формы диагонализуются в каком-то базисе, хорошо было бы иметь какой-нибудь (ну хотя бы плохонький) алгоритм, приводящий форму к диагональному виду, если она задана в каком-то случайном базисе.
Пусть, скажем, нам задана билинейная форма $\beta\colon F^n \times F^n\to F$ по правилу $(x,y)\mapsto x^t By$, где $B\in \operatorname{M}_n(F)$ -- некоторая симметричная матрица.
Тогда в  новом базисе матрица будет иметь вид $C^t B C$, где $C\in \operatorname{M}_n(F)$ -- некоторая невырожденная матрица.%
\footnote{На самом деле $C$ -- матрица перехода из старого в новый базис.}
Любая невырожденная матрица $C$ раскладывается в произведение элементарных матриц (утверждение~\ref{claim::InvertibleDiscription}).
С другой стороны, если $C$ -- матрица элементарного преобразования, то $B \mapsto C^tBC$ -- это выполнение одного и того же преобразования и над строками и над столбцами (не важно в каком порядке, так как произведение матриц ассоциативно).
То есть у нас есть следующий запас операций:
\begin{itemize}
\item Прибавляем $i$-ю строку умноженную на $\lambda$ к $j$-ой строке, потом прибавляем $i$-ый столбец умноженный на $\lambda$ к $j$-ому столбцу.

\item Меняем местами $i$ и $j$ строки, после чего меняем местами $i$ и $j$ столбцы.

\item Умножаем на ненулевое $\lambda$ $i$-ю строку, потом умножаем на $\lambda$ $i$-ый столбец.
\end{itemize}
Таким образом предыдущая теорема гласит, что выполняя подобные симметричные элементарные преобразования над симметричной матрицей, мы обязательно приведем ее к диагональному виду.


\subsection{Метод Якоби}
\label{subsection::Jacoby}

\paragraph{Постановка задачи}

Пусть $\beta\colon V\times V\to F$ -- некоторая симметричная билинейная форма, $e_1,\ldots,e_n$ -- базис пространства $V$ и $B = (\beta(e_i, e_j))$ -- матрица билинейной формы в этом базисе, то есть в базисе $e_1,\ldots,e_n$ билинейная форма задается как $\beta(x, y) = x^t B y$, где $x,y\in F^n$ -- координаты векторов в базисе $e_1,\ldots,e_n$.

Введем следующие обозначения: $U_k = \langle e_1,\ldots,e_k\rangle$  -- подпространство натянутое на первые $k$ векторов исходного базиса.
Теперь выделим в матрице $B$ верхние левые блоки:
\[
B =
\begin{pmatrix}
{\boxed{
\begin{matrix}
{
\boxed{
\begin{matrix}
{
\boxed{
\begin{matrix}
{\boxed{b_{11}}}&{}\\
{}&{\ddots}
\end{matrix}
}
}&{}\\
{}&{B_k}
\end{matrix}
}
}&{}\\
{}&{\ddots}
\end{matrix}
}
}&{}\\
{}&{}
\end{pmatrix}
\]
То есть $B_k$ -- подматрица состоящая из первых $k$ строк и столбцов.
Тогда $B_k$ -- это в точности матрица $\beta|_{U_k}$ в базисе $e_1,\ldots,e_k$.
Так же обозначим через $\Delta_k$ определители $\det (B_k)$.
В дальнейшем мы будем предполагать, что все $\Delta_k \neq 0$ и наша задача будет найти базис $e_1',\ldots,e_n'$ в пространстве $V$ такой, чтоб $\langle e_1',\ldots,e_k'\rangle = U_k$ и $\beta(e_i',e_j') = 0$ при $i\neq j$.


\paragraph{Описание метода}

Предположим теперь, что все $\Delta_k\neq 0$.
Будем строить векторы нового базиса $e_1',\ldots,e_n'$ по следующим рекурентным формулам
\[
\left\{
\begin{aligned}
e_1' &= e_1\\
e_2'  &= e_2 - \frac{\beta(e_2, e_1')}{\beta(e_1',e_1')}e_1'\\
e_3'  &= e_3 - \frac{\beta(e_3, e_1')}{\beta(e_1',e_1')}e_1'- \frac{\beta(e_3, e_2')}{\beta(e_2',e_2')}e_2'\\
&\ldots\\
e_k' &= e_k - \frac{\beta(e_k, e_1')}{\beta(e_1',e_1')}e_1' - \ldots - \frac{\beta(e_k, e_{k-1}')}{\beta(e_{k-1}',e_{k-1}')}e_{k-1}'\\
&\ldots
\end{aligned}
\right.
\]
Наша задача показать, что эти формулы всегда сработают и приведут к нужному результату.
То есть нам надо показать, что все знаменатели вида $\beta(e_i',e_i')$ не равны нулю и все векторы $e_1',\ldots,e_n'$ ортогональны друг другу, то есть $\beta(e_i', e_j') = 0$ при $i\neq j$.
Для того, чтобы доказать это, мы будем индукцией по номеру построенного вектора проверять выполнимость трех инвариантов
\begin{gather*}
\langle e_1,\ldots,e_k\rangle = \langle e_1',\ldots, e_k'\rangle\\
\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k
\end{gather*}
И уже из этого мы выведем корректность алгоритма.


\begin{claim}
\label{claim::JacobiInvariants}
Пусть $\beta\colon V\times V\to F$ -- симметричная билинейная форма, $e_1,\ldots,e_n$ -- базис пространства $V$ и $B = (\beta(e_i, e_j))$ и все диагональные подматрицы $B_k$ не вырождены, то есть $\Delta_k \neq 0$ для любого $1\leqslant k\leqslant n$.
Предположим, что мы построили $k$ векторов $e_1',\ldots,e_k'$ по методу Якоби описанному выше и при этом выполнено
\begin{gather*}
\langle e_1,\ldots,e_k\rangle = \langle e_1',\ldots, e_k'\rangle\\
\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k
\end{gather*}
Тогда
\begin{enumerate}
\item Вектор 
\[
e_{k+1}' = e_{k+1} - \frac{\beta(e_{k+1}, e_1')}{\beta(e_1',e_1')}e_1' - \ldots - \frac{\beta(e_{k+1}, e_{k}')}{\beta(e_{k}',e_{k}')}e_{k}'
\]
корректно определен и ортогонален всем векторам $e_1',\ldots,e_k'$.

\item Выполнены равенства
\begin{gather*}
\langle e_1,\ldots,e_{k+1}\rangle = \langle e_1',\ldots, e_{k+1}'\rangle\\
\Delta_{k+1} = \beta(e_1',e_1') \ldots \beta(e_{k+1}',e_{k+1}')\\
\beta(e_i',e_j') = 0\text{ при }i\neq j,\;i,j\leqslant k+1
\end{gather*}
\end{enumerate}
\end{claim}
\begin{proof}
1) Так как $\Delta_k\neq 0$, то из условия $\Delta_k = \beta(e_1',e_1') \ldots \beta(e_k',e_k')$ следует, что все знаменатели в формуле для $e_{k+1}'$ не равны нулю.
Значит $e_{k+1}'$ корректно определен.
Давайте проверим, что он оказался ортогонален $\langle e_1',\ldots,e_k'\rangle = \langle e_1,\ldots,e_k\rangle$.
Для этого посчитаем 
\[
\beta(e_{k+1}', e_i') = \beta(e_{k+1}, e_i') - \frac{\beta(e_{k+1}, e_1')}{\beta(e_1',e_1')}\beta(e_1', e_i') - \ldots - \frac{\beta(e_{k+1}, e_{k}')}{\beta(e_{k}',e_{k}')}\beta(e_{k}', e_i')
\]
Так как все построенные вектры $e_1',\ldots,e_k'$ были ортогональны, то справа выживает лишь одно слагаемое, то есть
\[
\beta(e_{k+1}', e_i') = \beta(e_{k+1}, e_i') - \frac{\beta(e_{k+1}, e_i')}{\beta(e_i',e_i')}\beta(e_i', e_i') = 0
\]

2) По построению $e_{k+1}'-e_{k+1} \in \langle e_1',\ldots,e_k'\rangle = \langle e_1,\ldots,e_k\rangle$.
Откуда получаем, что $\langle e_1,\ldots,e_{k+1}\rangle = \langle e_1',\ldots,e_{k+1}'\rangle$.
Кроме того, все векторы по построению получились ортогональными.
Осталось лишь показать, что $\Delta_{k+1}$ равно произведению $\beta(e_1',e_1')\ldots \beta(e_{k+1}',e_{k+1}')$.
Для этого заметим, что мы в подпространстве $U_{k+1}$ сделали замену базиса по правилам
\[
\begin{pmatrix}
{e_1}&{\ldots}&{e_{k+1}}
\end{pmatrix}
=
\begin{pmatrix}
{e_1'}&{\ldots}&{e_{k+1}'}
\end{pmatrix}
\begin{pmatrix}
{1}&{\frac{\beta(e_2,e_1')}{\beta(e_1',e_1')}}&{\frac{\beta(e_3,e_1')}{\beta(e_1',e_1')}}&{\ldots}&{\frac{\beta(e_{k+1},e_1')}{\beta(e_1',e_1')}}\\
{}&{1}&{\frac{\beta(e_3,e_2')}{\beta(e_2',e_2')}}&{\ldots}&{\frac{\beta(e_{k+1},e_2')}{\beta(e_2',e_2')}}\\
{}&{}&{1}&{}&{}\\
{}&{}&{}&{\ddots}&{\vdots}\\
{}&{}&{}&{}&{1}\\
\end{pmatrix}
\]
Обозначим матрицу справа за $C$.
Тогда это будет матрица перехода от $e_1',\ldots,e_{k+1}'$ к базису $e_1,\ldots,e_{k+1}$.
Пусть $B_k'$ будет матрица $\beta$ в штрихованном базисе.
Тогда $B_{k+1} = C^t B_{k+1}' C$, а значит 
\[
\Delta_{k+1} = \det B_{k+1}  = \det B_{k+1}' \det C^2 = \det B_{k+1} = \beta(e_1',e_1')\ldots \beta(e_{k+1}',e_{k+1}')
\]
Что доказывает оставшееся равенство.
\end{proof}

\paragraph{Замечания}

\begin{enumerate}
\item Утверждение~\ref{claim::JacobiInvariants} показывает, что 
\begin{itemize}
\item Метод Якоби для поиска базиса $e_1',\ldots,e_n'$ сработает на каждом шаге и в итоге мы получим диагональную матрицу $B'$.

\item Полученные диагональные элементы $b_{ii}'$ матрицы $B'$ можно вычислить по формуле
\[
b_{ii}' = \beta(e_i',e_i') = \frac{\Delta_i}{\Delta_{i-1}}
\]
при этом мы считаем, что $\Delta_0 = 1$.

\item Исходная матрица $B$ представляется в виде $B = C^t B' C$, где
\[
C = 
\begin{pmatrix}
{1}&{\frac{\beta(e_2,e_1')}{\beta(e_1',e_1')}}&{\frac{\beta(e_3,e_1')}{\beta(e_1',e_1')}}&{\ldots}&{\frac{\beta(e_{k+1},e_1')}{\beta(e_1',e_1')}}\\
{}&{1}&{\frac{\beta(e_3,e_2')}{\beta(e_2',e_2')}}&{\ldots}&{\frac{\beta(e_{k+1},e_2')}{\beta(e_2',e_2')}}\\
{}&{}&{1}&{}&{}\\
{}&{}&{}&{\ddots}&{\vdots}\\
{}&{}&{}&{}&{1}\\
\end{pmatrix}
\quad
B' =
\begin{pmatrix}
{\Delta_1}&{}&{}&{}\\
{}&{\frac{\Delta_2}{\Delta_1}}&{}&{}\\
{}&{}&{\ddots}&{}\\
{}&{}&{}&{\frac{\Delta_n}{\Delta_{n-1}}}\\
\end{pmatrix}
\]
\end{itemize}

\item Если вы недоумеваете (а вообще говоря очень даже должны) откуда взялись эти дурацкие формулы для векторов $e_k'$ и как вообще можно было до них догадаться, то давайте я приоткрою завесу тайны и сообщу всю правду.
На самом деле мы производили следующий процесс.
Мы взяли вектор $e_1' = e_1$.
Далее мы решили ортогонализовать вектор $e_2$ относительно $e_1'$.
А именно, в плоскости $\langle e_1, e_2\rangle = \langle e_1', e_2\rangle$ вектор $e_2$ представляется как что-то параллельное $e_1'$ и что-то ортогональное $e_1'$.%
\footnote{Например потому что $\langle e_1, e_2\rangle = \langle e_1\rangle \oplus \langle e_1\rangle^\bot$, где ортогональное дополнение берется внутри $\langle e_1,e_2\rangle$.}
Тогда будем искать разложение вида $e_2 = \lambda e_1' + w$.
При этом будем подбирать параметр $\lambda$ так, чтобы $w$ оказался ортогонален $e_1'$, то есть
\[
0 = \beta(w, e_1')  = \beta(e_2, e_1') - \lambda\beta( e_1',e_1')
\]
Отсюда находим формулу для $\lambda$, а $w$ полагаем новым вектором $e_2'$.
Аналогично поступаем на следующем шаге, мы теперь будем пытаться раскладывать вектор $e_2$ в виде
\[
e_2 = \lambda e_1' + \mu e_2' + w
\]
И коэффициенты $\lambda$ и $\mu$ будут искаться из соображений, чтобы $w$ был ортогонален $e_1'$ и $e_2'$.
Применяя $\beta({-}, e_i')$ к $w$ мы находим коэффициенты $\lambda$ и $\mu$, а $w$ полагаем за $e_2'$ и т.д.
\end{enumerate}

\subsection{Алгоритм диагонализации на основе метода Якоби}
\label{subsection::JacobyAlg}

\begin{definition}
Пусть $B, L, U\in\operatorname{M}_n(F)$ -- матрицы такие, что $L$ нижнетреугольная с единицами на диагонали, $U$ верхнетреугольная.
Тогда представление $B = LU$ называется LU-разложением матрицы $B$.
\end{definition}

Отметим, что LU-разложение не всегда существует.
Можно показать, что для невырожденной матрицы $B$ оно существует тогда и только тогда, когда все угловые подматрицы $B_k$ невырождены.
Мы же показали этот результат только для симметрических матриц.%
\footnote{На самом деле можно рассмотреть несимметрическую форму $\beta\colon F^n\times F^n \to F$, считая левое $F^n$ и правое $F^n$ разными пространствами одной размерности.
Тогда приведенные в предыдущем разделе рассуждения остаются верны и в этом случае и доказывают LU-разложение в общем случае.
Другой подход -- сделать все на матричном языке.}
Важно, что LU-разложение обязательно единственно для невырожденной матрицы.

\begin{claim}
Пусть $B\in \operatorname{M}_n(F)$ -- некоторая невырожденная матрица.
\begin{enumerate}
\item Пусть $B = L_1 U_1 = L_2 U_2$ -- два LU-разложения матрицы $B$, тогда $L_1 = L_2$ и $U_1 = U_2$.

\item Если матрица $B$ -- симметрична и найдутся матрицы $C_1,C_2, D_1,D_2\in\operatorname{M}_n(F)$ такие, что $C_1, C_2$ -- верхнетреугольные с единицей на диагонали, $D_1, D_2$ -- диагональные и $B = C_1^t D_1 C_1 = C_2^t D_2 C_2$, то $C_1 = C_2$ и $D_1 = D_2$.
\end{enumerate}
\end{claim}
\begin{proof}
(1) Пусть $L_1 U_1 = L_2 U_2$, тогда $L_2^{-1}L_1 = U_2 U_1^{-1}$ (в силу обратимости).
Тогда левая часть -- нижне треугольная с единицами на диагонали, а правая часть -- верхне треугольная.
Такое может быть лишь когда они обе единичные, то есть $L_2^{-1} L_1 = E$ и $U_2 U_1^{-1} = E$, что и требовалось.

(2) Так как $C_1^t(DC_1)$ и $C_2^t(D_2 C_2)$ -- два LU-разложения, то $C_1 = C_2$ и $D_1C_1 = D_2C_2$.
Откуда получаем требуемое из обратимости $C_1 = C_2$.
\end{proof}

\subsubsection*{Алгоритм диагонализации на основе метода Якоби}

\paragraph{Дано}

Симметрическая матрица $B\in \operatorname{M}_n(F)$.

\paragraph{Задача}

Проверить, что все ее угловые подматрицы $B_k$ невырождены и если это так, то найти их значения, а также найти верхнетреугольную матрицу с единицами на диагонали $C\in \operatorname{M}_n(F)$ и диагональную матрицу $D\in\operatorname{M}_n(F)$ такие, что $B = C^t D C$.

\paragraph{Алгоритм}

\begin{enumerate}
\item Начнем приводить матрицу $B$ к верхнетреугольному виду элементарными преобразованиями первого типа, когда нам разрешено прибавлять строку с коэффициентом только к более низкой строке.
Возможны два исхода:
\begin{itemize}
\item На каком-то этапе получили, что на диагонали на $k$-ом месте стоит $0$, а под диагональю есть ненулевой элемент.
Это значит, что $\Delta_k = 0$.
Условие на матрицу не выполнено.

\item Мы привели матрицу $B$ к верхнетреугольной матрице $U$.
Переходим к следующему шагу.
\end{itemize}

\item Восстановим все необходимые данные по матрице $U$ следующим образом:
\begin{enumerate}
\item $D$ -- диагональ матрицы $U$.

\item $C =  D^{-1}U$.

\item $\Delta_k$ -- произведение первых $k$ элементов диагонали матрицы $D$.
\end{enumerate}
\end{enumerate}

\paragraph{Замечание}

Заметим, что данный метод работает с вдвое меньшим количеством операций нежели общий симметрический Гаусс.
Однако, для него требуется дополнительное условие, чтобы все  угловые подматрицы были невырожденные.
На практике же, для невырожденной матрицы, условие вырожденности минора -- это условие случающееся с нулевой вероятностью и в реальных данных скорее всего будет выполнено.
Но если даже оно не выполнено для невырожденной матрицы $B$, то можно взять случайную матрицу $C$ и рассмотреть $C^tBC$ вместо $B$.
Тогда с вероятностью единица, у новой матрицы все угловые миноры будут невырожденные.
Есть и другой способ.
Можно очень хитро модифицировать алгоритм выше, добавив один дополнительный шаг, который будет бороться с вырожденными угловыми минорами и станет работать всегда.


\subsection{Квадратичные формы}


\begin{definition}
Пусть $\beta\colon V\times V\to F$ -- некоторая билинейная форма (не обязательно симметричная), тогда отображение $Q\colon V\to F$ по правилу $Q(v) = \beta(v, v)$ называется квадратичной формой.
Если надо подчеркнуть связь с $\beta$ пишут $Q_\beta$.
 Множество квадратичных форм на пространстве $V$ будем обозначать через $\Quad(V)$.
\end{definition}

Отметим, что квадратичные формы являются векторным пространством над полем $F$.
Действительно, мы умеем складывать многочлены и умножать их на скаляры из $F$.

\paragraph{Однородность степени $2$}

Квадратичная форма является однородной функцией степени $2$ в следующем смысле.
Для любого $\lambda\in F$ и любого вектора $v\in V$ выполнено $Q(\lambda v) = \lambda^2 Q(v)$.


\paragraph{Замечание}

Идея квадратичной формы в следующем.
Когда нам задана билинейная форма, то мы имеем функцию двух аргументов, но она по каждому аргументу линейная.
В случае квадратичной формы, мы имеем функцию одного аргумента, однако, теряем линейность и становимся квадратичными по аргументу.
Забегая вперед, скажу, что окажется, что симметричные билинейные формы будут однозначно описываться квадратичными формами.
А так как для нас симметричные билинейные формы -- это самый интересный случай, то удобно иметь подобный механизм, когда в зависимости от задачи нам удобнее иметь два аргумента, но линейных или наоборот один, но квадратичный.


\paragraph{Квадратичные формы в координатах}

Если нам дана какая-то билинейная форма $\beta \colon F^n\times F^n\to F$ вида $\beta(x, y) = x^t B y$, где $B\in \operatorname{M}_n(F)$.
Квадратичная форма $Q_\beta\colon F^n\to F$ тогда имеет вид $Q(x) = x^t B x$.
Если расписать явно последнее выражение, то мы получим
\[
Q(x) = \beta(x,x) = x^t B x = \sum_{ij} b_{ij}x_ix_j = \sum_{i}b_{ii}x_i^2+ \sum_{i<j}(b_{ij} + b_{ji})x_i x_j
\]
Обратите внимание, что в отличие от билинейной формы, квадратичная форма не однозначно задается матрицей $B$.
Действительно, 
\[
Q(x_1,x_2) = 
\begin{pmatrix}
{x_1}&{x_2}
\end{pmatrix}
\begin{pmatrix}
{0}&{2}\\
{0}&{0}
\end{pmatrix}
\begin{pmatrix}
{x_1}\\{x_2}
\end{pmatrix}=
\begin{pmatrix}
{x_1}&{x_2}
\end{pmatrix}
\begin{pmatrix}
{0}&{0}\\
{2}&{0}
\end{pmatrix}
\begin{pmatrix}
{x_1}\\{x_2}
\end{pmatrix}=
\begin{pmatrix}
{x_1}&{x_2}
\end{pmatrix}
\begin{pmatrix}
{0}&{1}\\
{1}&{0}
\end{pmatrix}
\begin{pmatrix}
{x_1}\\{x_2}
\end{pmatrix}
=2x_1x_2
\]
За счет этого эффекта, при переходе к квадратичным формам от билинейных, мы теряем часть информации.
Однако, квадратичная форма однозначно задается симметрической матрицей $B$, то есть матрицей $B$ с условием $B^t = B$.
В примере выше -- это последний случай.


\subsection{Связь между квадратичными и билинейными формами}


\begin{claim}
[Поляризационная формула]
Пусть $V$ -- векторное пространство над полем $F$, причем $2 \neq 0$, и $Q\colon V\to F$ -- квадратичная форма.
Тогда отображение $\beta_Q\colon V\times V\to F$ по правилу 
\[
(v,u)\mapsto \beta_Q(v, u) = \frac{1}{2}\left(Q(v+u) - Q(v) - Q(u)\right)
\]
является симметричной билинейной формой.
\end{claim}
\begin{proof}
Для начала заметим, что $\beta_Q$ по формуле выше действительно получается симметричной функцией, потому нам лишь надо показать, что она будет билинейной.
Для этого вспомним, что $Q(v) = \beta(v, v)$ для некоторой необязательно симметричной билинейной формы $\beta\colon V\times V\to F$.
Теперь подставим это выражение в определение $\beta_Q$ и получим
\begin{gather*}
\beta_Q(v, u) = \frac{1}{2}\left(\beta(v + u, v + u) - \beta(v, v) - \beta(u, u)\right) =\\
= \frac{1}{2}\left(\beta(v,v) + \beta(v, u) + \beta(u, v) + \beta(u,u)- \beta(v, v) - \beta(u, u)\right) = \frac{1}{2}\left(\beta(v, u) + \beta(u, v) \right)
\end{gather*}
Что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Таким образом у нас получается отображение $\Phi\colon\Quad(V) \to \Bil(V)$ по правилу $Q\mapsto \beta_Q$.
Кроме того, это отображение является линейным.

\item После выбора базиса билинейные и квадратичные формы задаются матрицами.
Тогда отображение записывается так: пусть квадратичная форма $Q(x) = x^t A x$ задается какой-нибудь матрицей $A$, тогда $\beta_Q(x, y) = x^t \left(\frac{A + A^t}{2}\right) y$.
Причем результат не зависит от выбора матрицы $A$, которой задается квадратичная форма (просто потому что поляризационная формула не зависит от этого выбора, а матричная формула -- это координатная запись поляризационной формулы).
\end{itemize}


Теперь сформулируем общее утверждение о взаимосвязи между билинейными и квадратичными формами.

\begin{claim}
\label{claim::SBilQuad}
Пусть $V$ -- векторное пространство над полем $F$, в котором $2 \neq 0$.
Пусть $\Psi\colon \Bil(V) \to \Quad(V)$ по правилу $\beta\mapsto Q_\beta$ и $\Phi\colon \Quad(V)\to \Bil(V)$ по правилу $Q \mapsto \beta_Q$ по поляризационной формуле.
Тогда
\begin{enumerate}
\item $\Psi\circ \Phi$ является тождественным на $\Quad(V)$.

\item $\Phi\circ \Psi$ является проектором на пространство симметричных билинейных форм вдоль подпространства кососимметричных билинейных форм.
\end{enumerate}

Как следствие:
\begin{enumerate}
[\rm I.]
\item $\ker \Psi$ совпадает с подпространством кососимметричных билинейных форм.

\item $\Im \Psi$ совпадает с пространством всех квадратичных форм, то есть $\Psi$ сюръективно.

\item $\Psi$ и $\Phi$ являются взаимно обратными изоморфизмами между $\SBil(V)$ (симметричными билинейными формами) и $\Quad(V)$.
\end{enumerate}
\end{claim}
\begin{proof}
1) Проверяется в лоб прямым вычислением.
Пусть $Q(v) = \beta(v,v)$ -- некоторая квадратичная форма, тогда $\beta_Q(v,u) = \frac{1}{2}(\beta(v,u) + \beta(u,v))$ (как мы видели в доказательстве поляризационной формулы).
И теперь надо взять $Q_{\beta_Q}(v) =  \frac{1}{2}(\beta(v,v) + \beta(v,v)) = \beta(v,v)$.

2) Пусть $\SBil(V)$ -- пространство симметричных билинейных форм и $\ABil(V)$ -- пространство кососимметричных билинейных форм.
Тогда $\Bil(V) = \SBil(V) \oplus \ABil(V)$ по утверждению~\ref{claim::BilDirectSA}.
Нам надо показать, что $\Phi\circ \Psi$ зануляет кососимметричные билинейные формы и оставляет на месте все симметричные.
Но $\Psi$ отправляет форму $\beta(v, u)$ в форму $\beta(v,v)$.
Потому если форма кососимметрична, то результат ноль.
Теперь пусть $\beta$ -- симметричная форма.
Тогда она идет в $Q_\beta(v) = \beta(v,v)$, которая идет по поляризационной формуле в $\beta_{Q_\beta}(v, u)= \frac{1}{2}(\beta(v,u) + \beta(u,v)) = \beta(v,u)$.

I) По определению $\ABil(V) \subseteq \ker \Psi \subseteq \ker \Phi\circ \Psi = \ABil(V)$.

II) Для любой квадратичной формы $Q$ верно $Q = \Psi(\Phi(Q))$, значит $\Psi$ сюръективно.

III) Этот пункт непосредственно проверялся во время доказательства (1) и (2).

\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Таким образом нет разницы между симметрическими билинейными формами и квадратичными формами, в случае $2 \neq 0$ в поле $F$.
Это значит, что если вы что-то доказали для симметрической билинейной формы, то этот факт можно перевести на язык квадратичных форм и он там будет верен автоматически.
И наоборот, если вы что-то сделали для квадратичных форм, то вы автоматически что-то показали для билинейных.

\item Когда мы работаем с билинейными формами плохо то, что они имеют два аргумента.
Зато по каждому аргументу форма линейна.
Квадратичная форма имеет только один аргумент, зато она по нему не линейна, а является однородной функцией степени $2$.
В зависимости от задачи бывает удобнее пользоваться квадратичными формами, бывает билинейными.
Потому полезно понимать, что вы выигрываете, а что проигрываете при переходе от одних к другим.

\item Другой взгляд на последнее утверждение такой: квадратичные формы однозначно задаются симметрическими матрицами.

\end{itemize}


\subsection{Метод Лагранжа}

Когда нам задана симметрическая билинейная форма, одна из основных задач -- диагонализировать ее в каком-нибудь базисе.
Так как симметрические билинейные формы соответствуют квадратичным формам (утверждение~\ref{claim::SBilQuad}), то неплохо было бы понять, что это означает для последних.
Метод пристального взгляда говорит, что симметричная билинейная форма $\beta\colon F^n \times F^n \to F$ по правилу $(x, y)\mapsto x^t B y$ задана в диагональном виде тогда и только тогда, когда $Q_\beta(x) = a_{11}x_1^2 + \ldots + a_{nn}x_n^2$.
То есть на языке квадратичных форм диагонализация матрицы -- это представление формы в виде суммы квадратов координат с коэффициентами.
Существует универсальный метод, приводящий любую квадратичную форму к такому виду -- метод Лагранжа.%
\footnote{На мой взгляд метод Лагранжа не самый удачный.
Точнее мы уже знаем симметричный метод Гаусса, который сильно удобнее и в целом решает все нужные проблемы.
Но тем не менее, для полноты картины я привожу здесь подробное описание метода Лагранжа, иногда даже он бывает полезен.}

Этот метод работает на языке замены координат.
Пусть у нас
\[
Q_\beta(x) = \sum_{ij}b_{ij}x_i x_j = b_{11}x_1^2 + \sum_{j=2}^n b_{1j}x_j x_1 + Q'(x_2,\ldots,x_n)
\]
Далее поведение метода зависит от того является ли коэффициент $b_{11}$ нулем.
Давайте в начале разберем типичный шаг метода, когда этот коэффициент не ноль.

В этом случае выделим полный квадрат из первых двух слагаемых
\[
Q_\beta(x) = b_{11}\left(x_1^2 + 2 \sum_{j=2}^n \frac{b_{1j}}{2 b_{11}}x_j x_1 + \left(\sum_{j=2}^n \frac{b_{1j}}{2 b_{11}}x_j \right)^2\right) - b_{11}\left(\sum_{j=2}^n \frac{b_{1j}}{2 b_{11}}x_j\right)^2 + Q'(x_2,\ldots,x_n)
\]
Тогда
\[
Q_\beta(x) = b_{11}\left(x_1 + \sum_{j=2}^n \frac{b_{1j}}{2 b_{11}}x_j \right)^2 + Q''(x_2,\ldots,x_n)
\]
Сделаем замену
\[
\left\{
\begin{aligned}
\bar x_1 &= x_1 + \sum_{j=2}^n \frac{b_{1j}}{2 b_{11}}x_j \\
\bar x_2 &= x_2\\
&\ldots\\
\bar x_n &= x_n
\end{aligned}
\right.
\quad\text{то есть}\quad
\begin{pmatrix}
{\bar x_1}\\{\bar x_2}\\{\vdots}\\{\bar x_n}
\end{pmatrix}
=
\begin{pmatrix}
{1}&{\frac{b_{12}}{2b_{11}}}&{\ldots}&{\frac{b_{1n}}{2b_{11}}}\\
{}&{1}&{}&{}\\
{}&{}&{1}&{}\\
{}&{}&{}&{1}\\
\end{pmatrix}
\begin{pmatrix}
{x_1}\\{x_2}\\{\vdots}\\{x_n}
\end{pmatrix}
\]
Теперь мы получили, что $Q(\bar x) = b_{11}\bar x_1^2 + Q''(\bar x_2,\ldots, \bar x_n)$.
Далее переходим к форме $Q''(\bar x_2,\ldots, \bar x_n)$ и повторяем процедуру.

Теперь давайте разберемся что делать, если $b_{11} = 0$.
Если хотя бы один из коэффициентов $b_{ii}\neq 0$, мы переставим координаты так, чтобы $x_i$ превратилась в $x_1$.
В противном случае $Q(x)$ вообще не зависит от $x_i^2$ ни для какого $i$.
Но тогда она должна зависеть от какого-то $x_i x_j$.
После переупорядочивания координат, мы можем считать, что это $x_1x_2$.
Тогда
\[
Q(x) = b_{12}x_1 x_2 + \sum_{\substack{i < j\\ (i, j)\neq (1, 2)}} b_{ij}x_i x_j
\]
В этом случае сделаем следующую замену переменных
\[
\left\{
\begin{aligned}
x_1 &= \bar x_1 + \bar x_2\\
x_2 &= \bar x_1 - \bar x_2\\
x_3 &= \bar x_3\\
&\vdots\\
x_n &= \bar x_n
\end{aligned}
\right.
\quad\text{то есть}\quad
\begin{pmatrix}
{x_1}\\{x_2}\\{x_3}\\{\vdots}\\{x_n}
\end{pmatrix}
=
\begin{pmatrix}
{1}&{1}&{}&{}&{}\\
{1}&{-1}&{}&{}&{}\\
{}&{}&{1}&{}&{}\\
{}&{}&{}&{\ddots}&{}\\
{}&{}&{}&{}&{1}\\
\end{pmatrix}
\begin{pmatrix}
{\bar x_1}\\{\bar x_2}\\{\bar x_3}\\{\vdots}\\{\bar x_n}
\end{pmatrix}
\]
Обратите внимание, что в этот раз мы выражаем старые переменные через новые, а не наоборот как в первом шаге.
После такой подстановки форма примет вид
\[
Q(\bar x) = b_{12}\bar x_1^2 - b_{12}\bar x_2^2 + Q'(\bar x_1,\ldots,\bar x_n)
\]
причем $Q'$ не зависит от $\bar x_1^2$ и $\bar x_2^2$.
После такой замены можно перейти к первому шагу.

\subsection{Классификация симметрических билинейных форм над алгебраически замкнутым полем}

\begin{claim}
Пусть $V$ -- векторное пространство над полем $F$, $F$ алгебраически замкнуто и $ 2 \neq 0$.
Тогда для любой симметрической билинейной формы $\beta\colon V\times V\to F$ существует базис, в котором ее матрица имеет вид
\[
\begin{pmatrix}
{E}&{0}\\
{0}&{0}
\end{pmatrix}
\]
причем размер блока $E$ совпадает с рангом $\beta$.
\end{claim}
\begin{proof}
При условии $2\neq 0$ в поле $F$ любая симметрическая билинейная форма диагонализируется по утверждению~\ref{claim::SBilToDiag}.
То есть $\beta(x,y) = a_{11}x_1 y_1 + \ldots + a_{kk}x_ky_k$, где $k\leqslant n$ (часть диагональных элементов может быть нулевыми).
Для каждого числа $a_{ii}$ найдем $s_i$ такой, что $s_i^2 = a_{ii}$.
Такое число найти можно в силу алгебраической замкнутости $F$, в нем решаются все уравнения вида $t^2 - a = 0$.
Теперь сделаем замену
\[
\bar x_1 = s_1 x_1,\ldots,\bar x_k = s_k x_k, \bar x_{k+1} = x_{k+1},\ldots,\bar x_n = x_n
\]
После такой замены $\beta(\bar x,\bar y) = x_1y_1 + \ldots x_k y_k$, что и требовалось.
\end{proof}

\paragraph{Замечания}

\begin{itemize}
\item Таким образом две симметрические матрицы $A,B\in \operatorname{M}_n(F)$ над алгебраически замкнутым полем $F$ задают одну и ту же билинейную форму в разных базисах тогда и только тогда, когда у них совпадают ранги.

\item Обратите внимание, что ранг -- это единственная содержательная характеристика симметрической билинейной формы над алгебраически замкнутым полем.
\end{itemize}

\section{Симметричные билинейные формы в вещественном пространстве}

\subsection{Классификация}

\begin{claim}
\label{claim::SBilReal}
Пусть $V$ -- векторное пространство над полем $\mathbb R$.
Тогда для любой симметрической билинейной формы $\beta\colon V\times V\to \mathbb R$ существует базис, в котором ее матрица имеет вид
\[
\begin{pmatrix}
{E}&{}&{}\\
{}&{-E}&{}\\
{}&{}&{0}\\
\end{pmatrix}
\]
причем количество единиц, минус единиц и нулей на диагонали не зависит от выбора базиса.
При этом количество единиц и минус единиц вместе совпадает с рангом $\beta$.
\end{claim}
\begin{proof}
В силу утверждения~\ref{claim::SBilToDiag}, мы можем привести форму к диагональному виду $\beta(x,y) = a_{11}x_1 y_1 + \ldots + a_{kk}x_ky_k$, где $k\leqslant n$ (часть диагональных элементов -- ноль).
При этом считаем, что сначала идут положительные $a_{ii}$, а потом отрицательные.
В отличие от алгебраически замкнутого случая, мы можем извлекать корни только из положительных чисел.
Потому сделаем такую замену
\[
\bar x_1 = \sqrt{|a_{11}|} x_1,\ldots,\bar x_k = \sqrt{|a_{kk}|} x_k, \bar x_{k+1} = x_{k+1},\ldots,\bar x_n = x_n
\]
Тогда форма примет вид 
\[
\beta(\bar x,\bar y) = \bar x_1\bar y_1+\ldots +\bar x_s \bar y_s - \bar x_{s+1}\bar y_{s+1} - \ldots - \bar x_k \bar y_k
\]

Теперь давайте докажем, что количество единиц, минус единиц и нулей не зависит от выбора базиса.
Количество нулей -- это размерность ядра формы, а количестве единиц и минус единиц -- это ранг формы.
Потому нам лишь надо доказать, что количество единиц и минус единиц не зависит от выбора базиса.

Предположим противное -- пусть зависит.
Пусть найдутся два базиса $e_1,\ldots,e_n$ и $f_1,\ldots,f_n$, так что форма в них имеет вид
\begin{align*}
\beta(x, y) &= x_1y_1+\ldots +x_s y_s - x_{s+1}y_{s+1} - \ldots - x_k y_k\\
\beta(\bar x,\bar  y) &= \bar x_1\bar y_1+\ldots +\bar x_t \bar y_t - \bar x_{t+1}\bar y_{t+1} - \ldots - \bar x_k \bar y_k\\
\end{align*}
Пусть для определенности $s > t$.
Тогда положим $W = \langle e_1,\ldots, e_s\rangle$ и $U = \langle f_{t+1},\ldots, f_n\rangle$.
Заметим, что $Q_\beta(w) > 0$ для любого ненулевого $w\in W$ и $Q_\beta(u) \leqslant 0$ для любого $u\in U$.
Следовательно подпространства $W$ и $U$ могут пересекаться только по нулю.
С другой $\dim W+\dim U = s + n - t > n$, а значит $\dim(W\cap U) > 0$, противоречие.
\end{proof}

\paragraph{Замечание}

Очень полезно о диагонализации симметричных форм думать в терминах квадратичных.
А именно, после диагонализации формы мы получили $Q_\beta(x) = a_{11}x_1^2 + \ldots +a_{kk}x_k^2$.
Теперь, чтобы сделать замену координат, нам надо внести под корень коэффициент $a_{ii}$.

\begin{definition}
Пусть $\beta\colon V\times V\to \mathbb R$ -- симметричная билинейная форма.
Тогда количество положительных, отрицательных и нулевых элементов в ее диагональной форме называются ее индексами инерции.
Это тоже самое, что посчитать количество единиц, минус единиц и нулей в диагональной форме из единиц, минус единиц и нулей.

Число единиц -- это положительный индекс инерции, он будет обозначаться $\#1$.
Число минус единиц -- это отрицательный индекс инерции, он будет обозначаться $\#-1$.
Число нулей будем для единообразия обозначать $\#0$.
\end{definition}

Обратите внимание, что утверждение~\ref{claim::SBilReal} показывает корректность этого определения, то есть, что указанные числа не зависят от диагональной формы, а зависят только от самой билинейной формы.

